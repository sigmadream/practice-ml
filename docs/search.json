[
  {
    "objectID": "ML06.html",
    "href": "ML06.html",
    "title": "알고리즘 체인과 파이프라인",
    "section": "",
    "text": "from preamble import *\nimport koreanize_matplotlib",
    "crumbs": [
      "Introduction",
      "알고리즘 체인과 파이프라인"
    ]
  },
  {
    "objectID": "ML06.html#데이터-전처리와-매개변수-선택",
    "href": "ML06.html#데이터-전처리와-매개변수-선택",
    "title": "알고리즘 체인과 파이프라인",
    "section": "데이터 전처리와 매개변수 선택",
    "text": "데이터 전처리와 매개변수 선택\n\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# 데이터 적재와 분할\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\n\n# 훈련 데이터의 최솟값, 최댓값을 계산합니다\nscaler = MinMaxScaler().fit(X_train)\n\n\n# 훈련 데이터의 스케일을 조정합니다\nX_train_scaled = scaler.transform(X_train)\nsvm = SVC()\n# 스케일 조정된 훈련데이터에 SVM을 학습시킵니다\nsvm.fit(X_train_scaled, y_train)\n# 테스트 데이터의 스케일을 조정하고 점수를 계산합니다\nX_test_scaled = scaler.transform(X_test)\nprint(\"테스트 점수: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\n\n테스트 점수: 0.97\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n# 이 코드는 예를 위한 것입니다. 실제로 사용하지 마세요.\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\ngrid.fit(X_train_scaled, y_train)\nprint(\"최상의 교차 검증 정확도: {:.2f}\".format(grid.best_score_))\nprint(\"테스트 점수: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\nprint(\"최적의 매개변수: \", grid.best_params_)\n\n최상의 교차 검증 정확도: 0.98\n테스트 점수: 0.97\n최적의 매개변수:  {'C': 1, 'gamma': 1}\n\n\n\nmglearn.plots.plot_improper_processing()",
    "crumbs": [
      "Introduction",
      "알고리즘 체인과 파이프라인"
    ]
  },
  {
    "objectID": "ML06.html#파이프라인-구축하기",
    "href": "ML06.html#파이프라인-구축하기",
    "title": "알고리즘 체인과 파이프라인",
    "section": "파이프라인 구축하기",
    "text": "파이프라인 구축하기\n\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\n\n\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('scaler', MinMaxScaler()), ('svm', SVC())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', MinMaxScaler()), ('svm', SVC())])  MinMaxScaler?Documentation for MinMaxScalerMinMaxScaler()  SVC?Documentation for SVCSVC() \n\n\n\nprint(\"테스트 점수: {:.2f}\".format(pipe.score(X_test, y_test)))\n\n테스트 점수: 0.97",
    "crumbs": [
      "Introduction",
      "알고리즘 체인과 파이프라인"
    ]
  },
  {
    "objectID": "ML06.html#그리드-서치에-파이프라인-적용하기",
    "href": "ML06.html#그리드-서치에-파이프라인-적용하기",
    "title": "알고리즘 체인과 파이프라인",
    "section": "그리드 서치에 파이프라인 적용하기",
    "text": "그리드 서치에 파이프라인 적용하기\n\nparam_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"최상의 교차 검증 정확도: {:.2f}\".format(grid.best_score_))\nprint(\"테스트 세트 점수: {:.2f}\".format(grid.score(X_test, y_test)))\nprint(\"최적의 매개변수:\", grid.best_params_)\n\n최상의 교차 검증 정확도: 0.98\n테스트 세트 점수: 0.97\n최적의 매개변수: {'svm__C': 1, 'svm__gamma': 1}\n\n\n\nmglearn.plots.plot_proper_processing()\n\n\n\n\n\n\n\n\n\nrnd = np.random.RandomState(seed=0)\nX = rnd.normal(size=(100, 10000))\ny = rnd.normal(size=(100,))\n\n\nfrom sklearn.feature_selection import SelectPercentile, f_regression\nselect = SelectPercentile(score_func=f_regression, percentile=5).fit(X, y)\nX_selected = select.transform(X)\nprint(\"X_selected.shape:\", X_selected.shape)\n\nX_selected.shape: (100, 500)\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Ridge\nprint(\"교차 검증 점수 (릿지): {:.2f}\".format(\n    np.mean(cross_val_score(Ridge(), X_selected, y, cv=5))))\n\n교차 검증 점수 (릿지): 0.91\n\n\n\npipe = Pipeline([(\"select\", SelectPercentile(score_func=f_regression,\n                                             percentile=5)),\n                 (\"ridge\", Ridge())])\nprint(\"교차 검증 점수 (파이프라인): {:.2f}\".format(\n      np.mean(cross_val_score(pipe, X, y, cv=5))))\n\n교차 검증 점수 (파이프라인): -0.25",
    "crumbs": [
      "Introduction",
      "알고리즘 체인과 파이프라인"
    ]
  },
  {
    "objectID": "ML06.html#파이프라인-인터페이스",
    "href": "ML06.html#파이프라인-인터페이스",
    "title": "알고리즘 체인과 파이프라인",
    "section": "파이프라인 인터페이스",
    "text": "파이프라인 인터페이스\n\ndef fit(self, X, y):\n    X_transformed = X\n    for name, estimator in self.steps[:-1]:\n        # 마지막 단계를 빼고 fit과 transform을 반복합니다\n        X_transformed = estimator.fit_transform(X_transformed, y)\n    # 마지막 단계 fit을 호출합니다\n    self.steps[-1][1].fit(X_transformed, y)\n    return self\n\n\ndef predict(self, X):\n    X_transformed = X\n    for step in self.steps[:-1]:\n        # 마지막 단계를 빼고 transform을 반복합니다\n        X_transformed = step[1].transform(X_transformed)\n    # 마지막 단계 predict을 호출합니다\n    return self.steps[-1][1].predict(X_transformed)\n\n\n파이프라인 그리기\n\nfrom sklearn import set_config\nset_config(display='diagram')\npipe\n\nPipeline(steps=[('select',\n                 SelectPercentile(percentile=5,\n                                  score_func=&lt;function f_regression at 0x000001675D920720&gt;)),\n                ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('select',\n                 SelectPercentile(percentile=5,\n                                  score_func=&lt;function f_regression at 0x000001675D920720&gt;)),\n                ('ridge', Ridge())])  SelectPercentile?Documentation for SelectPercentileSelectPercentile(percentile=5,\n                 score_func=&lt;function f_regression at 0x000001675D920720&gt;)  Ridge?Documentation for RidgeRidge() \n\n\n\n\nmake_pipleline을 사용한 파이프라인 생성\n\nfrom sklearn.pipeline import make_pipeline\n# 표준적인 방법\npipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n# 간소화된 방법\npipe_short = make_pipeline(MinMaxScaler(), SVC(C=100))\n\n\nprint(\"파이프라인 단계:\\n\", pipe_short.steps)\n\n파이프라인 단계:\n [('minmaxscaler', MinMaxScaler()), ('svc', SVC(C=100))]\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\npipe = make_pipeline(StandardScaler(), PCA(n_components=2), StandardScaler())\nprint(\"파이프라인 단계:\\n\", pipe.steps)\n\n파이프라인 단계:\n [('standardscaler-1', StandardScaler()), ('pca', PCA(n_components=2)), ('standardscaler-2', StandardScaler())]\n\n\n\n\n단계 속성에 접근하기\n\n# cancer 데이터셋에 앞서 만든 파이프라인을 적용합니다\npipe.fit(cancer.data)\n# \"pca\" 단계의 두 개 주성분을 추출합니다\ncomponents = pipe.named_steps[\"pca\"].components_\nprint(\"components.shape:\", components.shape)\n\ncomponents.shape: (2, 30)\n\n\n\n\n그리드 서치 안의 파이프라인의 속성에 접근하기\n\nfrom sklearn.linear_model import LogisticRegression\n\npipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n\n\nparam_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100]}\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=4)\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(X_train, y_train)\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n                                       ('logisticregression',\n                                        LogisticRegression(max_iter=1000))]),\n             param_grid={'logisticregression__C': [0.01, 0.1, 1, 10, 100]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n                                       ('logisticregression',\n                                        LogisticRegression(max_iter=1000))]),\n             param_grid={'logisticregression__C': [0.01, 0.1, 1, 10, 100]}) estimator: PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression(max_iter=1000))])  StandardScaler?Documentation for StandardScalerStandardScaler()  LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000) \n\n\n\nprint(\"최상의 모델:\\n\", grid.best_estimator_)\n\n최상의 모델:\n Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression(C=1, max_iter=1000))])\n\n\n\nprint(\"로지스틱 회귀 단계:\\n\",\n      grid.best_estimator_.named_steps[\"logisticregression\"])\n\n로지스틱 회귀 단계:\n LogisticRegression(C=1, max_iter=1000)\n\n\n\nprint(\"로지스틱 회귀 계수:\\n\",\n      grid.best_estimator_.named_steps[\"logisticregression\"].coef_)\n\n로지스틱 회귀 계수:\n [[-0.448 -0.346 -0.417 -0.529 -0.158  0.603 -0.718 -0.784  0.048  0.275\n  -1.295  0.053 -0.691 -0.919 -0.148  0.461 -0.126 -0.103  0.428  0.715\n  -1.085 -1.093 -0.851 -1.041 -0.728  0.077 -0.836 -0.649 -0.649 -0.43 ]]",
    "crumbs": [
      "Introduction",
      "알고리즘 체인과 파이프라인"
    ]
  },
  {
    "objectID": "ML06.html#전처리와-모델의-매개변수를-위한-그리드-서치",
    "href": "ML06.html#전처리와-모델의-매개변수를-위한-그리드-서치",
    "title": "알고리즘 체인과 파이프라인",
    "section": "전처리와 모델의 매개변수를 위한 그리드 서치",
    "text": "전처리와 모델의 매개변수를 위한 그리드 서치\n\n# 보스턴 주택 데이터셋이 1.2 버전에서 삭제되므로 직접 다운로드합니다.\ndata_url = \"http://lib.stat.cmu.edu/datasets/boston\"\nraw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\ndata = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\ntarget = raw_df.values[1::2, 2]\nX_train, X_test, y_train, y_test = train_test_split(data, target,\n                                                    random_state=0)\n\nfrom sklearn.preprocessing import PolynomialFeatures\npipe = make_pipeline(\n    StandardScaler(),\n    PolynomialFeatures(),\n    Ridge())\n\n\nparam_grid = {'polynomialfeatures__degree': [1, 2, 3],\n              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\ngrid.fit(X_train, y_train)\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n                                       ('polynomialfeatures',\n                                        PolynomialFeatures()),\n                                       ('ridge', Ridge())]),\n             n_jobs=-1,\n             param_grid={'polynomialfeatures__degree': [1, 2, 3],\n                         'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n                                       ('polynomialfeatures',\n                                        PolynomialFeatures()),\n                                       ('ridge', Ridge())]),\n             n_jobs=-1,\n             param_grid={'polynomialfeatures__degree': [1, 2, 3],\n                         'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}) estimator: PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('polynomialfeatures', PolynomialFeatures()),\n                ('ridge', Ridge())])  StandardScaler?Documentation for StandardScalerStandardScaler()  PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures()  Ridge?Documentation for RidgeRidge() \n\n\n\nmglearn.tools.heatmap(grid.cv_results_['mean_test_score'].reshape(3, -1),\n                      xlabel=\"ridge__alpha\", ylabel=\"polynomialfeatures__degree\",\n                      xticklabels=param_grid['ridge__alpha'],\n                      yticklabels=param_grid['polynomialfeatures__degree'], vmin=0)\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nprint(\"최적의 매개변수:\", grid.best_params_)\n\n최적의 매개변수: {'polynomialfeatures__degree': 2, 'ridge__alpha': 10}\n\n\n\nprint(\"테스트 세트 점수: {:.2f}\".format(grid.score(X_test, y_test)))\n\n테스트 세트 점수: 0.77\n\n\n\nparam_grid = {'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\npipe = make_pipeline(StandardScaler(), Ridge())\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"다항 특성이 없을 때 점수: {:.2f}\".format(grid.score(X_test, y_test)))\n\n다항 특성이 없을 때 점수: 0.63",
    "crumbs": [
      "Introduction",
      "알고리즘 체인과 파이프라인"
    ]
  },
  {
    "objectID": "ML06.html#모델-선택을-위한-그리드-서치",
    "href": "ML06.html#모델-선택을-위한-그리드-서치",
    "title": "알고리즘 체인과 파이프라인",
    "section": "모델 선택을 위한 그리드 서치",
    "text": "모델 선택을 위한 그리드 서치\n\npipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', SVC())])\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid = [\n    {'classifier': [SVC()], 'preprocessing': [StandardScaler()],\n     'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n     'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]},\n    {'classifier': [RandomForestClassifier(n_estimators=100)],\n     'preprocessing': [None], 'classifier__max_features': [1, 2, 3]}]\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\n\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(X_train, y_train)\n\nprint(\"최적의 매개변수:\\n{}\\n\".format(grid.best_params_))\nprint(\"최상의 교차 검증 점수: {:.2f}\".format(grid.best_score_))\nprint(\"테스트 세트 점수: {:.2f}\".format(grid.score(X_test, y_test)))\n\n최적의 매개변수:\n{'classifier': SVC(), 'classifier__C': 10, 'classifier__gamma': 0.01, 'preprocessing': StandardScaler()}\n\n최상의 교차 검증 점수: 0.99\n테스트 세트 점수: 0.98\n\n\n\n중복 계산 피하기\n\npipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', SVC())], memory=\"cache_folder\")",
    "crumbs": [
      "Introduction",
      "알고리즘 체인과 파이프라인"
    ]
  },
  {
    "objectID": "ML03.html",
    "href": "ML03.html",
    "title": "비지도 학습",
    "section": "",
    "text": "from preamble import *\nimport koreanize_matplotlib\n%config InlineBackend.figure_format='retina'",
    "crumbs": [
      "Introduction",
      "비지도 학습"
    ]
  },
  {
    "objectID": "ML03.html#비지도-학습-개요",
    "href": "ML03.html#비지도-학습-개요",
    "title": "비지도 학습",
    "section": "비지도 학습 개요",
    "text": "비지도 학습 개요\n\nmglearn.plots.plot_scaling()",
    "crumbs": [
      "Introduction",
      "비지도 학습"
    ]
  },
  {
    "objectID": "ML03.html#여러가지-전처리-방법",
    "href": "ML03.html#여러가지-전처리-방법",
    "title": "비지도 학습",
    "section": "여러가지 전처리 방법",
    "text": "여러가지 전처리 방법\n\n데이터 변환 적용하기\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n                                                    random_state=1)\nprint(X_train.shape)\nprint(X_test.shape)\n\n(426, 30)\n(143, 30)\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\n\nscaler.fit(X_train)\n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  MinMaxScaler?Documentation for MinMaxScaleriFittedMinMaxScaler() \n\n\n\n# 데이터 변환\nX_train_scaled = scaler.transform(X_train)\n# 스케일이 조정된 후 데이터셋의 속성을 출력합니다\nprint(\"변환된 후 크기:\", X_train_scaled.shape)\nprint(\"스케일 조정 전 특성별 최소값:\\n\", X_train.min(axis=0))\nprint(\"스케일 조정 전 특성별 최대값:\\n\", X_train.max(axis=0))\nprint(\"스케일 조정 후 특성별 최소값:\\n\", X_train_scaled.min(axis=0))\nprint(\"스케일 조정 후 특성별 최대값:\\n\", X_train_scaled.max(axis=0))\n\n변환된 후 크기: (426, 30)\n스케일 조정 전 특성별 최소값:\n [  6.981   9.71   43.79  143.5     0.053   0.019   0.      0.      0.106\n   0.05    0.115   0.36    0.757   6.802   0.002   0.002   0.      0.\n   0.01    0.001   7.93   12.02   50.41  185.2     0.071   0.027   0.\n   0.      0.157   0.055]\n스케일 조정 전 특성별 최대값:\n [  28.11    39.28   188.5   2501.       0.163    0.287    0.427    0.201\n    0.304    0.096    2.873    4.885   21.98   542.2      0.031    0.135\n    0.396    0.053    0.061    0.03    36.04    49.54   251.2   4254.\n    0.223    0.938    1.17     0.291    0.577    0.149]\n스케일 조정 후 특성별 최소값:\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n스케일 조정 후 특성별 최대값:\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1.]\n\n\n\n# 테스트 데이터 변환\nX_test_scaled = scaler.transform(X_test)\n# 스케일이 조정된 후 테스트 데이터의 속성을 출력합니다\nprint(\"스케일 조정 후 특성별 최소값:\\n\", X_test_scaled.min(axis=0))\nprint(\"스케일 조정 후 특성별 최대값:\\n\", X_test_scaled.max(axis=0))\n\n스케일 조정 후 특성별 최소값:\n [ 0.034  0.023  0.031  0.011  0.141  0.044  0.     0.     0.154 -0.006\n -0.001  0.006  0.004  0.001  0.039  0.011  0.     0.    -0.032  0.007\n  0.027  0.058  0.02   0.009  0.109  0.026  0.     0.    -0.    -0.002]\n스케일 조정 후 특성별 최대값:\n [0.958 0.815 0.956 0.894 0.811 1.22  0.88  0.933 0.932 1.037 0.427 0.498\n 0.441 0.284 0.487 0.739 0.767 0.629 1.337 0.391 0.896 0.793 0.849 0.745\n 0.915 1.132 1.07  0.924 1.205 1.631]\n\n\n\n\n훈련 데이터와 테스트 데이터의 스케일을 같은 방법으로 조정하기\n\nfrom sklearn.datasets import make_blobs\n# 인위적인 데이터셋 생성\nX, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n# 훈련 세트와 테스트 세트로 나눕니다\nX_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n\n# 훈련 세트와 테스트 세트의 산점도를 그립니다\nfig, axes = plt.subplots(1, 3, figsize=(13, 4))\naxes[0].scatter(X_train[:, 0], X_train[:, 1],\n                c=mglearn.cm2.colors[0], label=\"훈련 세트\", s=60)\naxes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^',\n                c=mglearn.cm2.colors[1], label=\"테스트 세트\", s=60)\naxes[0].legend(loc='upper left')\naxes[0].set_title(\"원본 데이터\")\n\n# MinMaxScaler를 사용해 스케일을 조정합니다\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 스케일이 조정된 데이터의 산점도를 그립니다\naxes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                c=mglearn.cm2.colors[0], label=\"훈련 세트\", s=60)\naxes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n                c=mglearn.cm2.colors[1], label=\"테스트 세트\", s=60)\naxes[1].set_title(\"스케일 조정된 데이터\")\n\n# 테스트 세트의 스케일을 따로 조정합니다\n# 테스트 세트의 최솟값은 0, 최댓값은 1이 됩니다\n# 이는 예제를 위한 것으로 절대로 이렇게 사용해서는 안됩니다\ntest_scaler = MinMaxScaler()\ntest_scaler.fit(X_test)\nX_test_scaled_badly = test_scaler.transform(X_test)\n\n# 잘못 조정된 데이터의 산점도를 그립니다\naxes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                c=mglearn.cm2.colors[0], label=\"training set\", s=60)\naxes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],\n                marker='^', c=mglearn.cm2.colors[1], label=\"test set\", s=60)\naxes[2].set_title(\"잘못 조정된 데이터\")\n\nfor ax in axes:\n    ax.set_xlabel(\"특성 0\")\n    ax.set_ylabel(\"특성 1\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# 메소드 체이닝(chaining)을 사용하여 fit과 transform을 연달아 호출합니다\nX_scaled = scaler.fit(X_train).transform(X_train)\n# 위와 동일하지만 더 효율적입니다\nX_scaled_d = scaler.fit_transform(X_train)\n\n\n\n지도 학습에서 데이터 전처리 효과\n\nfrom sklearn.svm import SVC\n\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n                                                    random_state=0)\n\nsvm = SVC(gamma='auto')\nsvm.fit(X_train, y_train)\nprint(\"테스트 세트 정확도: {:.2f}\".format(svm.score(X_test, y_test)))\n\n테스트 세트 정확도: 0.63\n\n\n\n# 0~1 사이로 스케일 조정\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 조정된 데이터로 SVM 학습\nsvm.fit(X_train_scaled, y_train)\n\n# 스케일 조정된 테스트 세트의 정확도\nprint(\"스케일 조정된 테스트 세트의 정확도: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\n\n스케일 조정된 테스트 세트의 정확도: 0.95\n\n\n\n# 평균 0, 분산 1을 갖도록 스케일 조정\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 조정된 데이터로 SVM 학습\nsvm.fit(X_train_scaled, y_train)\n\n# 스케일 조정된 테스트 세트의 정확도\nprint(\"SVM 테스트 정확도: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\n\nSVM 테스트 정확도: 0.97\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler, PowerTransformer\n\n\nX, y = make_blobs(n_samples=50, centers=2, random_state=4, cluster_std=1)\nX += 3\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, edgecolors='black')\nplt.xlim(0, 16)\nplt.xlabel('x0')\nplt.ylim(0, 10)\nplt.ylabel('x1')\nplt.title(\"Original Data\")\nplt.show()\n\n\n\n\n\n\n\n\n\nscaler = QuantileTransformer(n_quantiles=50)\nX_trans = scaler.fit_transform(X)\n\nplt.scatter(X_trans[:, 0], X_trans[:, 1], c=y, s=30, edgecolors='black')\nplt.xlim(0, 5)\nplt.xlabel('x0')\nplt.ylim(0, 5)\nplt.ylabel('x1')\nplt.title(type(scaler).__name__)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.hist(X_trans)\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(scaler.quantiles_.shape)\n\n(50, 2)\n\n\n\nx = np.array([[0], [5], [8], [9], [10]])\nprint(np.percentile(x[:, 0], [0, 25, 50, 75, 100]))\n\n[ 0.  5.  8.  9. 10.]\n\n\n\nx_trans = QuantileTransformer(n_quantiles=5).fit_transform(x)\nprint(np.percentile(x_trans[:, 0], [0, 25, 50, 75, 100]))\n\n[0.   0.25 0.5  0.75 1.  ]\n\n\n\nscaler = QuantileTransformer(n_quantiles=50, output_distribution='normal')\nX_trans = scaler.fit_transform(X)\n\nplt.scatter(X_trans[:, 0], X_trans[:, 1], c=y, s=30, edgecolors='black')\nplt.xlim(-5, 5)\nplt.xlabel('x0')\nplt.ylim(-5, 5)\nplt.ylabel('x1')\nplt.title(type(scaler).__name__)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.hist(X)\nplt.title('Original Data')\nplt.show()\n\nX_trans = QuantileTransformer(n_quantiles=50, output_distribution='normal').fit_transform(X)\nplt.hist(X_trans)\nplt.title('QuantileTransformer')\nplt.show()\n\nX_trans = StandardScaler().fit_transform(X)\nplt.hist(X_trans)\nplt.title('StandardScaler')\nplt.show()\n\nX_trans = PowerTransformer(method='box-cox').fit_transform(X)\nplt.hist(X_trans)\nplt.title('PowerTransformer box-cox')\nplt.show()\n\nX_trans = PowerTransformer(method='yeo-johnson').fit_transform(X)\nplt.hist(X_trans)\nplt.title('PowerTransformer yeo-johnson')\nplt.show()\n\nfrom sklearn.preprocessing import SplineTransformer\n\nX_trans = SplineTransformer().fit_transform(X)\nplt.hist(X_trans)\nplt.title('SplineTransformer')\nplt.show()",
    "crumbs": [
      "Introduction",
      "비지도 학습"
    ]
  },
  {
    "objectID": "ML03.html#차원-축소-특성-추출-매니폴드-학습",
    "href": "ML03.html#차원-축소-특성-추출-매니폴드-학습",
    "title": "비지도 학습",
    "section": "차원 축소, 특성 추출, 매니폴드 학습",
    "text": "차원 축소, 특성 추출, 매니폴드 학습\n\n주성분 분석 (PCA)\n\nmglearn.plots.plot_pca_illustration()\n\n\n\n\n\n\n\n\n\nPCA 적용해 유방암 데이터셋 시각화하기\n\nfig, axes = plt.subplots(15, 2, figsize=(10, 20))\nmalignant = cancer.data[cancer.target == 0]\nbenign = cancer.data[cancer.target == 1]\n\nax = axes.ravel()\n\nfor i in range(30):\n    _, bins = np.histogram(cancer.data[:, i], bins=50)\n    ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)\n    ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)\n    ax[i].set_title(cancer.feature_names[i])\n    ax[i].set_yticks(())\nax[0].set_xlabel(\"특성 크기\")\nax[0].set_ylabel(\"빈도\")\nax[0].legend([\"악성\", \"양성\"], loc=\"best\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\n\nscaler = StandardScaler()\nscaler.fit(cancer.data)\nX_scaled = scaler.transform(cancer.data)\n\n\nfrom sklearn.decomposition import PCA\n# 데이터의 처음 두 개 주성분만 유지시킵니다\npca = PCA(n_components=2)\n# 유방암 데이터로 PCA 모델을 만듭니다\npca.fit(X_scaled)\n\n# 처음 두 개의 주성분을 사용해 데이터를 변환합니다\nX_pca = pca.transform(X_scaled)\nprint(\"원본 데이터 형태:\", str(X_scaled.shape))\nprint(\"축소된 데이터 형태:\", str(X_pca.shape))\n\n원본 데이터 형태: (569, 30)\n축소된 데이터 형태: (569, 2)\n\n\n\n# 클래스를 색깔로 구분하여 처음 두 개의 주성분을 그래프로 나타냅니다.\nplt.figure(figsize=(8, 8))\nmglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\nplt.legend([\"악성\", \"양성\"], loc=\"best\")\nplt.gca().set_aspect(\"equal\")\nplt.xlabel(\"첫 번째 주성분\")\nplt.ylabel(\"두 번째 주성분\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nprint(\"PCA 주성분 형태:\", pca.components_.shape)\n\nPCA 주성분 형태: (2, 30)\n\n\n\nprint(\"PCA 주성분:\", pca.components_)\n\nPCA 주성분: [[ 0.219  0.104  0.228  0.221  0.143  0.239  0.258  0.261  0.138  0.064\n   0.206  0.017  0.211  0.203  0.015  0.17   0.154  0.183  0.042  0.103\n   0.228  0.104  0.237  0.225  0.128  0.21   0.229  0.251  0.123  0.132]\n [-0.234 -0.06  -0.215 -0.231  0.186  0.152  0.06  -0.035  0.19   0.367\n  -0.106  0.09  -0.089 -0.152  0.204  0.233  0.197  0.13   0.184  0.28\n  -0.22  -0.045 -0.2   -0.219  0.172  0.144  0.098 -0.008  0.142  0.275]]\n\n\n\nplt.matshow(pca.components_, cmap='viridis')\nplt.yticks([0, 1], [\"첫 번째 주성분\", \"두 번째 주성분\"])\nplt.colorbar()\nplt.xticks(range(len(cancer.feature_names)),\n           cancer.feature_names, rotation=60, ha='left')\nplt.xlabel(\"특성\")\nplt.ylabel(\"주성분\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\n고유얼굴 특성 추출\n\nfrom sklearn.datasets import fetch_lfw_people\npeople = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\nimage_shape = people.images[0].shape\n\nfig, axes = plt.subplots(2, 5, figsize=(15, 8), subplot_kw={'xticks': (), 'yticks': ()})\nfor target, image, ax in zip(people.target, people.images, axes.ravel()):\n    ax.imshow(image)\n    ax.set_title(people.target_names[target])\n\n\n\n\n\n\n\n\n\npeople.target[0:10], people.target_names[people.target[0:10]]\n\n(array([61, 25,  9,  5,  1, 10, 48, 17, 13, 54], dtype=int64),\n array(['Winona Ryder', 'Jean Chretien', 'Carlos Menem', 'Ariel Sharon',\n        'Alvaro Uribe', 'Colin Powell', 'Recep Tayyip Erdogan',\n        'Gray Davis', 'George Robertson', 'Silvio Berlusconi'],\n       dtype='&lt;U25'))\n\n\n\nprint(\"people.images.shape:\", people.images.shape)\nprint(\"클래스 개수:\", len(people.target_names))\n\npeople.images.shape: (3023, 87, 65)\n클래스 개수: 62\n\n\n\n# 각 타깃이 나타난 횟수 계산\ncounts = np.bincount(people.target)\n# 타깃별 이름과 횟수 출력\nfor i, (count, name) in enumerate(zip(counts, people.target_names)):\n    print(\"{0:25} {1:3}\".format(name, count), end='   ')\n    if (i + 1) % 3 == 0:\n        print()\n\nAlejandro Toledo           39   Alvaro Uribe               35   Amelie Mauresmo            21   \nAndre Agassi               36   Angelina Jolie             20   Ariel Sharon               77   \nArnold Schwarzenegger      42   Atal Bihari Vajpayee       24   Bill Clinton               29   \nCarlos Menem               21   Colin Powell              236   David Beckham              31   \nDonald Rumsfeld           121   George Robertson           22   George W Bush             530   \nGerhard Schroeder         109   Gloria Macapagal Arroyo    44   Gray Davis                 26   \nGuillermo Coria            30   Hamid Karzai               22   Hans Blix                  39   \nHugo Chavez                71   Igor Ivanov                20   Jack Straw                 28   \nJacques Chirac             52   Jean Chretien              55   Jennifer Aniston           21   \nJennifer Capriati          42   Jennifer Lopez             21   Jeremy Greenstock          24   \nJiang Zemin                20   John Ashcroft              53   John Negroponte            31   \nJose Maria Aznar           23   Juan Carlos Ferrero        28   Junichiro Koizumi          60   \nKofi Annan                 32   Laura Bush                 41   Lindsay Davenport          22   \nLleyton Hewitt             41   Luiz Inacio Lula da Silva  48   Mahmoud Abbas              29   \nMegawati Sukarnoputri      33   Michael Bloomberg          20   Naomi Watts                22   \nNestor Kirchner            37   Paul Bremer                20   Pete Sampras               22   \nRecep Tayyip Erdogan       30   Ricardo Lagos              27   Roh Moo-hyun               32   \nRudolph Giuliani           26   Saddam Hussein             23   Serena Williams            52   \nSilvio Berlusconi          33   Tiger Woods                23   Tom Daschle                25   \nTom Ridge                  33   Tony Blair                144   Vicente Fox                32   \nVladimir Putin             49   Winona Ryder               24   \n\n\n\n# np.bool은 1.20버전부터 deprecated됩니다. 대신 bool을 사용하세요.\nmask = np.zeros(people.target.shape, dtype=bool)\nfor target in np.unique(people.target):\n    mask[np.where(people.target == target)[0][:50]] = 1\n    \nX_people = people.data[mask]\ny_people = people.target[mask]\n\n# 0~255 사이의 흑백 이미지의 픽셀 값을 0~1 사이로 스케일 조정합니다.\n# (옮긴이) MinMaxScaler를 적용하는 것과 거의 동일합니다.\nX_people = X_people / 255.\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n# 데이터를 훈련 세트와 테스트 세트로 나눕니다\nX_train, X_test, y_train, y_test = train_test_split(\n    X_people, y_people, stratify=y_people, random_state=0)\n# 이웃 개수를 한 개로 하여 KNeighborsClassifier 모델을 만듭니다\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\nprint(\"1-최근접 이웃의 테스트 세트 점수: {:.2f}\".format(knn.score(X_test, y_test)))\n\n1-최근접 이웃의 테스트 세트 점수: 0.22\n\n\n\nmglearn.plots.plot_pca_whitening()\n\n\n\n\n\n\n\n\n\npca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\n\nprint(\"X_train_pca.shape:\", X_train_pca.shape)\n\nX_train_pca.shape: (1547, 100)\n\n\n\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train_pca, y_train)\nprint(\"테스트 세트 정확도: {:.2f}\".format(knn.score(X_test_pca, y_test)))\n\n테스트 세트 정확도: 0.30\n\n\n\nprint(\"pca.components_.shape:\", pca.components_.shape)\n\npca.components_.shape: (100, 5655)\n\n\n\nfig, axes = plt.subplots(3, 5, figsize=(15, 12),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\n    ax.imshow(component.reshape(image_shape), cmap='viridis')\n    ax.set_title(\"주성분 {}\".format((i + 1)))\n\n\n\n\n\n\n\n\n\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\n\nimage_shape = people.images[0].shape\nplt.figure(figsize=(20, 3))\nax = plt.gca()\n\nimagebox = OffsetImage(people.images[0], zoom=2, cmap=\"gray\")\nab = AnnotationBbox(imagebox, (.05, 0.4), pad=0.0, xycoords='data')\nax.add_artist(ab)\n\nfor i in range(4):\n    imagebox = OffsetImage(pca.components_[i].reshape(image_shape), zoom=2,\n                           cmap=\"viridis\")\n\n    ab = AnnotationBbox(imagebox, (.285 + .2 * i, 0.4),\n                        pad=0.0, xycoords='data')\n    ax.add_artist(ab)\n    if i == 0:\n        plt.text(.155, .3, 'x_{} *'.format(i), fontdict={'fontsize': 30})\n    else:\n        plt.text(.145 + .2 * i, .3, '+ x_{} *'.format(i),\n                 fontdict={'fontsize': 30})\n\nplt.text(.95, .3, '+ ...', fontdict={'fontsize': 30})\n\nplt.rc('text')\nplt.text(.12, .3, '=', fontdict={'fontsize': 30})\nplt.axis(\"off\")\nplt.show()\nplt.close()\nplt.rc('text')\n\n\n\n\n\n\n\n\n\nmglearn.plots.plot_pca_faces(X_train, X_test, image_shape)\n\n\n\n\n\n\n\n\n\nmglearn.discrete_scatter(X_train_pca[:, 0], X_train_pca[:, 1], y_train)\nplt.xlabel(\"첫 번째 주성분\")\nplt.ylabel(\"두 번째 주성분\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\n\nNMF (음수 미포함 행렬 분해)\n\n인위적 데이터에 NMF 적용하기\n\nmglearn.plots.plot_nmf_illustration()\n\n\n\n\n\n\n\n\n\n\n얼굴 이미지에 NMF 적용하기\n\nmglearn.plots.plot_nmf_faces(X_train, X_test[:3], image_shape)\n\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import NMF\nnmf = NMF(n_components=15, init='nndsvd', random_state=0, max_iter=1000, tol=1e-2)\nnmf.fit(X_train)\nX_train_nmf = nmf.transform(X_train)\nX_test_nmf = nmf.transform(X_test)\n\nfig, axes = plt.subplots(3, 5, figsize=(15, 12),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):\n    ax.imshow(component.reshape(image_shape))\n    ax.set_title(\"성분 {}\".format(i))\n\n\n\n\n\n\n\n\n\ncompn = 3\n# 4번째 성분으로 정렬하여 처음 10개 이미지를 출력합니다\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n    ax.imshow(X_train[ind].reshape(image_shape))\n    \ncompn = 7\n# 8번째 성분으로 정렬하여 처음 10개 이미지를 출력합니다\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n    ax.imshow(X_train[ind].reshape(image_shape))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS = mglearn.datasets.make_signals()\nplt.figure(figsize=(6, 1))\nplt.plot(S, '-')\nplt.xlabel(\"시간\")\nplt.ylabel(\"신호\")\nplt.margins(0)\n\n\n\n\n\n\n\n\n\n# 원본 데이터를 사용해 100개의 측정 데이터를 만듭니다\nA = np.random.RandomState(0).uniform(size=(100, 3))\nX = np.dot(S, A.T)\nprint(\"측정 데이터 형태:\", X.shape)\n\n측정 데이터 형태: (2000, 100)\n\n\n\nnmf = NMF(n_components=3, init='nndsvd', random_state=42, max_iter=1000, tol=1e-2)\nS_ = nmf.fit_transform(X)\nprint(\"복원한 신호 데이터 형태:\", S_.shape)\n\n복원한 신호 데이터 형태: (2000, 3)\n\n\n\npca = PCA(n_components=3)\nH = pca.fit_transform(X)\n\n\nmodels = [X, S, S_, H]\nnames = ['측정 신호 (처음 3개)',\n         '원본 신호',\n         'NMF로 복원한 신호', \n         'PCA로 복원한 신호']\n\nfig, axes = plt.subplots(4, figsize=(8, 4), gridspec_kw={'hspace': .5},\n                         subplot_kw={'xticks': (), 'yticks': ()})\n\nfor model, name, ax in zip(models, names, axes):\n    ax.set_title(name)\n    ax.plot(model[:, :3], '-')\n    ax.margins(0)\n\n\n\n\n\n\n\n\n\n\n\nt-SNE를 이용한 매니폴드 학습\n\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\n\nfig, axes = plt.subplots(2, 5, figsize=(10, 5),\n                         subplot_kw={'xticks':(), 'yticks': ()})\nfor ax, img in zip(axes.ravel(), digits.images):\n    ax.imshow(img)\n\n\n\n\n\n\n\n\n\n# PCA 모델을 생성합니다\npca = PCA(n_components=2)\npca.fit(digits.data)\n# 처음 두 개의 주성분으로 숫자 데이터를 변환합니다\ndigits_pca = pca.transform(digits.data)\ncolors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\",\"#535D8E\"]\nplt.figure(figsize=(10, 10))\nplt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\nplt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\nfor i in range(len(digits.data)):\n    # 숫자 텍스트를 이용해 산점도를 그립니다\n    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\n             color = colors[digits.target[i]],\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xlabel(\"첫 번째 주성분\")\nplt.ylabel(\"두 번째 주성분\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n# TSNE의 init 매개변수가 1.2버전에서 'random'에서 'pca'로 바뀔 예정입니다.\n# 경고를 피하기 위해 다음 코드를 추가합니다.\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n                        \nfrom sklearn.manifold import TSNE\ntsne = TSNE(random_state=42)\n# TSNE에는 transform 메소드가 없으므로 대신 fit_transform을 사용합니다\ndigits_tsne = tsne.fit_transform(digits.data)\n\n\nplt.figure(figsize=(10, 10))\nplt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\nplt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\nfor i in range(len(digits.data)):\n    # 숫자 텍스트를 이용해 산점도를 그립니다\n    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\n             color = colors[digits.target[i]],\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xlabel(\"t-SNE 특성 0\")\nplt.ylabel(\"t-SNE 특성 1\")\nplt.show() # 책에는 없음",
    "crumbs": [
      "Introduction",
      "비지도 학습"
    ]
  },
  {
    "objectID": "ML03.html#군집",
    "href": "ML03.html#군집",
    "title": "비지도 학습",
    "section": "군집",
    "text": "군집\n\nk-평균 군집\n\nmglearn.plots.plot_kmeans_algorithm()\n\n\n\n\n\n\n\n\n\nmglearn.plots.plot_kmeans_boundaries()\n\n\n\n\n\n\n\n\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\n\n# 인위적으로 2차원 데이터를 생성합니다\nX, y = make_blobs(random_state=1)\n\n# 군집 모델을 만듭니다\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\n\nKMeans(n_clusters=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3) \n\n\n\nprint(kmeans.labels_)\n\n[0 1 1 1 2 2 2 1 0 0 1 1 2 0 2 2 2 0 1 1 2 1 2 0 1 2 2 0 0 2 0 0 2 0 1 2 1\n 1 1 2 2 1 0 1 1 2 0 0 0 0 1 2 2 2 0 2 1 1 0 0 1 2 2 1 1 2 0 2 0 1 1 1 2 0\n 0 1 2 2 0 1 0 1 1 2 0 0 0 0 1 0 2 0 0 1 1 2 2 0 2 0]\n\n\n\nprint(kmeans.predict(X))\n\n[0 1 1 1 2 2 2 1 0 0 1 1 2 0 2 2 2 0 1 1 2 1 2 0 1 2 2 0 0 2 0 0 2 0 1 2 1\n 1 1 2 2 1 0 1 1 2 0 0 0 0 1 2 2 2 0 2 1 1 0 0 1 2 2 1 1 2 0 2 0 1 1 1 2 0\n 0 1 2 2 0 1 0 1 1 2 0 0 0 0 1 0 2 0 0 1 1 2 2 0 2 0]\n\n\n\nmglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\nmglearn.discrete_scatter(\n    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],\n    markers='^', markeredgewidth=2)\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n# 두 개의 클러스터 중심을 사용합니다\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X)\nassignments = kmeans.labels_\n\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[0])\n\n# 다섯 개의 클러스터 중심을 사용합니다\nkmeans = KMeans(n_clusters=5)\nkmeans.fit(X)\nassignments = kmeans.labels_\n\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[1])\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nk-평균 알고리즘이 실패하는 경우\n\nX_varied, y_varied = make_blobs(n_samples=200,\n                                cluster_std=[1.0, 2.5, 0.5],\n                                random_state=170)\ny_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X_varied)\n\nmglearn.discrete_scatter(X_varied[:, 0], X_varied[:, 1], y_pred)\nplt.legend([\"클러스터 0\", \"클러스터 1\", \"클러스터 2\"], loc='best')\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n# 무작위로 클러스터 데이터 생성합니다\nX, y = make_blobs(random_state=170, n_samples=600)\nrng = np.random.RandomState(74)\n\n# 데이터가 길게 늘어지도록 변경합니다\ntransformation = rng.normal(size=(2, 2))\nX = np.dot(X, transformation)\n\n# 세 개의 클러스터로 데이터에 KMeans 알고리즘을 적용합니다\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\n\n# 클러스터 할당과 클러스터 중심을 나타냅니다\nmglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\nmglearn.discrete_scatter(\n    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],\n    markers='^', markeredgewidth=2)\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n# two_moons 데이터를 생성합니다(이번에는 노이즈를 조금만 넣습니다)\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n\n# 두 개의 클러스터로 데이터에 KMeans 알고리즘을 적용합니다\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\n\n# 클러스터 할당과 클러스터 중심을 표시합니다\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60, edgecolors='k')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2, edgecolors='k')\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\n\nText(0, 0.5, '특성 1')\n\n\n\n\n\n\n\n\n\n\n\n벡터 양자화 또는 분해 메소드로서의 k-평균\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_people, y_people, stratify=y_people, random_state=42)\nnmf = NMF(n_components=100, init='nndsvd', max_iter=1000, tol=1e-2, random_state=0)\nnmf.fit(X_train)\npca = PCA(n_components=100, random_state=0)\npca.fit(X_train)\nkmeans = KMeans(n_clusters=100, random_state=0)\nkmeans.fit(X_train)\n\nX_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\nX_reconstructed_kmeans = kmeans.cluster_centers_[kmeans.predict(X_test)]\nX_reconstructed_nmf = np.dot(nmf.transform(X_test), nmf.components_)\n\n\nfig, axes = plt.subplots(3, 5, figsize=(8, 8), subplot_kw={'xticks': (), 'yticks': ()})\nfig.suptitle(\"추출한 성분\")\nfor ax, comp_kmeans, comp_pca, comp_nmf in zip(\n        axes.T, kmeans.cluster_centers_, pca.components_, nmf.components_):\n    ax[0].imshow(comp_kmeans.reshape(image_shape))\n    ax[1].imshow(comp_pca.reshape(image_shape), cmap='viridis')\n    ax[2].imshow(comp_nmf.reshape(image_shape))\n\naxes[0, 0].set_ylabel(\"kmeans\")\naxes[1, 0].set_ylabel(\"pca\")\naxes[2, 0].set_ylabel(\"nmf\")\n\nfig, axes = plt.subplots(4, 5, subplot_kw={'xticks': (), 'yticks': ()},\n                         figsize=(8, 8))\nfig.suptitle(\"재구성\")\nfor ax, orig, rec_kmeans, rec_pca, rec_nmf in zip(\n        axes.T, X_test, X_reconstructed_kmeans, X_reconstructed_pca,\n        X_reconstructed_nmf):\n    \n    ax[0].imshow(orig.reshape(image_shape))\n    ax[1].imshow(rec_kmeans.reshape(image_shape))\n    ax[2].imshow(rec_pca.reshape(image_shape))\n    ax[3].imshow(rec_nmf.reshape(image_shape))\n\naxes[0, 0].set_ylabel(\"원본\")\naxes[1, 0].set_ylabel(\"kmeans\")\naxes[2, 0].set_ylabel(\"pca\")\naxes[3, 0].set_ylabel(\"nmf\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n\nkmeans = KMeans(n_clusters=10, random_state=0)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\n\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60, cmap='Paired', edgecolors='black')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=60,\n            marker='^', c=range(kmeans.n_clusters), linewidth=2, cmap='Paired', edgecolors='black')\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nprint(\"클러스터 레이블:\\n\", y_pred)\n\n클러스터 레이블:\n [8 4 6 3 1 1 5 2 8 4 9 2 1 4 7 5 0 2 0 7 1 2 0 4 9 6 1 1 6 0 8 9 2 6 8 1 2\n 5 3 6 2 7 8 6 4 9 5 7 6 2 7 2 1 3 4 8 0 4 0 9 2 3 1 8 4 3 9 4 9 3 2 3 2 6\n 2 3 6 8 0 2 1 9 2 1 6 9 5 9 2 1 0 5 1 7 1 1 4 2 3 6 4 1 9 5 3 6 3 7 4 0 7\n 9 9 3 8 4 8 1 2 8 8 7 6 9 6 7 5 6 4 1 5 7 3 6 4 4 4 3 1 8 6 6 0 9 7 5 6 4\n 0 6 2 4 8 0 2 9 4 2 0 0 6 4 0 4 2 1 0 2 4 2 0 3 3 7 6 2 1 7 7 0 4 3 1 4 1\n 0 9 2 3 7 3 0 8 5 6 7 1 6 9 4]\n\n\n\n\n\n\n\n\n\n\ndistance_features = kmeans.transform(X)\nprint(\"클러스터 거리 데이터의 형태:\", distance_features.shape)\nprint(\"클러스터 거리:\\n\", distance_features)\n\n클러스터 거리 데이터의 형태: (200, 10)\n클러스터 거리:\n [[0.537 1.15  0.932 ... 1.48  0.003 1.077]\n [1.741 0.606 1.007 ... 2.529 1.208 2.237]\n [0.757 1.931 0.916 ... 0.783 0.876 0.718]\n ...\n [0.927 1.738 0.579 ... 1.115 0.834 1.041]\n [0.323 1.976 1.479 ... 0.814 0.846 0.284]\n [1.633 0.472 1.023 ... 2.466 1.098 2.148]]\n\n\n\n\n\n병합 군집\n\nmglearn.plots.plot_agglomerative_algorithm()\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\nX, y = make_blobs(random_state=1)\n\nagg = AgglomerativeClustering(n_clusters=3)\nassignment = agg.fit_predict(X)\n\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)\nplt.legend([\"클러스터 0\", \"클러스터 1\", \"클러스터 2\"], loc=\"best\")\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\n\nText(0, 0.5, '특성 1')\n\n\n\n\n\n\n\n\n\n\n계층적 군집과 덴드로그램(dendrograms)\n\nmglearn.plots.plot_agglomerative()\n\n\n\n\n\n\n\n\n\n# SciPy에서 ward 군집 함수와 덴드로그램 함수를 임포트합니다\nfrom scipy.cluster.hierarchy import dendrogram, ward\n\nX, y = make_blobs(random_state=0, n_samples=12)\n# 데이터 배열 X 에 ward 함수를 적용합니다\n# SciPy의 ward 함수는 병합 군집을 수행할 때 생성된\n# 거리 정보가 담긴 배열을 리턴합니다\nlinkage_array = ward(X)\n# 클러스터 간의 거리 정보가 담긴 linkage_array를 사용해 덴드로그램을 그립니다\ndendrogram(linkage_array)\n\n# 두 개와 세 개의 클러스터를 구분하는 커트라인을 표시합니다\nax = plt.gca()\nbounds = ax.get_xbound()\nax.plot(bounds, [7.25, 7.25], '--', c='k')\nax.plot(bounds, [4, 4], '--', c='k')\n\nax.text(bounds[1], 7.25, ' 두 개 클러스터', va='center', fontdict={'size': 15})\nax.text(bounds[1], 4, ' 세 개 클러스터', va='center', fontdict={'size': 15})\nplt.xlabel(\"샘플 번호\")\nplt.ylabel(\"클러스터 거리\")\n\nText(0, 0.5, '클러스터 거리')\n\n\n\n\n\n\n\n\n\n\n\nDBSCAN\n\nfrom sklearn.cluster import DBSCAN\nX, y = make_blobs(random_state=0, n_samples=12)\n\ndbscan = DBSCAN()\nclusters = dbscan.fit_predict(X)\nprint(\"클러스터 레이블:\\n\", clusters)\n\n클러스터 레이블:\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n\n\n\nmglearn.plots.plot_dbscan()\n\nmin_samples: 2 eps: 1.000000  클러스터: [-1  0  0 -1  0 -1  1  1  0  1 -1 -1]\nmin_samples: 2 eps: 1.500000  클러스터: [0 1 1 1 1 0 2 2 1 2 2 0]\nmin_samples: 2 eps: 2.000000  클러스터: [0 1 1 1 1 0 0 0 1 0 0 0]\nmin_samples: 2 eps: 3.000000  클러스터: [0 0 0 0 0 0 0 0 0 0 0 0]\nmin_samples: 3 eps: 1.000000  클러스터: [-1  0  0 -1  0 -1  1  1  0  1 -1 -1]\nmin_samples: 3 eps: 1.500000  클러스터: [0 1 1 1 1 0 2 2 1 2 2 0]\nmin_samples: 3 eps: 2.000000  클러스터: [0 1 1 1 1 0 0 0 1 0 0 0]\nmin_samples: 3 eps: 3.000000  클러스터: [0 0 0 0 0 0 0 0 0 0 0 0]\nmin_samples: 5 eps: 1.000000  클러스터: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\nmin_samples: 5 eps: 1.500000  클러스터: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\nmin_samples: 5 eps: 2.000000  클러스터: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\nmin_samples: 5 eps: 3.000000  클러스터: [0 0 0 0 0 0 0 0 0 0 0 0]\n\n\n\n\n\n\n\n\n\n\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n\n# 평균이 0, 분산이 1이 되도록 데이터의 스케일을 조정합니다\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n\ndbscan = DBSCAN()\nclusters = dbscan.fit_predict(X_scaled)\n# 클러스터 할당을 표시합니다\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm2, s=60, edgecolors='black')\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\n\n군집 알고리즘의 비교와 평가\n\n타겟값으로 군집 평가하기\n\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n\n# 평균이 0, 분산이 1이 되도록 데이터의 스케일을 조정합니다\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n\n# 사용할 알고리즘 모델을 리스트로 만듭니다\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n\n# 비교를 위해 무작위로 클러스터 할당을 합니다\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n\n# 무작위 할당한 클러스터를 그립니다\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60, edgecolors='black')\naxes[0].set_title(\"무작위 할당 - ARI: {:.2f}\".format(\n        adjusted_rand_score(y, random_clusters)))\n\nfor ax, algorithm in zip(axes[1:], algorithms):\n    # 클러스터 할당과 클러스터 중심을 그립니다\n    clusters = algorithm.fit_predict(X_scaled)\n    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters,\n               cmap=mglearn.cm3, s=60, edgecolors='black')\n    ax.set_title(\"{} - ARI: {:.2f}\".format(algorithm.__class__.__name__,\n                                           adjusted_rand_score(y, clusters)))\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\n\n# 포인트가 클러스터로 나뉜 두 가지 경우\nclusters1 = [0, 0, 1, 1, 0]\nclusters2 = [1, 1, 0, 0, 1]\n# 모든 레이블이 달라졌으므로 정확도는 0입니다\nprint(\"정확도: {:.2f}\".format(accuracy_score(clusters1, clusters2)))\n# 같은 포인트가 클러스터에 모였으므로 ARI는 1입니다\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(clusters1, clusters2)))\n\n정확도: 0.00\nARI: 1.00\n\n\n\n\n타겟값 없이 군집 평가하기\n\nfrom sklearn.metrics.cluster import silhouette_score\n\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n\n# 평균이 0, 분산이 1이 되도록 데이터의 스케일을 조정합니다\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n\n# 비교를 위해 무작위로 클러스터 할당을 합니다\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n\n# 무작위 할당한 클러스터를 그립니다\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60, edgecolors='black')\naxes[0].set_title(\"무작위 할당: {:.2f}\".format(\n        silhouette_score(X_scaled, random_clusters)))\n\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n\nfor ax, algorithm in zip(axes[1:], algorithms):\n    clusters = algorithm.fit_predict(X_scaled)\n    # 클러스터 할당과 클러스터 중심을 그립니다\n    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm3,\n               s=60, edgecolors='black')\n    ax.set_title(\"{} : {:.2f}\".format(algorithm.__class__.__name__,\n                                      silhouette_score(X_scaled, clusters)))\n\n\n\n\n\n\n\n\n\n\n얼굴 데이터셋으로 군집 알고리즘 비교\n\n# LFW 데이터에서 고유얼굴을 찾은 다음 데이터를 변환합니다\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=100, whiten=True, random_state=0)\nX_pca = pca.fit_transform(X_people)\n\n\n\nDBSCAN으로 얼굴 데이터셋 분석\n\n# 기본 매개변수로 DBSCAN을 적용합니다\ndbscan = DBSCAN()\nlabels = dbscan.fit_predict(X_pca)\nprint(\"고유한 레이블:\", np.unique(labels))\n\n고유한 레이블: [-1]\n\n\n\ndbscan = DBSCAN(min_samples=3)\nlabels = dbscan.fit_predict(X_pca)\nprint(\"고유한 레이블:\", np.unique(labels))\n\n고유한 레이블: [-1]\n\n\n\ndbscan = DBSCAN(min_samples=3, eps=15)\nlabels = dbscan.fit_predict(X_pca)\nprint(\"고유한 레이블:\", np.unique(labels))\n\n고유한 레이블: [-1  0]\n\n\n\n# 잡음 포인트와 클러스터에 속한 포인트 수를 셉니다.\n# bincount는 음수를 받을 수 없어서 labels에 1을 더했습니다.\n# 반환값의 첫 번째 원소는 잡음 포인트의 수입니다.\nprint(\"클러스터별 포인트 수:\", np.bincount(labels + 1))\n\n클러스터별 포인트 수: [  37 2026]\n\n\n\nnoise = X_people[labels==-1]\n\nfig, axes = plt.subplots(3, 9, subplot_kw={'xticks': (), 'yticks': ()},\n                         figsize=(12, 4))\nfor image, ax in zip(noise, axes.ravel()):\n    ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n\n\n\n\n\n\n\n\n\nfor eps in [1, 3, 5, 7, 9, 11, 13]:\n    print(\"\\neps=\", eps)\n    dbscan = DBSCAN(eps=eps, min_samples=3)\n    labels = dbscan.fit_predict(X_pca)\n    print(\"클러스터 수:\", len(np.unique(labels)))\n    print(\"클러스터 크기:\", np.bincount(labels + 1))\n\n\neps= 1\n클러스터 수: 1\n클러스터 크기: [2063]\n\neps= 3\n클러스터 수: 1\n클러스터 크기: [2063]\n\neps= 5\n클러스터 수: 2\n클러스터 크기: [2059    4]\n\neps= 7\n클러스터 수: 8\n클러스터 크기: [1954   75    4   14    6    4    3    3]\n\neps= 9\n클러스터 수: 3\n클러스터 크기: [1199  861    3]\n\neps= 11\n클러스터 수: 2\n클러스터 크기: [ 403 1660]\n\neps= 13\n클러스터 수: 2\n클러스터 크기: [ 119 1944]\n\n\n\ndbscan = DBSCAN(min_samples=3, eps=7)\nlabels = dbscan.fit_predict(X_pca)\n\nfor cluster in range(max(labels) + 1):\n    mask = labels == cluster\n    n_images =  np.sum(mask)\n    fig, axes = plt.subplots(1, 14, figsize=(14*1.5, 4),\n                             subplot_kw={'xticks': (), 'yticks': ()})\n    i = 0\n    for image, label, ax in zip(X_people[mask], y_people[mask], axes):\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n        ax.set_title(people.target_names[label].split()[-1])\n        i += 1\n    for j in range(len(axes) - i):\n        axes[j+i].imshow(np.array([[1]*65]*87), vmin=0, vmax=1)\n        axes[j+i].axis('off')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk-평균으로 얼굴 데이터셋 분석하기\n\nn_clusters = 10\n# k-평균으로 클러스터를 추출합니다\nkm = KMeans(n_clusters=n_clusters, random_state=0)\nlabels_km = km.fit_predict(X_pca)\nprint(\"k-평균의 클러스터 크기:\", np.bincount(labels_km))\n\nk-평균의 클러스터 크기: [  5 286 190 275 185 493   1 286   2 340]\n\n\n\nfig, axes = plt.subplots(2, 5, subplot_kw={'xticks': (), 'yticks': ()},\n                         figsize=(12, 4))\nfor center, ax in zip(km.cluster_centers_, axes.ravel()):\n    ax.imshow(pca.inverse_transform(center).reshape(image_shape),\n              vmin=0, vmax=1)\n\n\n\n\n\n\n\n\n\nmglearn.plots.plot_kmeans_faces(km, pca, X_pca, X_people,\n                                y_people, people.target_names)\n\n\n\n\n\n\n\n\n\n\n병합 군집으로 얼굴 데이터셋 분석하기\n\n# 병합 군집으로 클러스터를 추출합니다\nagglomerative = AgglomerativeClustering(n_clusters=10)\nlabels_agg = agglomerative.fit_predict(X_pca)\nprint(\"병합 군집의 클러스터 크기:\",\n       np.bincount(labels_agg))\n\n병합 군집의 클러스터 크기: [264 100 275 553  49  64 546  52  51 109]\n\n\n\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))\n\nARI: 0.09\n\n\n\nlinkage_array = ward(X_pca)\n# 클러스터 사이의 거리가 담겨있는 linkage_array로 덴드로그램을 그립니다\nplt.figure(figsize=(20, 5))\ndendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\nplt.xlabel(\"샘플 번호\")\nplt.ylabel(\"클러스터 거리\")\nax = plt.gca()\nbounds = ax.get_xbound()\nax.plot(bounds, [36, 36], '--', c='k')\n\n\n\n\n\n\n\n\n\nn_clusters = 10\nfor cluster in range(n_clusters):\n    mask = labels_agg == cluster\n    fig, axes = plt.subplots(1, 10, subplot_kw={'xticks': (), 'yticks': ()},\n                             figsize=(15, 8))\n    axes[0].set_ylabel(np.sum(mask))\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n                                      labels_agg[mask], axes):\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n        ax.set_title(people.target_names[label].split()[-1],\n                     fontdict={'fontsize': 9})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 병합 군집으로 클러스터를 추출합니다\nagglomerative = AgglomerativeClustering(n_clusters=40)\nlabels_agg = agglomerative.fit_predict(X_pca)\nprint(\"병합 군집의 클러스터 크기:\", np.bincount(labels_agg))\n\nn_clusters = 40\nfor cluster in [13, 16, 23, 38, 39]: # 흥미로운 클러스터 몇개를 골랐습니다\n    mask = labels_agg == cluster\n    fig, axes = plt.subplots(1, 15, subplot_kw={'xticks': (), 'yticks': ()},\n                             figsize=(15, 8))\n    cluster_size = np.sum(mask)\n    axes[0].set_ylabel(\"#{}: {}\".format(cluster, cluster_size))\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n                                      labels_agg[mask], axes):\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n        ax.set_title(people.target_names[label].split()[-1],\n                     fontdict={'fontsize': 9})\n    for i in range(cluster_size, 15):\n        axes[i].set_visible(False)\n\n병합 군집의 클러스터 크기: [139  35  23   2 111  39 106  33   5 161  60  41  70  17  30  20 134  40\n  23  38  56 264   4  35  44  16  29 135  25  37  42  34   3  17  31   3\n  21  27  76  37]",
    "crumbs": [
      "Introduction",
      "비지도 학습"
    ]
  },
  {
    "objectID": "ML01.html",
    "href": "ML01.html",
    "title": "머신러닝 소개 및 개요",
    "section": "",
    "text": "import sklearn\nfrom preamble import *\n%config InlineBackend.figure_format='retina'",
    "crumbs": [
      "Introduction",
      "머신러닝 소개 및 개요"
    ]
  },
  {
    "objectID": "ML01.html#붓꽃의-품종-분류",
    "href": "ML01.html#붓꽃의-품종-분류",
    "title": "머신러닝 소개 및 개요",
    "section": "붓꽃의 품종 분류",
    "text": "붓꽃의 품종 분류\n\n데이터 적재\n\nfrom sklearn.datasets import load_iris\niris_dataset = load_iris()\n\n\nprint(\"iris_dataset의 키:\\n\", iris_dataset.keys())\n\niris_dataset의 키:\n dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n\n\n\nprint(iris_dataset['DESCR'][:193] + \"\\n...\")\n\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n:Number of Instances: 150 (50 in each of three classes)\n:Number of Attributes: 4 numeric, predictive \n...\n\n\n\nprint(\"타깃의 이름:\", iris_dataset['target_names'])\n\n타깃의 이름: ['setosa' 'versicolor' 'virginica']\n\n\n\nprint(\"특성의 이름:\\n\", iris_dataset['feature_names'])\n\n특성의 이름:\n ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n\n\n\nprint(\"data의 타입:\", type(iris_dataset['data']))\n\ndata의 타입: &lt;class 'numpy.ndarray'&gt;\n\n\n\nprint(\"data의 크기:\", iris_dataset['data'].shape)\n\ndata의 크기: (150, 4)\n\n\n\nprint(\"data의 처음 다섯 행:\\n\", iris_dataset['data'][:5])\n\ndata의 처음 다섯 행:\n [[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.6 1.4 0.2]]\n\n\n\nprint(\"target의 타입:\", type(iris_dataset['target']))\n\ntarget의 타입: &lt;class 'numpy.ndarray'&gt;\n\n\n\nprint(\"target의 크기:\", iris_dataset['target'].shape)\n\ntarget의 크기: (150,)\n\n\n\nprint(\"타깃:\\n\", iris_dataset['target'])\n\n타깃:\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n\n\n\n\n데이터 나누기\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    iris_dataset['data'], iris_dataset['target'], random_state=0)\n\n\nprint(\"X_train 크기:\", X_train.shape)\nprint(\"y_train 크기:\", y_train.shape)\n\nX_train 크기: (112, 4)\ny_train 크기: (112,)\n\n\n\nprint(\"X_test 크기:\", X_test.shape)\nprint(\"y_test 크기:\", y_test.shape)\n\nX_test 크기: (38, 4)\ny_test 크기: (38,)\n\n\n\n\n데이터 살펴보기\n\n# X_train 데이터를 사용해서 데이터프레임을 만듭니다.\n# 열의 이름은 iris_dataset.feature_names에 있는 문자열을 사용합니다.\niris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n# 데이터프레임을 사용해 y_train에 따라 색으로 구분된 산점도 행렬을 만듭니다.\npd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o',\n                           hist_kwds={'bins': 20}, s=60, alpha=.8, cmap=mglearn.cm3)\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\n머신 러닝 모델: k-최근접 이웃\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\n\n\nknn.fit(X_train, y_train)\n\nKNeighborsClassifier(n_neighbors=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=1) \n\n\n\n\n모델 평가하기\n\ny_pred = knn.predict(X_test)\nprint(\"테스트 세트에 대한 예측값:\\n\", y_pred)\n\n테스트 세트에 대한 예측값:\n [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n 2]\n\n\n\nprint(\"테스트 세트의 정확도: {:.2f}\".format(np.mean(y_pred == y_test)))\n\n테스트 세트의 정확도: 0.97\n\n\n\nprint(\"테스트 세트의 정확도: {:.2f}\".format(knn.score(X_test, y_test)))\n\n테스트 세트의 정확도: 0.97",
    "crumbs": [
      "Introduction",
      "머신러닝 소개 및 개요"
    ]
  },
  {
    "objectID": "Appendix04.html",
    "href": "Appendix04.html",
    "title": "타이타닉 v2",
    "section": "",
    "text": "::: {#cell-1 .cell _cell_guid=‘de05512e-6991-44df-9599-da92a7e459ac’ _uuid=‘d8bdd5f0320e244e4702ed8ec1c2482b022c51cd’ execution_count=1}\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\n\nimport seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n:::\n::: {#cell-2 .cell _cell_guid=‘e0a17223-f682-45fc-89a5-667af9782bbe’ _uuid=‘7964157913fbcff581fc1929eed487708e81ac9c’ execution_count=2}\ntrain_df = pd.read_csv(\"data/titanic_train.csv\")\ntest_df = pd.read_csv(\"data/titanic_test.csv\")\ntrain_df.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n:::\n::: {#cell-3 .cell _cell_guid=‘1d969b76-ea88-4d32-a58e-f22a070258bf’ _uuid=‘bff38fcf31baf67493513c06f0c2f6e50576ff09’ execution_count=3}\ntest_df.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n\n\n\n\n:::\n::: {#cell-4 .cell _cell_guid=‘29dddd33-d995-4b0f-92ea-a361b368cc42’ _uuid=‘d4fe22ead7e187724ca6f3ba7ba0e6412ae0e874’ execution_count=4}\n# check missing values in train data\ntrain_df.isnull().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n:::\n::: {#cell-5 .cell _cell_guid=‘6d65fcfa-52bf-45ab-b959-64a32c1c1976’ _uuid=‘c6fd60f15d5e803d4dffc89e782c6fbc72445a83’ execution_count=5}\nax = train_df[\"Age\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\ntrain_df[\"Age\"].plot(kind='density', color='teal')\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()\n\n\n\n\n\n\n\n:::\n::: {#cell-6 .cell _cell_guid=‘1d70c27b-1e4d-4d5e-8a39-c134389d436c’ _uuid=‘4f13840d4f9bf1b4331523c99274aa0627485e6c’ execution_count=6}\n# mean age\nprint('The mean of \"Age\" is %.2f' %(train_df[\"Age\"].mean(skipna=True)))\n# median age\nprint('The median of \"Age\" is %.2f' %(train_df[\"Age\"].median(skipna=True)))\n\nThe mean of \"Age\" is 29.70\nThe median of \"Age\" is 28.00\n\n:::\n::: {#cell-7 .cell _cell_guid=‘1a1ad808-0a63-43ac-b757-71195880ed4f’ _uuid=‘1acbce9c6bc5d586dda3e47b7506067a85524e66’ execution_count=7}\n# percent of missing \"Cabin\" \nprint('Percent of missing \"Cabin\" records is %.2f%%' %((train_df['Cabin'].isnull().sum()/train_df.shape[0])*100))\n\nPercent of missing \"Cabin\" records is 77.10%\n\n:::\n::: {#cell-8 .cell _cell_guid=‘f21c2b55-2126-439d-8b1d-e96dafc97d81’ _uuid=‘92ab9e62fb62f2a0fb9972baf6ada444187540e6’ execution_count=8}\n# percent of missing \"Embarked\" \nprint('Percent of missing \"Embarked\" records is %.2f%%' %((train_df['Embarked'].isnull().sum()/train_df.shape[0])*100))\n\nPercent of missing \"Embarked\" records is 0.22%\n\n:::\n::: {#cell-9 .cell _cell_guid=‘22924bc4-5dfa-4df7-b0d0-de3ede9c58b7’ _uuid=‘f2a915f45264f8a580de6cc382d96b370eb75730’ execution_count=9}\nprint('Boarded passengers grouped by port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton):')\nprint(train_df['Embarked'].value_counts())\nsns.countplot(x='Embarked', data=train_df, palette='Set2')\nplt.show()\n\nBoarded passengers grouped by port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton):\nEmbarked\nS    644\nC    168\nQ     77\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n:::\n::: {#cell-10 .cell _cell_guid=‘def67427-3257-4dce-872e-7f5b4202d18a’ _uuid=‘c57a9f8a54efa382bc94b695c9664330d01709ea’ execution_count=10}\nprint('The most common boarding port of embarkation is %s.' %train_df['Embarked'].value_counts().idxmax())\n\nThe most common boarding port of embarkation is S.\n\n:::\n::: {#cell-11 .cell _cell_guid=‘bc0d7121-1008-4890-9043-07eba1524e15’ _uuid=‘feeed4b6775f88edf5de12b0ee6ee73c16eba61d’ execution_count=11}\ntrain_data = train_df.copy()\ntrain_data[\"Age\"].fillna(train_df[\"Age\"].median(skipna=True), inplace=True)\ntrain_data[\"Embarked\"].fillna(train_df['Embarked'].value_counts().idxmax(), inplace=True)\ntrain_data.drop('Cabin', axis=1, inplace=True)\n:::\n::: {#cell-12 .cell _cell_guid=‘0cfe1c08-71a6-493e-803d-db255af01697’ _uuid=‘d6be29651bb903964e02d3a7bcc7033513eb76c9’ execution_count=12}\n# check missing values in adjusted train data\ntrain_data.isnull().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nEmbarked       0\ndtype: int64\n\n:::\n::: {#cell-13 .cell _cell_guid=‘10dcfe1b-34f1-4bd8-b937-5ae8daf4a378’ _uuid=‘3ee37b1151416aeeec8ebd7b94bb0184aabc57cd’ execution_count=13}\n# preview adjusted train data\ntrain_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nS\n\n\n\n\n\n\n:::\n::: {#cell-14 .cell _cell_guid=‘dda26046-b93b-49ee-a52e-35355ecb425c’ _uuid=‘293aec20df86ef529d10ae1f051dfe921ba07b88’ execution_count=14}\nplt.figure(figsize=(15,8))\nax = train_df[\"Age\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\ntrain_df[\"Age\"].plot(kind='density', color='teal')\nax = train_data[\"Age\"].hist(bins=15, density=True, stacked=True, color='orange', alpha=0.5)\ntrain_data[\"Age\"].plot(kind='density', color='orange')\nax.legend(['Raw Age', 'Adjusted Age'])\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()\n\n\n\n\n\n\n\n:::\n::: {#cell-15 .cell _cell_guid=‘759c3c8e-8db6-41d9-a1a2-058a15b338a6’ _uuid=‘d1f5815ba663f7e8cc17d7efcff73653af5b1bdb’ execution_count=15}\n## Create categorical variable for traveling alone\ntrain_data['TravelAlone']=np.where((train_data[\"SibSp\"]+train_data[\"Parch\"])&gt;0, 0, 1)\ntrain_data.drop('SibSp', axis=1, inplace=True)\ntrain_data.drop('Parch', axis=1, inplace=True)\n:::\n::: {#cell-16 .cell _cell_guid=‘f95361e8-2533-4731-a7ab-a99cf686ed50’ _uuid=‘4494fcbf9faa90151e20042f74d73395fac3cc8e’ execution_count=16}\n#create categorical variables and drop some variables\ntraining=pd.get_dummies(train_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\ntraining.drop('Sex_female', axis=1, inplace=True)\ntraining.drop('PassengerId', axis=1, inplace=True)\ntraining.drop('Name', axis=1, inplace=True)\ntraining.drop('Ticket', axis=1, inplace=True)\n\nfinal_train = training\nfinal_train.head()\n\n\n\n\n\n\n\n\nSurvived\nAge\nFare\nTravelAlone\nPclass_1\nPclass_2\nPclass_3\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nSex_male\n\n\n\n\n0\n0\n22.0\n7.2500\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n\n\n1\n1\n38.0\n71.2833\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n2\n1\n26.0\n7.9250\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n3\n1\n35.0\n53.1000\n0\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\n0\n35.0\n8.0500\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n\n\n\n\n\n\n:::\n::: {#cell-17 .cell _cell_guid=‘501f9a53-881d-4440-9366-7aae67eb358b’ _uuid=‘d80416a026d17ccac3bf793408dd5f4f1e17bf63’ execution_count=17}\ntest_df.isnull().sum()\n\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\n:::\n::: {#cell-18 .cell _cell_guid=‘8b9ef076-3669-4339-8d10-0d8783a92e07’ _uuid=‘145675b90aa2befa533c640aaedd4bf8069b12d4’ execution_count=18}\ntest_data = test_df.copy()\ntest_data[\"Age\"].fillna(train_df[\"Age\"].median(skipna=True), inplace=True)\ntest_data[\"Fare\"].fillna(train_df[\"Fare\"].median(skipna=True), inplace=True)\ntest_data.drop('Cabin', axis=1, inplace=True)\n\ntest_data['TravelAlone']=np.where((test_data[\"SibSp\"]+test_data[\"Parch\"])&gt;0, 0, 1)\n\ntest_data.drop('SibSp', axis=1, inplace=True)\ntest_data.drop('Parch', axis=1, inplace=True)\n\ntesting = pd.get_dummies(test_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\ntesting.drop('Sex_female', axis=1, inplace=True)\ntesting.drop('PassengerId', axis=1, inplace=True)\ntesting.drop('Name', axis=1, inplace=True)\ntesting.drop('Ticket', axis=1, inplace=True)\n\nfinal_test = testing\nfinal_test.head()\n\n\n\n\n\n\n\n\nAge\nFare\nTravelAlone\nPclass_1\nPclass_2\nPclass_3\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nSex_male\n\n\n\n\n0\n34.5\n7.8292\n1\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n1\n47.0\n7.0000\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n2\n62.0\n9.6875\n1\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n3\n27.0\n8.6625\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n\n\n4\n22.0\n12.2875\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n\n\n:::\n::: {#cell-19 .cell _cell_guid=‘9f9ca9e5-50a0-4487-ba53-815dda90af1c’ _uuid=‘790e8d7ca89d19e276b3398e299c42893a796b79’ execution_count=19}\nplt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train[\"Age\"][final_train.Survived == 1], color=\"darkturquoise\", shade=True)\nsns.kdeplot(final_train[\"Age\"][final_train.Survived == 0], color=\"lightcoral\", shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Age for Surviving Population and Deceased Population')\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()\n\n\n\n\n\n\n\n:::\n::: {#cell-20 .cell _cell_guid=‘d2aa9f59-c433-4258-b8db-225b63a5eab6’ _uuid=‘9cf1794d9db2fdc314c20ca97a76e9470e81a354’ execution_count=20}\nplt.figure(figsize=(20,8))\navg_survival_byage = final_train[[\"Age\", \"Survived\"]].groupby(['Age'], as_index=False).mean()\ng = sns.barplot(x='Age', y='Survived', data=avg_survival_byage, color=\"LightSeaGreen\")\nplt.show()\n\n\n\n\n\n\n\n:::\n::: {#cell-21 .cell _cell_guid=‘1655b49b-b33f-4236-8b31-d995ef26c6f6’ _uuid=‘8918defa6e17b83c700ea45357ebd67a3a22f02f’ execution_count=21}\nfinal_train['IsMinor']=np.where(final_train['Age']&lt;=16, 1, 0)\nfinal_test['IsMinor']=np.where(final_test['Age']&lt;=16, 1, 0)\n:::\n::: {#cell-22 .cell _cell_guid=‘9f31ffe1-7cd8-4169-b193-ed44e56d0bd4’ _uuid=‘4a1c521f08460f6983eca0c4e01294fb7c86e4f9’ execution_count=22}\nplt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train[\"Fare\"][final_train.Survived == 1], color=\"darkturquoise\", shade=True)\nsns.kdeplot(final_train[\"Fare\"][final_train.Survived == 0], color=\"lightcoral\", shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Fare for Surviving Population and Deceased Population')\nax.set(xlabel='Fare')\nplt.xlim(-20,200)\nplt.show()\n\n\n\n\n\n\n\n:::\n::: {#cell-23 .cell _cell_guid=‘676548e8-6dd4-4180-800c-7b164acb3877’ _uuid=‘08fd677214959e0b938a0f8a94b63ab548673ea5’ execution_count=23}\nsns.barplot(x='Pclass', y='Survived', data=train_df, color=\"darkturquoise\")\nplt.show()\n\n\n\n\n\n\n\n:::\n::: {#cell-24 .cell _cell_guid=‘6e5bec50-2f5e-433e-9130-c56956fddad3’ _uuid=‘a9f0598701c7c5224eaa73dafa869af73beffe18’ execution_count=24}\nsns.barplot(x='Embarked', y='Survived', data=train_df, color=\"teal\")\nplt.show()\n\n\n\n\n\n\n\n:::\n::: {#cell-25 .cell _cell_guid=‘67017a88-93d4-412b-9adf-8b4d1d9b9db0’ _uuid=‘e0c3dc16292ef0bcabf0fc680d821ef654084ab4’ execution_count=25}\nsns.barplot(x='TravelAlone', y='Survived', data=final_train, color=\"mediumturquoise\")\nplt.show()\n\n\n\n\n\n\n\n:::\n::: {#cell-26 .cell _cell_guid=‘7b416e59-8616-4a44-93e1-a8005eff78a9’ _uuid=‘354794315925dff1e96229cc737eaf299aaea17a’ execution_count=26}\nsns.barplot(x='Sex', y='Survived', data=train_df, color=\"aquamarine\")\nplt.show()\n\n\n\n\n\n\n\n:::\n::: {#cell-27 .cell _cell_guid=‘11a2a468-20df-40cd-a4ba-4ae7bd2fc403’ _uuid=‘64befdf1182c2b4e845f488f5bfd0e19ce3dc17a’ execution_count=27}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n\ncols = [\"Age\",\"Fare\",\"TravelAlone\",\"Pclass_1\",\"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"] \nX = final_train[cols]\ny = final_train['Survived']\n# Build a logreg and compute the feature importances\nmodel = LogisticRegression()\n# create the RFE model and select 8 attributes\nrfe = RFE(model)\nrfe = rfe.fit(X, y)\n# summarize the selection of the attributes\nprint('Selected features: %s' % list(X.columns[rfe.support_]))\n\nSelected features: ['Pclass_1', 'Pclass_2', 'Sex_male', 'IsMinor']\n\n:::\n::: {#cell-28 .cell _cell_guid=‘7239aa6f-7fd2-4b75-a387-f6624f1c338c’ _uuid=‘53d79f38cfe33d75d6ff869a443b9a29c93b4cbd’ execution_count=28}\nfrom sklearn.feature_selection import RFECV\n# Create the RFE object and compute a cross-validated score.\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfecv = RFECV(estimator=LogisticRegression(), step=1, cv=10, scoring='accuracy')\nrfecv.fit(X, y)\nprint(\"Optimal number of features: %d\" % rfecv.n_features_)\nprint('Selected features: %s' % list(X.columns[rfecv.support_]))\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.cv_results_) + 1), rfecv.cv_results_.keys())\nplt.show()\n\nOptimal number of features: 9\nSelected features: ['Age', 'Fare', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', 'Embarked_S', 'Sex_male', 'IsMinor']\n\n\n\n\n\n\n\n\n:::\n::: {#cell-29 .cell _cell_guid=‘08986ec4-79ff-466b-b763-61bf84a0879b’ _uuid=‘3f6950a7c24c629b72e17e54c556f3c183b3f779’ execution_count=29}\nSelected_features = ['Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', \n                     'Embarked_S', 'Sex_male', 'IsMinor']\nX = final_train[Selected_features]\n\nplt.subplots(figsize=(8, 5))\nsns.heatmap(X.corr(), annot=True, cmap=\"RdYlGn\")\nplt.show()\n\n\n\n\n\n\n\n:::\n::: {#cell-30 .cell _cell_guid=‘84233f59-f3c7-4ea0-884d-96f8ad4d5b10’ _uuid=‘46336228eeb864bc82e6739768122579d1c9634c’ execution_count=30}\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n\n# create X (features) and y (response)\nX = final_train[Selected_features]\ny = final_train['Survived']\n\n# use train/test split with different random_state values\n# we can change the random_state values that changes the accuracy scores\n# the scores change a lot, this is why testing scores is a high-variance estimate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n\n# check classification scores of logistic regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\ny_pred_proba = logreg.predict_proba(X_test)[:, 1]\n[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\nprint('Train/Test split results:')\nprint(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\nprint(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba))\nprint(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n\nidx = np.min(np.where(tpr &gt; 0.95)) # index of the first threshold for which the sensibility &gt; 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))\n\nTrain/Test split results:\nLogisticRegression accuracy is 0.782\nLogisticRegression log_loss is 0.504\nLogisticRegression auc is 0.838\nUsing a threshold of 0.070 guarantees a sensitivity of 0.962 and a specificity of 0.200, i.e. a false positive rate of 80.00%.\n\n\n\n\n\n\n\n\n:::\n::: {#cell-31 .cell _cell_guid=‘32a611ae-b2b7-43e0-8fa8-3cc56e351bf6’ _uuid=‘7f0aba7b861c3fa1748060b4733778851fb00a31’ execution_count=31}\n# 10-fold cross-validation logistic regression\nlogreg = LogisticRegression()\n# Use cross_val_score function\n# We are passing the entirety of X and y, not X_train or y_train, it takes care of splitting the data\n# cv=10 for 10 folds\n# scoring = {'accuracy', 'neg_log_loss', 'roc_auc'} for evaluation metric - althought they are many\nscores_accuracy = cross_val_score(logreg, X, y, cv=10, scoring='accuracy')\nscores_log_loss = cross_val_score(logreg, X, y, cv=10, scoring='neg_log_loss')\nscores_auc = cross_val_score(logreg, X, y, cv=10, scoring='roc_auc')\nprint('K-fold cross-validation results:')\nprint(logreg.__class__.__name__+\" average accuracy is %2.3f\" % scores_accuracy.mean())\nprint(logreg.__class__.__name__+\" average log_loss is %2.3f\" % -scores_log_loss.mean())\nprint(logreg.__class__.__name__+\" average auc is %2.3f\" % scores_auc.mean())\n\nK-fold cross-validation results:\nLogisticRegression average accuracy is 0.796\nLogisticRegression average log_loss is 0.454\nLogisticRegression average auc is 0.850\n\n:::\n::: {#cell-32 .cell _cell_guid=‘9ea95aac-00b2-413e-ab86-c6b8782c40ef’ _uuid=‘90840c89d42e9284c9480a256dc259778c3a5b1b’ execution_count=32}\nfrom sklearn.model_selection import cross_validate\n\nscoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n\nmodelCV = LogisticRegression()\n\nresults = cross_validate(modelCV, X, y, cv=10, scoring=list(scoring.values()), \n                         return_train_score=False)\n\nprint('K-fold cross-validation results:')\nfor sc in range(len(scoring)):\n    print(modelCV.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n                               if list(scoring.values())[sc]=='neg_log_loss' \n                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n                               results['test_%s' % list(scoring.values())[sc]].std()))\n\nK-fold cross-validation results:\nLogisticRegression average accuracy: 0.796 (+/-0.024)\nLogisticRegression average log_loss: 0.454 (+/-0.037)\nLogisticRegression average auc: 0.850 (+/-0.028)\n\n:::\n::: {#cell-33 .cell _cell_guid=‘80b5d5c3-ead3-48f7-84eb-42c801e0370a’ _uuid=‘0fdf4a257a49b0b47552a4ed307c86852c4ed271’ execution_count=33}\ncols = [\"Age\",\"Fare\",\"TravelAlone\",\"Pclass_1\",\"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"]\nX = final_train[cols]\n\nscoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n\nmodelCV = LogisticRegression()\n\nresults = cross_validate(modelCV, final_train[cols], y, cv=10, scoring=list(scoring.values()), \n                         return_train_score=False)\n\nprint('K-fold cross-validation results:')\nfor sc in range(len(scoring)):\n    print(modelCV.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n                               if list(scoring.values())[sc]=='neg_log_loss' \n                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n                               results['test_%s' % list(scoring.values())[sc]].std()))\n\nK-fold cross-validation results:\nLogisticRegression average accuracy: 0.797 (+/-0.028)\nLogisticRegression average log_loss: 0.455 (+/-0.037)\nLogisticRegression average auc: 0.849 (+/-0.028)\n\n:::\n::: {#cell-34 .cell _cell_guid=‘4a39cb49-b446-4ffa-88a2-e37b627eb5e5’ _uuid=‘765695a9712d3fe1ecff10f17dcc077a80ed7682’ execution_count=34}\nfrom sklearn.model_selection import GridSearchCV\n\nX = final_train[Selected_features]\n\nparam_grid = {'C': np.arange(1e-05, 3, 0.1)}\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\n\ngs = GridSearchCV(LogisticRegression(), return_train_score=True,\n                  param_grid=param_grid, scoring=scoring, cv=10, refit='Accuracy')\n\ngs.fit(X, y)\nresults = gs.cv_results_\n\nprint('='*20)\nprint(\"best params: \" + str(gs.best_estimator_))\nprint(\"best params: \" + str(gs.best_params_))\nprint('best score:', gs.best_score_)\nprint('='*20)\n\nplt.figure(figsize=(10, 10))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n\nplt.xlabel(\"Inverse of regularization strength: C\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, param_grid['C'].max()) \nax.set_ylim(0.35, 0.95)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_C'].data, dtype=float)\n\nfor scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n        \n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show()\n\n====================\nbest params: LogisticRegression(C=2.4000100000000004)\nbest params: {'C': 2.4000100000000004}\nbest score: 0.8069662921348316\n====================\n\n\n\n\n\n\n\n\n:::\n::: {#cell-35 .cell _cell_guid=‘a777c645-7413-4e1d-855a-0b416be1b48e’ _uuid=‘514d0e3aabe57aaad38d10d6101dbdac5af89ca4’ execution_count=35}\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\n\n#Define simple model\n###############################################################################\nC = np.arange(1e-05, 5.5, 0.1)\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\nlog_reg = LogisticRegression()\n\n#Simple pre-processing estimators\n###############################################################################\nstd_scale = StandardScaler(with_mean=False, with_std=False)\n#std_scale = StandardScaler()\n\n#Defining the CV method: Using the Repeated Stratified K Fold\n###############################################################################\n\nn_folds=5\nn_repeats=5\n\nrskfold = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=2)\n\n#Creating simple pipeline and defining the gridsearch\n###############################################################################\nlog_clf_pipe = Pipeline(steps=[('scale',std_scale), ('clf',log_reg)])\nlog_clf = GridSearchCV(estimator=log_clf_pipe, cv=rskfold,\n              scoring=scoring, return_train_score=True,\n              param_grid=dict(clf__C=C), refit='Accuracy')\nlog_clf.fit(X, y)\nresults = log_clf.cv_results_\nprint('='*20)\nprint(\"best params: \" + str(log_clf.best_estimator_))\nprint(\"best params: \" + str(log_clf.best_params_))\nprint('best score:', log_clf.best_score_)\nprint('='*20)\n\nplt.figure(figsize=(10, 10))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n\nplt.xlabel(\"Inverse of regularization strength: C\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, C.max()) \nax.set_ylim(0.35, 0.95)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_clf__C'].data, dtype=float)\n\nfor scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n        \n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show()\n\n====================\nbest params: Pipeline(steps=[('scale', StandardScaler(with_mean=False, with_std=False)),\n                ('clf', LogisticRegression(C=4.10001))])\nbest params: {'clf__C': 4.10001}\nbest score: 0.7995505617977527\n====================\n\n\n\n\n\n\n\n\n:::\n::: {#cell-36 .cell _cell_guid=‘e52c3bfb-4325-4c27-9f8e-5514dae799c2’ _uuid=‘b3d12ca129d816d2434e74ed6574f829f826c3fc’ execution_count=36}\n# final_test['Survived'] = log_clf.predict(final_test[Selected_features])\n# final_test['PassengerId'] = test_df['PassengerId']\n# submission = final_test[['PassengerId','Survived']]\n# submission.to_csv(\"submission.csv\", index=False)\n# submission.tail()\n:::",
    "crumbs": [
      "Practice",
      "타이타닉 v2"
    ]
  },
  {
    "objectID": "Appendix02.html",
    "href": "Appendix02.html",
    "title": "KOSPI와 연관된 지수는?",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport koreanize_matplotlib\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import KMeans\n\nimport FinanceDataReader as fdr\nimport cvxopt as opt\nfrom cvxopt import solvers",
    "crumbs": [
      "Practice",
      "KOSPI와 연관된 지수는?"
    ]
  },
  {
    "objectID": "Appendix02.html#세계-각국의-지수들-관련-데이터-불러오기",
    "href": "Appendix02.html#세계-각국의-지수들-관련-데이터-불러오기",
    "title": "KOSPI와 연관된 지수는?",
    "section": "세계 각국의 지수들 관련 데이터 불러오기",
    "text": "세계 각국의 지수들 관련 데이터 불러오기\n\nKOSPI\n\nchange(수익률)에 * 100을 해서 %수치로 보이기 편하게 수정하였고, Open(시가), High(고가), Close(종가), Volume(거래량)을 제거\n\n\nkospi = fdr.DataReader('KS11')\nkospi['Change'] = kospi['Change'] * 100\nkospi = kospi.drop(['Open', 'High', 'Low', 'Close', 'Volume', 'UpDown', 'Comp', 'Amount', 'MarCap'], axis = 1)\nkospi.rename(columns = {'Change':'KOSPI'},inplace=True)\nkospi.head()\n\n\n\n\n\n\n\n\nKOSPI\n\n\nDate\n\n\n\n\n\n2001-06-11\n-2.18\n\n\n2001-06-12\n-0.18\n\n\n2001-06-13\n1.14\n\n\n2001-06-14\n-0.05\n\n\n2001-06-15\n0.85\n\n\n\n\n\n\n\n\n\nNasdaq\n\nnasdaq = fdr.DataReader('IXIC', '2001-06-08')\nnasdaq['Change'] = round(nasdaq['Adj Close'].pct_change() *100, 2)\nnasdaq = nasdaq.drop(['High', 'Low', 'Open', 'Close', 'Volume', 'Adj Close'], axis = 1)\nnasdaq = nasdaq.drop('2001-06-08', axis = 0)\nnasdaq.rename(columns = {'Change':'NASDAQ'},inplace=True)\nnasdaq\n\n\n\n\n\n\n\n\nNASDAQ\n\n\nDate\n\n\n\n\n\n2001-06-11\n-2.00\n\n\n2001-06-12\n-0.04\n\n\n2001-06-13\n-2.23\n\n\n2001-06-14\n-3.66\n\n\n2001-06-15\n-0.77\n\n\n...\n...\n\n\n2024-01-22\n0.32\n\n\n2024-01-23\n0.43\n\n\n2024-01-24\n0.36\n\n\n2024-01-25\n0.18\n\n\n2024-01-26\n0.11\n\n\n\n\n5693 rows × 1 columns\n\n\n\n\n\nS&P\n\nsap = fdr.DataReader('S&P500', '2001-06-08', '2024-01-23')\nsap['Change'] = round(sap['Adj Close'].pct_change() * 100, 2)\nsap = sap.drop(['High', 'Low', 'Open', 'Close', 'Volume', 'Adj Close'], axis = 1)\nsap = sap.drop('2001-06-08', axis = 0)\nsap.rename(columns = {'Change':'S&P'},inplace=True)\nsap.head()\n\n\n\n\n\n\n\n\nS&P\n\n\nDate\n\n\n\n\n\n2001-06-11\n-0.84\n\n\n2001-06-12\n0.12\n\n\n2001-06-13\n-1.13\n\n\n2001-06-14\n-1.75\n\n\n2001-06-15\n-0.45\n\n\n\n\n\n\n\n\n\n미국 10년 국채\n\nusa_treasury = fdr.DataReader('US10YT', '2001-06-08', '2024-01-23')\nusa_treasury['Change'] = round(usa_treasury['Adj Close'].pct_change() * 100, 2)\nusa_treasury = usa_treasury.drop(['High', 'Low', 'Open', 'Close', 'Volume', 'Adj Close'], axis = 1)\nusa_treasury = usa_treasury.drop('2001-06-08', axis = 0)\nusa_treasury = usa_treasury.rename(columns = {'Change':'USA_TREASURY'})\nusa_treasury\n\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_28696\\2197024751.py:2: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n  usa_treasury['Change'] = round(usa_treasury['Adj Close'].pct_change() * 100, 2)\n\n\n\n\n\n\n\n\n\nUSA_TREASURY\n\n\nDate\n\n\n\n\n\n2001-06-10\n0.00\n\n\n2001-06-11\n-0.86\n\n\n2001-06-12\n-0.89\n\n\n2001-06-13\n0.32\n\n\n2001-06-14\n-0.80\n\n\n...\n...\n\n\n2024-01-17\n0.98\n\n\n2024-01-18\n0.93\n\n\n2024-01-19\n0.05\n\n\n2024-01-21\n0.00\n\n\n2024-01-22\n-1.25\n\n\n\n\n6929 rows × 1 columns\n\n\n\n\n\nNikkei\n\njapan = fdr.DataReader('N225', '2001-06-08', '2024-01-23')\njapan['Change'] = round(japan['Adj Close'].pct_change() * 100, 2)\njapan = japan.drop(['High', 'Low', 'Open', 'Close', 'Volume', 'Adj Close'], axis = 1)\njapan = japan.drop('2001-06-08', axis = 0)\njapan.rename(columns = {'Change':'Nikkei'},inplace=True)\njapan\n\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_28696\\3872993107.py:2: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n  japan['Change'] = round(japan['Adj Close'].pct_change() * 100, 2)\n\n\n\n\n\n\n\n\n\nNikkei\n\n\nDate\n\n\n\n\n\n2001-06-11\n-1.52\n\n\n2001-06-12\n-2.92\n\n\n2001-06-13\n-0.13\n\n\n2001-06-14\n0.18\n\n\n2001-06-15\n-0.44\n\n\n...\n...\n\n\n2024-01-16\n-0.79\n\n\n2024-01-17\n-0.40\n\n\n2024-01-18\n-0.03\n\n\n2024-01-19\n1.40\n\n\n2024-01-22\n1.62\n\n\n\n\n5641 rows × 1 columns\n\n\n\n\nusd_krw = fdr.DataReader('USD/KRW', '2001-06-08')\nusd_krw['Change'] = round(usd_krw['Adj Close'].pct_change() * 100, 2)\nusd_krw = usd_krw.drop(['High', 'Low', 'Open', 'Close', 'Volume', 'Adj Close'], axis = 1)\nusd_krw = usd_krw.rename(columns = {'Change':'USD/KRW'})\nusd_krw\n\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_28696\\4191296131.py:2: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n  usd_krw['Change'] = round(usd_krw['Adj Close'].pct_change() * 100, 2)\n\n\n\n\n\n\n\n\n\nUSD/KRW\n\n\nDate\n\n\n\n\n\n2003-12-01\nNaN\n\n\n2003-12-02\n-0.28\n\n\n2003-12-03\n-0.21\n\n\n2003-12-04\n-0.20\n\n\n2003-12-05\n-0.45\n\n\n...\n...\n\n\n2024-01-22\n-0.33\n\n\n2024-01-23\n0.41\n\n\n2024-01-24\n0.00\n\n\n2024-01-25\n0.00\n\n\n2024-01-26\n-0.32\n\n\n\n\n5260 rows × 1 columns",
    "crumbs": [
      "Practice",
      "KOSPI와 연관된 지수는?"
    ]
  },
  {
    "objectID": "Appendix02.html#데이터-병합",
    "href": "Appendix02.html#데이터-병합",
    "title": "KOSPI와 연관된 지수는?",
    "section": "데이터 병합",
    "text": "데이터 병합\n각 지수들 및 환율 국채를 합쳐줍니다. 결측치는 평균으로 대체하였습니다.\n\nresult_data = pd.concat([kospi, nasdaq, sap, usd_krw, usa_treasury, japan], axis = 1)\nresult_data.fillna(result_data.mean(), inplace = True)\nresult_data = round(result_data, 2)\n\n\nresult_data.to_csv('./data/kospi_result_data.csv')",
    "crumbs": [
      "Practice",
      "KOSPI와 연관된 지수는?"
    ]
  },
  {
    "objectID": "Appendix02.html#군집분석",
    "href": "Appendix02.html#군집분석",
    "title": "KOSPI와 연관된 지수는?",
    "section": "군집분석",
    "text": "군집분석",
    "crumbs": [
      "Practice",
      "KOSPI와 연관된 지수는?"
    ]
  },
  {
    "objectID": "Appendix02.html#k_means-를-사용하여-군집을-분류하였습니다.",
    "href": "Appendix02.html#k_means-를-사용하여-군집을-분류하였습니다.",
    "title": "KOSPI와 연관된 지수는?",
    "section": "K_Means 를 사용하여 군집을 분류하였습니다.",
    "text": "K_Means 를 사용하여 군집을 분류하였습니다.\n\nkospi 를 target으로 설정하였습니다\n\n\nX = result_data[[\"NASDAQ\",\"S&P\",\"USD/KRW\",\"USA_TREASURY\",\"Nikkei\"]]\ny = result_data[\"KOSPI\"]\n\n\n\\(N\\) = \\(\\{3,4\\}\\)일때 최적의 분류를 확인\n\nks = range(1,10)\n\ninertias = []\n\nfor k in ks:\n    model = KMeans(n_clusters=k,random_state=42)\n    model.fit(X)\n    inertias.append(model.inertia_)\n\nplt.plot(ks, inertias, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show() \n\n\n\n\n\n\n\n\n\n\n\\(N\\) = \\(3\\)로 군집분석 진행\n\nn_clusters = 3\nkmeans = KMeans(n_clusters=n_clusters,random_state=42)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\ny_kmeans[1:10]\n\narray([2, 2, 1, 2, 2, 2, 2, 2, 1])\n\n\n\n\nTSNE를 통한 차원축소 진행\n\nimport time\nfrom sklearn.manifold import TSNE\n\nn_sne = X.shape[0]\n\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=1, perplexity=32, n_iter=1000,random_state=0,angle=0.5)\ntsne_results = tsne.fit_transform(X)\nprint( 't-SNE done! Time elapsed: {} seconds'.format(time.time() - time_start ))\n\n[t-SNE] Computing 97 nearest neighbors...\n[t-SNE] Indexed 7086 samples in 0.002s...\n[t-SNE] Computed neighbors for 7086 samples in 0.169s...\n[t-SNE] Computed conditional probabilities for sample 1000 / 7086\n[t-SNE] Computed conditional probabilities for sample 2000 / 7086\n[t-SNE] Computed conditional probabilities for sample 3000 / 7086\n[t-SNE] Computed conditional probabilities for sample 4000 / 7086\n[t-SNE] Computed conditional probabilities for sample 5000 / 7086\n[t-SNE] Computed conditional probabilities for sample 6000 / 7086\n[t-SNE] Computed conditional probabilities for sample 7000 / 7086\n[t-SNE] Computed conditional probabilities for sample 7086 / 7086\n[t-SNE] Mean sigma: 0.000000\n[t-SNE] KL divergence after 250 iterations with early exaggeration: 77.912430\n[t-SNE] KL divergence after 1000 iterations: 1.677088\nt-SNE done! Time elapsed: 11.547185897827148 seconds\n\n\n\n\n군집분석 결과 시각화\n\nplt.scatter(tsne_results[:,0], tsne_results[:,1], c=y_kmeans, s=20, cmap='bwr')\nsns.scatterplot(x=tsne_results[:,0], y=tsne_results[:,1], hue=y_kmeans, palette='bwr')\nplt.show()\n\n\n\n\n\n\n\n\n\ndf1 = result_data.copy()\n\n\ndf1['cluster'] = y_kmeans\ndf1\n\n\n\n\n\n\n\n\nKOSPI\nNASDAQ\nS&P\nUSD/KRW\nUSA_TREASURY\nNikkei\ncluster\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2001-06-11\n-2.18\n-2.00\n-0.84\n0.01\n-0.86\n-1.52\n1\n\n\n2001-06-12\n-0.18\n-0.04\n0.12\n0.01\n-0.89\n-2.92\n2\n\n\n2001-06-13\n1.14\n-2.23\n-1.13\n0.01\n0.32\n-0.13\n2\n\n\n2001-06-14\n-0.05\n-3.66\n-1.75\n0.01\n-0.80\n0.18\n1\n\n\n2001-06-15\n0.85\n-0.77\n-0.45\n0.01\n0.25\n-0.44\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-12-24\n0.03\n0.04\n0.03\n0.01\n0.00\n0.03\n2\n\n\n2023-12-31\n0.03\n0.04\n0.03\n0.01\n0.00\n0.03\n2\n\n\n2024-01-07\n0.03\n0.04\n0.03\n0.01\n0.00\n0.03\n2\n\n\n2024-01-14\n0.03\n0.04\n0.03\n0.01\n0.00\n0.03\n2\n\n\n2024-01-21\n0.03\n0.04\n0.03\n0.01\n0.00\n0.03\n2\n\n\n\n\n7086 rows × 7 columns\n\n\n\n\n\n군집분석 결과에 따른 군집들을 새로운 데이터 프레임으로 생성\n\n## 군집 0 ~ 3 까지 분류된거를 변수명에 추가\ncluster_2 = df1[df1['cluster']==2]\ncluster_1 = df1[df1['cluster']==1]\ncluster_0 = df1[df1['cluster']==0]\n\n\ncluster_0\n\n\n\n\n\n\n\n\nKOSPI\nNASDAQ\nS&P\nUSD/KRW\nUSA_TREASURY\nNikkei\ncluster\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2001-06-26\n-1.56\n0.67\n-0.15\n0.01\n1.79\n0.64\n0\n\n\n2001-06-28\n-0.25\n2.44\n1.25\n0.01\n1.78\n-1.16\n0\n\n\n2001-09-04\n3.13\n-1.92\n-0.06\n0.01\n3.38\n3.49\n0\n\n\n2001-10-11\n2.70\n4.62\n1.52\n0.01\n1.63\n3.83\n0\n\n\n2001-11-02\n1.19\n-0.03\n0.29\n0.01\n3.06\n0.35\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2022-06-06\n0.03\n0.40\n0.31\n0.78\n2.74\n0.56\n0\n\n\n2023-05-01\n0.03\n-0.11\n-0.04\n-0.17\n3.53\n0.92\n0\n\n\n2023-05-05\n0.03\n2.25\n1.85\n-1.13\n2.83\n0.03\n0\n\n\n2023-10-02\n0.03\n0.67\n0.01\n0.30\n2.41\n-0.31\n0\n\n\n2023-10-03\n0.03\n-1.87\n-1.37\n0.17\n2.54\n-1.64\n0\n\n\n\n\n995 rows × 7 columns\n\n\n\n\ncluster_1\n\n\n\n\n\n\n\n\nKOSPI\nNASDAQ\nS&P\nUSD/KRW\nUSA_TREASURY\nNikkei\ncluster\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2001-06-11\n-2.18\n-2.00\n-0.84\n0.01\n-0.86\n-1.52\n1\n\n\n2001-06-14\n-0.05\n-3.66\n-1.75\n0.01\n-0.80\n0.18\n1\n\n\n2001-06-22\n0.60\n-1.16\n-0.95\n0.01\n-1.04\n0.63\n1\n\n\n2001-07-06\n-2.54\n-3.65\n-2.35\n0.01\n-0.70\n-2.39\n1\n\n\n2001-07-10\n-0.25\n-3.15\n-1.44\n0.01\n-1.27\n0.50\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2020-08-17\n0.03\n1.00\n0.27\n0.16\n-3.67\n-0.83\n1\n\n\n2021-08-16\n0.03\n-0.20\n0.26\n-0.06\n-3.08\n-1.62\n1\n\n\n2021-09-20\n0.03\n-2.19\n-1.70\n0.57\n-4.45\n0.03\n1\n\n\n2021-10-04\n0.03\n-2.14\n-1.30\n-0.31\n0.14\n-1.13\n1\n\n\n2022-03-01\n0.03\n-1.59\n-1.55\n0.28\n-7.18\n1.20\n1\n\n\n\n\n1181 rows × 7 columns\n\n\n\n\ncluster_2\n\n\n\n\n\n\n\n\nKOSPI\nNASDAQ\nS&P\nUSD/KRW\nUSA_TREASURY\nNikkei\ncluster\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2001-06-12\n-0.18\n-0.04\n0.12\n0.01\n-0.89\n-2.92\n2\n\n\n2001-06-13\n1.14\n-2.23\n-1.13\n0.01\n0.32\n-0.13\n2\n\n\n2001-06-15\n0.85\n-0.77\n-0.45\n0.01\n0.25\n-0.44\n2\n\n\n2001-06-18\n-1.64\n-1.96\n-0.49\n0.01\n0.25\n-0.72\n2\n\n\n2001-06-19\n0.02\n0.20\n0.34\n0.01\n-0.25\n-0.97\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-12-24\n0.03\n0.04\n0.03\n0.01\n0.00\n0.03\n2\n\n\n2023-12-31\n0.03\n0.04\n0.03\n0.01\n0.00\n0.03\n2\n\n\n2024-01-07\n0.03\n0.04\n0.03\n0.01\n0.00\n0.03\n2\n\n\n2024-01-14\n0.03\n0.04\n0.03\n0.01\n0.00\n0.03\n2\n\n\n2024-01-21\n0.03\n0.04\n0.03\n0.01\n0.00\n0.03\n2\n\n\n\n\n4910 rows × 7 columns\n\n\n\n\n\n군집별 수익률 확인\n\n군집 0 ~ 3 까지의 수익률을 한번 확인\n\n\ncluster_total= df1.groupby('cluster').mean()\ncluster_total['return'] = cluster_total.T.mean()\ncluster_total['std'] = cluster_total.T.std()\ncluster_total = cluster_total.T\ncluster_total\n\n\n\n\n\n\n\ncluster\n0\n1\n2\n\n\n\n\nKOSPI\n0.305276\n-0.370711\n0.074682\n\n\nNASDAQ\n0.798905\n-1.309001\n0.215778\n\n\nS&P\n0.709920\n-1.130449\n0.172595\n\n\nUSD/KRW\n-0.014442\n0.094522\n-0.009715\n\n\nUSA_TREASURY\n3.434774\n-2.606080\n-0.036902\n\n\nNikkei\n0.363940\n-0.446105\n0.074513\n\n\nreturn\n0.933062\n-0.961304\n0.081825\n\n\nstd\n1.150517\n0.874051\n0.090151\n\n\n\n\n\n\n\n\n군집1 : 위험자산 + USA_TREASURY로 이뤄진 군집\n\n군집2 : Dolllar로 이뤄진 군집\n\n군집3 : 위험자산으로 이뤄진 군집\n\n\n\n군집별 수익률 대비 risk 시각화\n\nplt.figure(figsize=(8,6))\nplt.scatter(cluster_total.std(), cluster_total.mean())\nplt.xlabel('risk')\nplt.ylabel('return')\nplt.title('평균수익률 및 표준편차')\nfor label, x, y in zip(cluster_total.columns, cluster_total.std(), cluster_total.mean()):\n    plt.annotate(label, xy=(x, y), xytext=(30, -30),\n    textcoords = 'offset points',\n    ha = 'right', va = 'bottom',\n    bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),\n    arrowprops = dict(arrowstyle = '-&gt;', connectionstyle = 'arc3,rad=0'))   \n\n\n\n\n\n\n\n\n각 금융자산의 특징과 잘 맞게 군집이 잘 작성되었습니다.",
    "crumbs": [
      "Practice",
      "KOSPI와 연관된 지수는?"
    ]
  },
  {
    "objectID": "Appendix02.html#상관관계-분석",
    "href": "Appendix02.html#상관관계-분석",
    "title": "KOSPI와 연관된 지수는?",
    "section": "상관관계 분석",
    "text": "상관관계 분석\n\nKospi와 나머지 변수들의 상관관계를 확인\n\n\n히트맵\n\ndf2 = df1.corr()\n\n\nplt.figure(figsize=(10,10))\nsns.heatmap(result_data.corr(), annot=True, fmt = '.2f', linewidths=.5, cmap='Blues')\n\n\n\n\n\n\n\n\n\n\n상관관계 계수 확인하기\n\n확실히 Nasdaq과 S&P가 서로 미국시장이라서 상관관계가 높은것을 확인할 수 있고, kospi와의 상관관계는 1.Nikkei &gt; 2. Nasdaq &gt; 3. S&P &gt; 4.usa_treasury &gt; 5.Dollar 순의 상관관계를 보임\n\n\nidx, vals = [], []\nfor ix, i in enumerate(result_data.columns.values):\n    for j in result_data.columns.values[ix + 1:]:\n        idx.append((i, j))\n        vals.append(result_data.corr()[i][j])\n\nser = pd.Series(data=vals, index=idx)\nser_ord = ser.sort_values(ascending=False)\nser_ord\n\n(NASDAQ, S&P)              0.938372\n(KOSPI, Nikkei)            0.576190\n(S&P, USA_TREASURY)        0.320329\n(NASDAQ, USA_TREASURY)     0.273523\n(KOSPI, S&P)               0.192238\n(KOSPI, NASDAQ)            0.182981\n(S&P, Nikkei)              0.150417\n(NASDAQ, Nikkei)           0.140276\n(KOSPI, USA_TREASURY)      0.113130\n(USA_TREASURY, Nikkei)     0.105009\n(USD/KRW, USA_TREASURY)    0.003050\n(NASDAQ, USD/KRW)         -0.063353\n(S&P, USD/KRW)            -0.083819\n(USD/KRW, Nikkei)         -0.186367\n(KOSPI, USD/KRW)          -0.213738\ndtype: float64\n\n\n\n\n산점도 그래프\n\nsns.set(font_scale=1.1) ## 폰트사이즈 조절\nsns.set_style('ticks') ## 축 눈금 표시\ndata = result_data[[\"KOSPI\", \"NASDAQ\",\"S&P\",\"USD/KRW\",\"USA_TREASURY\",\"Nikkei\"]]\nsns.pairplot(data,diag_kind=None)\nplt.show()",
    "crumbs": [
      "Practice",
      "KOSPI와 연관된 지수는?"
    ]
  },
  {
    "objectID": "Appendix02.html#세계-각국-지수의-수익률-대비-risk-시각화",
    "href": "Appendix02.html#세계-각국-지수의-수익률-대비-risk-시각화",
    "title": "KOSPI와 연관된 지수는?",
    "section": "세계 각국 지수의 수익률 대비 risk 시각화",
    "text": "세계 각국 지수의 수익률 대비 risk 시각화\n\n평균수익률 및 표준편차\n\nplt.figure(figsize=(8,6))\nplt.yticks(fontname = \"DejaVu Sans\") # 한글 폰트가 지수에 음수를 표시하지 못하므로 ytick의 폰트를 바꾸어 줍니다.\nplt.scatter(result_data.std(), result_data.mean())\nplt.xlabel('risk')\nplt.ylabel('return')\nplt.title('평균수익률 및 표준편차')\nfor label, x, y in zip(result_data.columns, result_data.std(), result_data.mean()):\n    plt.annotate(label, xy=(x, y), xytext=(30, -30),\n    textcoords = 'offset points',\n    ha = 'right', va = 'bottom',\n    bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),\n    arrowprops = dict(arrowstyle = '-&gt;', connectionstyle = 'arc3,rad=0'))   \n\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\events.py:82: UserWarning: Glyph 54217 (\\N{HANGUL SYLLABLE PYEONG}) missing from current font.\n  func(*args, **kwargs)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\events.py:82: UserWarning: Glyph 44512 (\\N{HANGUL SYLLABLE GYUN}) missing from current font.\n  func(*args, **kwargs)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\events.py:82: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from current font.\n  func(*args, **kwargs)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\events.py:82: UserWarning: Glyph 51061 (\\N{HANGUL SYLLABLE IG}) missing from current font.\n  func(*args, **kwargs)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\events.py:82: UserWarning: Glyph 47456 (\\N{HANGUL SYLLABLE RYUL}) missing from current font.\n  func(*args, **kwargs)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\events.py:82: UserWarning: Glyph 48143 (\\N{HANGUL SYLLABLE MIC}) missing from current font.\n  func(*args, **kwargs)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\events.py:82: UserWarning: Glyph 54364 (\\N{HANGUL SYLLABLE PYO}) missing from current font.\n  func(*args, **kwargs)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\events.py:82: UserWarning: Glyph 51456 (\\N{HANGUL SYLLABLE JUN}) missing from current font.\n  func(*args, **kwargs)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\events.py:82: UserWarning: Glyph 54200 (\\N{HANGUL SYLLABLE PYEON}) missing from current font.\n  func(*args, **kwargs)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\events.py:82: UserWarning: Glyph 52264 (\\N{HANGUL SYLLABLE CA}) missing from current font.\n  func(*args, **kwargs)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 54217 (\\N{HANGUL SYLLABLE PYEONG}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 44512 (\\N{HANGUL SYLLABLE GYUN}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 51061 (\\N{HANGUL SYLLABLE IG}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 47456 (\\N{HANGUL SYLLABLE RYUL}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 48143 (\\N{HANGUL SYLLABLE MIC}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 54364 (\\N{HANGUL SYLLABLE PYO}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 51456 (\\N{HANGUL SYLLABLE JUN}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 54200 (\\N{HANGUL SYLLABLE PYEON}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 52264 (\\N{HANGUL SYLLABLE CA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\n\n\n누적수익률 및 표준편차\n\nplt.figure(figsize=(8,6))\nplt.yticks(fontname = \"DejaVu Sans\") # 한글 폰트가 지수에 음수를 표시하지 못하므로 ytick의 폰트를 바꾸어 줍니다.\nplt.scatter(result_data.std(), result_data.sum())\nplt.xlabel('risk')\nplt.ylabel('return')\nplt.title('누적수익률 및 표준편차')\nfor label, x, y in zip(result_data.columns, result_data.std(), result_data.sum()):\n    plt.annotate(label, xy=(x, y), xytext=(30, -30),\n    textcoords = 'offset points',\n    ha = 'right', va = 'bottom',\n    bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),\n    arrowprops = dict(arrowstyle = '-&gt;', connectionstyle = 'arc3,rad=0'))   \n\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\events.py:82: UserWarning: Glyph 45572 (\\N{HANGUL SYLLABLE NU}) missing from current font.\n  func(*args, **kwargs)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\events.py:82: UserWarning: Glyph 51201 (\\N{HANGUL SYLLABLE JEOG}) missing from current font.\n  func(*args, **kwargs)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 45572 (\\N{HANGUL SYLLABLE NU}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 51201 (\\N{HANGUL SYLLABLE JEOG}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)",
    "crumbs": [
      "Practice",
      "KOSPI와 연관된 지수는?"
    ]
  },
  {
    "objectID": "Appendix01.html",
    "href": "Appendix01.html",
    "title": "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport koreanize_matplotlib\n\nfrom scipy.stats import skew\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor",
    "crumbs": [
      "Practice",
      "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)"
    ]
  },
  {
    "objectID": "Appendix01.html#데이터-읽어오기",
    "href": "Appendix01.html#데이터-읽어오기",
    "title": "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)",
    "section": "데이터 읽어오기",
    "text": "데이터 읽어오기\n\ndata = pd.read_csv('data/housing.csv')\ndata.head()\n\n\ndata.columns.values\n\n\ndata.info()",
    "crumbs": [
      "Practice",
      "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)"
    ]
  },
  {
    "objectID": "Appendix01.html#전처리nan-missing-value",
    "href": "Appendix01.html#전처리nan-missing-value",
    "title": "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)",
    "section": "전처리(NaN, Missing Value)",
    "text": "전처리(NaN, Missing Value)\n\ndata.isnull().sum()\n\n\ndata.describe()\n\n\n한 블록 내의 최대 침실이 6445개이고 평균 침실이 537임을 알 수 있습니다.\n데이터가 왜곡된 것 같으므로 히스토그램을 통해 이를 확인하겠습니다.\n\n\nplt.figure(figsize= (10, 6))\nsns.histplot(data['total_bedrooms'], color = '#005b96', kde= True);\n\n\n확실히 왜곡되어 있으므로 누락된 값은 블록 내 객실 수 중앙값으로 채웁니다.\n\n\ndata['total_bedrooms'] = data['total_bedrooms'].fillna(data['total_bedrooms'].median())",
    "crumbs": [
      "Practice",
      "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)"
    ]
  },
  {
    "objectID": "Appendix01.html#eda",
    "href": "Appendix01.html#eda",
    "title": "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)",
    "section": "EDA",
    "text": "EDA\n\nplt.figure(figsize= (20, 8))\nsns.heatmap(data.corr(numeric_only=True), annot= True, cmap='YlGnBu')\nplt.show()\n\n\n중위소득은 분명 가장 중요한 특징입니다.\n\n\nsns.histplot(data['median_house_value'], color = '#005b96', kde= True);\n\n\ndata['median_house_value'].skew()\n\n\n우리의 목표 변수는 분명히 왜곡되어 있습니다. 따라서 로그 변환을 늦게 적용할 것입니다.\n\n\ndata.hist(bins = 30, figsize=(20, 15), color = '#005b96');\n\n많은 기능이 왜곡되어 있다는 것을 분명히 알 수 있습니다. 따라서 나중에 기능 변환을 수행할 때 이 문제를 해결해야 할 것입니다.\n\ngrid = sns.PairGrid(data, vars=['total_rooms', 'housing_median_age', 'median_income', 'median_house_value'],\n                    height=2, aspect = 2)\ngrid = grid.map_diag(plt.hist)\ngrid = grid.map_lower(sns.regplot, scatter_kws = {'s': 15, 'alpha': 0.7, 'color': '#005b96'}, \n                      line_kws = {'color':'orange', 'linewidth': 2})\ngrid = grid.map_upper(sns.kdeplot, n_levels = 10, cmap= 'coolwarm', fill = True)\nplt.show()\n\n어떻게 들여다봐도 문제가 많은 데이터..?, 범주형 변수도 같이 확인해보죠\n\nsns.countplot(x = data['ocean_proximity']);\n\n\ndata.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n        s=data[\"population\"]/100, label=\"population\", figsize=(15,8),\n        c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),colorbar=True,\n    )\nplt.legend()\nplt.show()\n\n\n특성공학\n\ndata['bed_per_room'] = data['total_bedrooms'] / data['total_rooms']\n\n\nX = data.drop(['median_house_value'], axis=1)\ny = np.log(data.median_house_value) # 로그 변환",
    "crumbs": [
      "Practice",
      "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)"
    ]
  },
  {
    "objectID": "Appendix01.html#특성-변환",
    "href": "Appendix01.html#특성-변환",
    "title": "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)",
    "section": "특성 변환",
    "text": "특성 변환\n\nskew_df = pd.DataFrame(X.select_dtypes(np.number).columns, columns= ['Feature'])\nskew_df['Skew'] = skew_df['Feature'].apply(lambda feature: skew(X[feature]))\nskew_df['Abs_Skew'] = skew_df['Skew'].apply(abs)\nskew_df['Skewed'] = skew_df['Abs_Skew'].apply(lambda x: True if x &gt; 0.5 else False)\nskew_df\n\n\nskewed_columns = skew_df[skew_df['Abs_Skew'] &gt; 0.5]['Feature'].values\nskewed_columns\n\n\nfor column in skewed_columns:\n    X[column] = np.log(X[column])\n\n\nEncoding\n\nencoder=LabelEncoder()\nX['ocean_proximity']=encoder.fit_transform(X['ocean_proximity'])\n\n\n\nScaling\n\nX.head()\n\n\nscaler = StandardScaler()\nscaler.fit(X)\nX = pd.DataFrame(scaler.transform(X), index= X.index, columns= X.columns)",
    "crumbs": [
      "Practice",
      "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)"
    ]
  },
  {
    "objectID": "Appendix01.html#데이터-나누기",
    "href": "Appendix01.html#데이터-나누기",
    "title": "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)",
    "section": "데이터 나누기",
    "text": "데이터 나누기\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 42)",
    "crumbs": [
      "Practice",
      "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)"
    ]
  },
  {
    "objectID": "Appendix01.html#모델-생성",
    "href": "Appendix01.html#모델-생성",
    "title": "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)",
    "section": "모델 생성",
    "text": "모델 생성\n\nLinear Regression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\npredictions_lr = lr.predict(X_test)\n\n\nrmse = np.sqrt(mean_squared_error(y_test, predictions_lr))\nr2 = r2_score(y_test, predictions_lr)\n\nprint('RMSE:', rmse)\nprint('R-square:', r2)\n\n\n\nKNN\n\nknn = KNeighborsRegressor()\nknn.fit(X_train, y_train)\npredictions_knn = knn.predict(X_test)\n\n\nrmse = np.sqrt(mean_squared_error(y_test, predictions_knn))\nr2 = r2_score(y_test, predictions_knn)\n\nprint('RMSE:', rmse)\nprint('R-square:', r2)\n\n\n\nRandom Forest\n\nrf = RandomForestRegressor(n_estimators= 100)\nrf.fit(X_train, y_train)\npredictions_rf = rf.predict(X_test)\n\n\nrmse = np.sqrt(mean_squared_error(y_test, predictions_rf))\nr2 = r2_score(y_test, predictions_rf)\n\nprint('RMSE:', rmse)\nprint('R-square:', r2)\n\n\n\nCatBoost\n\ncatboost = CatBoostRegressor(verbose= 0)\ncatboost.fit(X_train, y_train)\npredictions_cb = catboost.predict(X_test)\n\n\nrmse = np.sqrt(mean_squared_error(y_test, predictions_cb))\nr2 = r2_score(y_test, predictions_cb)\n\nprint('RMSE:', rmse)\nprint('R-square:', r2)\n\n\n\nXGBoost\n\nxgboost = XGBRegressor()\nxgboost.fit(X_train, y_train)\npredictions_xgb = xgboost.predict(X_test)\n\n\nrmse = np.sqrt(mean_squared_error(y_test, predictions_xgb))\nr2 = r2_score(y_test, predictions_xgb)\n\nprint('RMSE:', rmse)\nprint('R-square:', r2)\n\n\n\nLightGBM\n\nlgb = LGBMRegressor()\nlgb.fit(X_train, y_train)\npredictions_lgb = lgb.predict(X_test)\n\n\nrmse = np.sqrt(mean_squared_error(y_test, predictions_lgb))\nr2 = r2_score(y_test, predictions_lgb)\n\nprint('RMSE:', rmse)\nprint('R-square:', r2)\n\n\n\nGradient Boosting\n\ngbr = GradientBoostingRegressor()\ngbr.fit(X_train, y_train)\npredictions_gbr = gbr.predict(X_test)\n\n\nrmse = np.sqrt(mean_squared_error(y_test, predictions_gbr))\nr2 = r2_score(y_test, predictions_gbr)\n\nprint('RMSE:', rmse)\nprint('R-square:', r2)",
    "crumbs": [
      "Practice",
      "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)"
    ]
  },
  {
    "objectID": "Appendix01.html#모델-결정",
    "href": "Appendix01.html#모델-결정",
    "title": "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)",
    "section": "모델 결정",
    "text": "모델 결정\nCatBoost, XGBoost, LightGBM, RandomForest 성능은 다중 선형 회귀, knn, Gradient Boosting 능가하는 것으로 나타났습니다. 이제 이 네 가지 모델을 결합하여 최종 예측을 해보겠습니다.\n\nfinal_predictions = (\n    0.25 * predictions_cb+\n    0.25 * predictions_rf+\n    0.25 * predictions_xgb+\n    0.25 * predictions_lgb\n)\n\n\nrmse = np.sqrt(mean_squared_error(y_test, final_predictions))\nr2 = r2_score(y_test, final_predictions)\n\nprint('RMSE:', rmse)\nprint('R-square:', r2)\n\n\nfinal_predictions\n\n최종 예측을 원래 규모로 되돌리려면 최종 예측의 지수를 취해야 합니다.\n\nfinal_predictions = np.exp(final_predictions)\ny_test = np.exp(y_test)\n\n\npd.DataFrame({'Actual': y_test, 'Predicted': final_predictions.round(2)})",
    "crumbs": [
      "Practice",
      "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)"
    ]
  },
  {
    "objectID": "Appendix01.html#결과-확인",
    "href": "Appendix01.html#결과-확인",
    "title": "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)",
    "section": "결과 확인",
    "text": "결과 확인\n\nplt.figure(figsize= (10, 6))\nsns.scatterplot(x= y_test, y= final_predictions, color= '#005b96')\nplt.xlabel('Actual House value')\nplt.ylabel('Predicted House Value')\nplt.show()\n\n\nplt.figure(figsize= (10, 6))\nsns.residplot(x= y_test, y = final_predictions, color= '#005b96')\nplt.show()\n\n\nresid = y_test - final_predictions\nplt.figure(figsize= (10, 6))\nsns.histplot(resid)\nplt.xlabel('Error');\n\n오류의 분포가 정상적으로 보이기 때문에 우리 모델이 제대로 작동하고 있는 것입니다.",
    "crumbs": [
      "Practice",
      "다양한 모델로 살펴보는 캘리포니아 집값 예측(회귀)"
    ]
  },
  {
    "objectID": "Appendix03.html",
    "href": "Appendix03.html",
    "title": "타이타닉 생존자 예측",
    "section": "",
    "text": "::: {#cell-1 .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2021-03-26T09:30:19.361826Z”,“iopub.status.busy”:“2021-03-26T09:30:19.360602Z”,“iopub.status.idle”:“2021-03-26T09:30:19.367551Z”,“shell.execute_reply”:“2021-03-26T09:30:19.366426Z”}’ papermill=‘{“duration”:0.036522,“end_time”:“2021-03-26T09:30:19.367861”,“exception”:false,“start_time”:“2021-03-26T09:30:19.331339”,“status”:“completed”}’ tags=‘[]’ execution_count=1}\nimport numpy as np\nimport pandas as pd \n:::\n\ntrain = pd.read_csv(\"data/titanic_train.csv\")\ntest = pd.read_csv(\"data/titanic_test.csv\")\ntrain.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\ntest.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n\n\n\n\n\n\nprint(train.isnull().sum())\nprint(test.isnull().sum())\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\n\n\ntrain[\"Age\"]=train[\"Age\"].fillna(train[\"Age\"].median())\ntest[\"Age\"]=test[\"Age\"].fillna(test[\"Age\"].median())\ntest[\"Fare\"]=test[\"Fare\"].fillna(test[\"Fare\"].median())\ntrain[\"Embarked\"]=train[\"Embarked\"].fillna(\"S\")\n\n\nprint(train.isnull().sum())\nprint(test.isnull().sum())\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge              0\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         0\ndtype: int64\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge              0\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          327\nEmbarked         0\ndtype: int64\n\n\n\ntrain[\"Sex\"][train[\"Sex\"] == \"male\"] = 0\ntrain[\"Sex\"][train[\"Sex\"] == \"female\"] = 1\ntest[\"Sex\"][test[\"Sex\"] == \"male\"] = 0\ntest[\"Sex\"][test[\"Sex\"] == \"female\"] = 1\n\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\97970295.py:1: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  train[\"Sex\"][train[\"Sex\"] == \"male\"] = 0\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\97970295.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train[\"Sex\"][train[\"Sex\"] == \"male\"] = 0\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\97970295.py:2: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  train[\"Sex\"][train[\"Sex\"] == \"female\"] = 1\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\97970295.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train[\"Sex\"][train[\"Sex\"] == \"female\"] = 1\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\97970295.py:3: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  test[\"Sex\"][test[\"Sex\"] == \"male\"] = 0\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\97970295.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[\"Sex\"][test[\"Sex\"] == \"male\"] = 0\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\97970295.py:4: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  test[\"Sex\"][test[\"Sex\"] == \"female\"] = 1\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\97970295.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[\"Sex\"][test[\"Sex\"] == \"female\"] = 1\n\n\n\ntrain[\"Embarked\"][train[\"Embarked\"]== \"S\"] = 0\ntrain[\"Embarked\"][train[\"Embarked\"]== \"C\"] =1\ntrain[\"Embarked\"][train[\"Embarked\"]== \"Q\"] =2\ntest[\"Embarked\"][test[\"Embarked\"]== \"S\"] = 0\ntest[\"Embarked\"][test[\"Embarked\"]== \"C\"] =1\ntest[\"Embarked\"][test[\"Embarked\"]== \"Q\"] =2\n\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\1241293558.py:1: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  train[\"Embarked\"][train[\"Embarked\"]== \"S\"] = 0\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\1241293558.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train[\"Embarked\"][train[\"Embarked\"]== \"S\"] = 0\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\1241293558.py:2: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  train[\"Embarked\"][train[\"Embarked\"]== \"C\"] =1\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\1241293558.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train[\"Embarked\"][train[\"Embarked\"]== \"C\"] =1\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\1241293558.py:3: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  train[\"Embarked\"][train[\"Embarked\"]== \"Q\"] =2\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\1241293558.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train[\"Embarked\"][train[\"Embarked\"]== \"Q\"] =2\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\1241293558.py:4: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  test[\"Embarked\"][test[\"Embarked\"]== \"S\"] = 0\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\1241293558.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[\"Embarked\"][test[\"Embarked\"]== \"S\"] = 0\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\1241293558.py:5: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  test[\"Embarked\"][test[\"Embarked\"]== \"C\"] =1\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\1241293558.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[\"Embarked\"][test[\"Embarked\"]== \"C\"] =1\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\1241293558.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  test[\"Embarked\"][test[\"Embarked\"]== \"Q\"] =2\nC:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_19968\\1241293558.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[\"Embarked\"][test[\"Embarked\"]== \"Q\"] =2\n\n\n\ny = train.loc[:,[\"Survived\"]]\nx = train.loc[:,[\"Sex\",\"Age\",\"Pclass\",\"SibSp\",\"Fare\",\"Embarked\"]]\nprint(x,y)\n\n    Sex   Age  Pclass  SibSp     Fare Embarked\n0     0  22.0       3      1   7.2500        0\n1     1  38.0       1      1  71.2833        1\n2     1  26.0       3      0   7.9250        0\n3     1  35.0       1      1  53.1000        0\n4     0  35.0       3      0   8.0500        0\n..   ..   ...     ...    ...      ...      ...\n886   0  27.0       2      0  13.0000        0\n887   1  19.0       1      0  30.0000        0\n888   1  28.0       3      1  23.4500        0\n889   0  26.0       1      0  30.0000        1\n890   0  32.0       3      0   7.7500        2\n\n[891 rows x 6 columns]      Survived\n0           0\n1           1\n2           1\n3           1\n4           0\n..        ...\n886         0\n887         1\n888         0\n889         1\n890         0\n\n[891 rows x 1 columns]\n\n\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,shuffle=True,random_state=0)\nprint(x_train,y_train)\nprint(x_test,y_test)\n\n    Sex   Age  Pclass  SibSp     Fare Embarked\n857   0  51.0       1      0  26.5500        0\n52    1  49.0       1      1  76.7292        1\n386   0   1.0       3      5  46.9000        0\n124   0  54.0       1      0  77.2875        0\n578   1  28.0       3      1  14.4583        1\n..   ..   ...     ...    ...      ...      ...\n835   1  39.0       1      1  83.1583        1\n192   1  19.0       3      1   7.8542        0\n629   0  28.0       3      0   7.7333        2\n559   1  36.0       3      1  17.4000        0\n684   0  60.0       2      1  39.0000        0\n\n[623 rows x 6 columns]      Survived\n857         1\n52          1\n386         0\n124         0\n578         0\n..        ...\n835         1\n192         1\n629         0\n559         1\n684         0\n\n[623 rows x 1 columns]\n    Sex   Age  Pclass  SibSp      Fare Embarked\n495   0  28.0       3      0   14.4583        1\n648   0  28.0       3      0    7.5500        0\n278   0   7.0       3      4   29.1250        2\n31    1  28.0       1      1  146.5208        1\n255   1  29.0       3      0   15.2458        1\n..   ..   ...     ...    ...       ...      ...\n263   0  40.0       1      0    0.0000        0\n718   0  28.0       3      0   15.5000        2\n620   0  27.0       3      1   14.4542        1\n786   1  18.0       3      0    7.4958        0\n64    0  28.0       1      0   27.7208        1\n\n[268 rows x 6 columns]      Survived\n495         0\n648         0\n278         0\n31          1\n255         1\n..        ...\n263         0\n718         0\n620         0\n786         1\n64          0\n\n[268 rows x 1 columns]\n\n\n\nx_train = x_train.astype(\"float32\")\ny_train = y_train.astype(\"float32\")\nx_test = x_test.astype(\"float32\")\ny_test = y_test.astype(\"float32\")\n\n\nparams = {\"objective\": [\"binary:logistic\"],\n          \"n_estimators\":[50000],\n          \"booster\":[\"gbtree\"],\n          \"eta\":[0.01],\n          \"max_depth\": [0,1,2,3,4,5,6,7,8,9,10],\n          \"min_child_weight\":[0,1,2,3,4,5,6,7,8,9,10],\n          \"random_state\":[0],\n          \"colsample_bytree\":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n          \"subsample\":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n          \"alpha\":[0,1,2,3,4,5],\n          \"lambda\":[0,1,2,3,4,5]}\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb\nclf = xgb.XGBClassifier()\nclf_grid = RandomizedSearchCV(clf,params,cv=5,n_iter=50,random_state=0,scoring=\"accuracy\")\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nclf_grid.fit(x_train,y_train,\n        early_stopping_rounds=50,\n        eval_set = [(x_test,y_test)],\n        eval_metric = \"error\",\n        verbose = 0)\n\nRandomizedSearchCV(cv=5,\n                   estimator=XGBClassifier(base_score=None, booster=None,\n                                           callbacks=None,\n                                           colsample_bylevel=None,\n                                           colsample_bynode=None,\n                                           colsample_bytree=None, device=None,\n                                           early_stopping_rounds=None,\n                                           enable_categorical=False,\n                                           eval_metric=None, feature_types=None,\n                                           gamma=None, grow_policy=None,\n                                           importance_type=None,\n                                           interaction_constraints=None,\n                                           learning_rate...\n                                        'booster': ['gbtree'],\n                                        'colsample_bytree': [0.1, 0.2, 0.3, 0.4,\n                                                             0.5, 0.6, 0.7, 0.8,\n                                                             0.9, 1.0],\n                                        'eta': [0.01],\n                                        'lambda': [0, 1, 2, 3, 4, 5],\n                                        'max_depth': [0, 1, 2, 3, 4, 5, 6, 7, 8,\n                                                      9, 10],\n                                        'min_child_weight': [0, 1, 2, 3, 4, 5,\n                                                             6, 7, 8, 9, 10],\n                                        'n_estimators': [50000],\n                                        'objective': ['binary:logistic'],\n                                        'random_state': [0],\n                                        'subsample': [0.1, 0.2, 0.3, 0.4, 0.5,\n                                                      0.6, 0.7, 0.8, 0.9,\n                                                      1.0]},\n                   random_state=0, scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(cv=5,\n                   estimator=XGBClassifier(base_score=None, booster=None,\n                                           callbacks=None,\n                                           colsample_bylevel=None,\n                                           colsample_bynode=None,\n                                           colsample_bytree=None, device=None,\n                                           early_stopping_rounds=None,\n                                           enable_categorical=False,\n                                           eval_metric=None, feature_types=None,\n                                           gamma=None, grow_policy=None,\n                                           importance_type=None,\n                                           interaction_constraints=None,\n                                           learning_rate...\n                                        'booster': ['gbtree'],\n                                        'colsample_bytree': [0.1, 0.2, 0.3, 0.4,\n                                                             0.5, 0.6, 0.7, 0.8,\n                                                             0.9, 1.0],\n                                        'eta': [0.01],\n                                        'lambda': [0, 1, 2, 3, 4, 5],\n                                        'max_depth': [0, 1, 2, 3, 4, 5, 6, 7, 8,\n                                                      9, 10],\n                                        'min_child_weight': [0, 1, 2, 3, 4, 5,\n                                                             6, 7, 8, 9, 10],\n                                        'n_estimators': [50000],\n                                        'objective': ['binary:logistic'],\n                                        'random_state': [0],\n                                        'subsample': [0.1, 0.2, 0.3, 0.4, 0.5,\n                                                      0.6, 0.7, 0.8, 0.9,\n                                                      1.0]},\n                   random_state=0, scoring='accuracy') estimator: XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...) XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...) \n\n\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nprint(clf_grid.best_params_)\nprint(clf_grid.best_score_)\n\n{'subsample': 0.5, 'random_state': 0, 'objective': 'binary:logistic', 'n_estimators': 50000, 'min_child_weight': 4, 'max_depth': 10, 'lambda': 4, 'eta': 0.01, 'colsample_bytree': 0.7, 'booster': 'gbtree', 'alpha': 1}\n0.7912\n\n\n\nbst = clf_grid.best_estimator_\nprint(bst)\n\nXGBClassifier(alpha=1, base_score=None, booster='gbtree', callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.7, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eta=0.01, eval_metric=None,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None, lambda=4,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n              max_leaves=None, min_child_weight=4, missing=nan,\n              monotone_constraints=None, multi_strategy=None,\n              n_estimators=50000, ...)\n\n\n\npred_1 = bst.predict(x_test)\ngrid_score= accuracy_score(y_test,pred_1)\nprint(grid_score)\n\n0.7985074626865671\n\n\n\nconfusion_matrix(y_test,pred_1)\n\narray([[167,   1],\n       [ 53,  47]], dtype=int64)\n\n\n\nimport graphviz\nfrom IPython.display import Image, display_png\ngraph = xgb.to_graphviz(bst)\ngraph.format = \"png\"\ngraph.render(\"tree\")\ndisplay_png(Image(\"tree.png\"))\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\ndef plot_feature_importances(model):\n    n_features = x.shape[1]\n    plt.barh(range(n_features),model.feature_importances_,align=\"center\")\n    plt.yticks(np.arange(n_features),x)\n    plt.xlabel(\"importance\")\n    plt.ylabel(\"features\")\n    plt.show\nplot_feature_importances(bst)\nplt.savefig(\"Features Importances\")\n\n\n\n\n\n\n\n\n\nX= test.loc[:,[\"Sex\",\"Age\",\"Pclass\",\"SibSp\",\"Fare\",\"Embarked\"]]\nX = X.astype(\"float32\")\nY= bst.predict(X)\nY = np.array(Y,dtype = \"int64\")\nprint(Y)\n\n[0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n 0 0 0 0 1 0 0 1 0 0 0]\n\n\n\nPassengerId = np.array(test[\"PassengerId\"]).astype(int)\nresult = pd.DataFrame(Y,PassengerId,columns = [\"Survived\"])\nprint(result)\n\n      Survived\n892          0\n893          0\n894          0\n895          0\n896          0\n...        ...\n1305         0\n1306         1\n1307         0\n1308         0\n1309         0\n\n[418 rows x 1 columns]\n\n\n\nresult.to_csv(\"my_submission.csv\",index_label = [\"PassengerId\"])\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!",
    "crumbs": [
      "Practice",
      "타이타닉 생존자 예측"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "머신러닝 강의 노트 v0.7",
    "section": "",
    "text": "본 노트는 PNU에서 진행된 머신러닝 강의(1차, 2024.01.15 ~ 2024.01.27)에 사용된 강의 노트 입니다. 해당 강의는 “파이썬 라이브러리를 활용한 머신러닝 (번역개정2판), 세라 가이도, 안드레아스 뮐러 ,박해선, 한빛미디어”를 사용하였습니다."
  },
  {
    "objectID": "ML02.html",
    "href": "ML02.html",
    "title": "지도 학습",
    "section": "",
    "text": "from preamble import *\nimport koreanize_matplotlib\n%config InlineBackend.figure_format='retina'",
    "crumbs": [
      "Introduction",
      "지도 학습"
    ]
  },
  {
    "objectID": "ML02.html#지도-학습-알고리즘",
    "href": "ML02.html#지도-학습-알고리즘",
    "title": "지도 학습",
    "section": "지도 학습 알고리즘",
    "text": "지도 학습 알고리즘\n\n# 데이터셋을 만듭니다\nX, y = mglearn.datasets.make_forge()\n# 산점도를 그립니다\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.legend([\"클래스 0\", \"클래스 1\"], loc=4)\nplt.xlabel(\"첫 번째 특성\")\nplt.ylabel(\"두 번째 특성\")\nprint(\"X.shape:\", X.shape)\nplt.show() # 책에는 없음\n\nX.shape: (26, 2)\n\n\n\n\n\n\n\n\n\n\nX, y = mglearn.datasets.make_wave(n_samples=40)\nplt.plot(X, y, 'o')\nplt.ylim(-3, 3)\nplt.xlabel(\"특성\")\nplt.ylabel(\"타깃\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nprint(\"cancer.keys():\\n\", cancer.keys())\n\ncancer.keys():\n dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n\n\n\nprint(\"유방암 데이터의 형태:\", cancer.data.shape)\n\n유방암 데이터의 형태: (569, 30)\n\n\n\nprint(\"클래스별 샘플 갯수:\\n\",\n      {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))})\n\n클래스별 샘플 갯수:\n {'malignant': 212, 'benign': 357}\n\n\n\nprint(\"특성 이름:\\n\", cancer.feature_names)\n\n특성 이름:\n ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n 'mean smoothness' 'mean compactness' 'mean concavity'\n 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n 'radius error' 'texture error' 'perimeter error' 'area error'\n 'smoothness error' 'compactness error' 'concavity error'\n 'concave points error' 'symmetry error' 'fractal dimension error'\n 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n 'worst smoothness' 'worst compactness' 'worst concavity'\n 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n\n\n\nX, y = mglearn.datasets.load_extended_boston()\nprint(\"X.shape:\", X.shape)\n\nX.shape: (506, 104)\n\n\n\nk-최근접 이웃\n\nk-최근접 이웃 분류\n\nmglearn.plots.plot_knn_classification(n_neighbors=1)\n\n\n\n\n\n\n\n\n\nmglearn.plots.plot_knn_classification(n_neighbors=3)\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX, y = mglearn.datasets.make_forge()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)\n\n\nclf.fit(X_train, y_train)\n\nKNeighborsClassifier(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=3) \n\n\n\nprint(\"테스트 세트 예측:\", clf.predict(X_test))\n\n테스트 세트 예측: [1 0 1 0 1 0 0]\n\n\n\nprint(\"테스트 세트 정확도: {:.2f}\".format(clf.score(X_test, y_test)))\n\n테스트 세트 정확도: 0.86\n\n\n\n\nk-최근접 이웃 분류(KNeighborsClassifier) 분석\n\nfig, axes = plt.subplots(1, 3, figsize=(10, 3))\n\nfor n_neighbors, ax in zip([1, 3, 9], axes):\n    # fit 메소드는 self 오브젝트를 리턴합니다\n    # 그래서 객체 생성과 fit 메소드를 한 줄에 쓸 수 있습니다\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    ax.set_title(\"{} 이웃\".format(n_neighbors))\n    ax.set_xlabel(\"특성 0\")\n    ax.set_ylabel(\"특성 1\")\naxes[0].legend(loc=3)\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, stratify=cancer.target, random_state=66)\n\ntraining_accuracy = []\ntest_accuracy = []\n# 1 에서 10 까지 n_neighbors 를 적용\nneighbors_settings = range(1, 11)\n\nfor n_neighbors in neighbors_settings:\n    # 모델 생성\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    clf.fit(X_train, y_train)\n    # 훈련 세트 정확도 저장\n    training_accuracy.append(clf.score(X_train, y_train))\n    # 일반화 정확도 저장\n    test_accuracy.append(clf.score(X_test, y_test))\n\nplt.plot(neighbors_settings, training_accuracy, label=\"훈련 정확도\")\nplt.plot(neighbors_settings, test_accuracy, label=\"테스트 정확도\")\nplt.ylabel(\"정확도\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\nk-최근접 이웃 회귀\n\nmglearn.plots.plot_knn_regression(n_neighbors=1)\n\n\n\n\n\n\n\n\n\nmglearn.plots.plot_knn_regression(n_neighbors=3)\n\n\n\n\n\n\n\n\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nX, y = mglearn.datasets.make_wave(n_samples=40)\n\n# wave 데이터셋을 훈련 세트와 테스트 세트로 나눕니다\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# 이웃의 수를 3으로 하여 모델의 객체를 만듭니다\nreg = KNeighborsRegressor(n_neighbors=3)\n# 훈련 데이터와 타깃을 사용하여 모델을 학습시킵니다\nreg.fit(X_train, y_train)\n\nKNeighborsRegressor(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsRegressor?Documentation for KNeighborsRegressoriFittedKNeighborsRegressor(n_neighbors=3) \n\n\n\nprint(\"테스트 세트 예측:\\n\", reg.predict(X_test))\n\n테스트 세트 예측:\n [-0.054  0.357  1.137 -1.894 -1.139 -1.631  0.357  0.912 -0.447 -1.139]\n\n\n\nprint(\"테스트 세트 R^2: {:.2f}\".format(reg.score(X_test, y_test)))\n\n테스트 세트 R^2: 0.83\n\n\n\n\nk-최근접 이웃 회귀(KNeighborsRegressor) 분석\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n# -3 과 3 사이에 1,000 개의 데이터 포인트를 만듭니다\nline = np.linspace(-3, 3, 1000).reshape(-1, 1)\nfor n_neighbors, ax in zip([1, 3, 9], axes):\n    # 1, 3, 9 이웃을 사용한 예측을 합니다\n    reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n    reg.fit(X_train, y_train)\n    ax.plot(line, reg.predict(line))\n    ax.plot(X_train, y_train, '^', c=mglearn.cm2(0), markersize=8)\n    ax.plot(X_test, y_test, 'v', c=mglearn.cm2(1), markersize=8)\n\n    ax.set_title(\n        \"{} 이웃의 훈련 스코어: {:.2f} 테스트 스코어: {:.2f}\".format(\n            n_neighbors, reg.score(X_train, y_train), reg.score(X_test, y_test)))\n    ax.set_xlabel(\"특성\")\n    ax.set_ylabel(\"타깃\")\naxes[0].legend([\"모델 예측\", \"훈련 데이터/타깃\", \"테스트 데이터/타깃\"], loc=\"best\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\n\n선형 모델\n\n회귀의 선형 모델\n\nmglearn.plots.plot_linear_regression_wave()\n\nw[0]: 0.393906  b: -0.031804\n\n\n\n\n\n\n\n\n\n\n선형 회귀(최소제곱법)\n\nfrom sklearn.linear_model import LinearRegression\nX, y = mglearn.datasets.make_wave(n_samples=60)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nlr = LinearRegression().fit(X_train, y_train)\n\n\nprint(\"lr.coef_:\", lr.coef_)\nprint(\"lr.intercept_:\", lr.intercept_)\n\nlr.coef_: [0.394]\nlr.intercept_: -0.031804343026759746\n\n\n\nprint(\"훈련 세트 점수: {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"테스트 세트 점수: {:.2f}\".format(lr.score(X_test, y_test)))\n\n훈련 세트 점수: 0.67\n테스트 세트 점수: 0.66\n\n\n\nX, y = mglearn.datasets.load_extended_boston()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nlr = LinearRegression().fit(X_train, y_train)\n\n\nprint(\"훈련 세트 점수: {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"테스트 세트 점수: {:.2f}\".format(lr.score(X_test, y_test)))\n\n훈련 세트 점수: 0.95\n테스트 세트 점수: 0.61\n\n\n\n\n리지 회귀\n\nfrom sklearn.linear_model import Ridge\n\nridge = Ridge().fit(X_train, y_train)\nprint(\"훈련 세트 점수: {:.2f}\".format(ridge.score(X_train, y_train)))\nprint(\"테스트 세트 점수: {:.2f}\".format(ridge.score(X_test, y_test)))\n\n훈련 세트 점수: 0.89\n테스트 세트 점수: 0.75\n\n\n\nridge10 = Ridge(alpha=10).fit(X_train, y_train)\nprint(\"훈련 세트 점수: {:.2f}\".format(ridge10.score(X_train, y_train)))\nprint(\"테스트 세트 점수: {:.2f}\".format(ridge10.score(X_test, y_test)))\n\n훈련 세트 점수: 0.79\n테스트 세트 점수: 0.64\n\n\n\nridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\nprint(\"훈련 세트 점수: {:.2f}\".format(ridge01.score(X_train, y_train)))\nprint(\"테스트 세트 점수: {:.2f}\".format(ridge01.score(X_test, y_test)))\n\n훈련 세트 점수: 0.93\n테스트 세트 점수: 0.77\n\n\n\nplt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\nplt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\")\nplt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\n\nplt.plot(lr.coef_, 'o', label=\"LinearRegression\")\nplt.xlabel(\"계수 목록\")\nplt.ylabel(\"계수 크기\")\nxlims = plt.xlim()\nplt.hlines(0, xlims[0], xlims[1])\nplt.xlim(xlims)\nplt.ylim(-25, 25)\nplt.legend()\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nmglearn.plots.plot_ridge_n_samples()\n\n\n\n\n\n\n\n\n\n\n라소\n\nfrom sklearn.linear_model import Lasso\n\nlasso = Lasso().fit(X_train, y_train)\nprint(\"훈련 세트 점수: {:.2f}\".format(lasso.score(X_train, y_train)))\nprint(\"테스트 세트 점수: {:.2f}\".format(lasso.score(X_test, y_test)))\nprint(\"사용한 특성의 개수:\", np.sum(lasso.coef_ != 0))\n\n훈련 세트 점수: 0.29\n테스트 세트 점수: 0.21\n사용한 특성의 개수: 4\n\n\n\n# max_iter 기본 값을 증가시키지 않으면 max_iter 값을 늘이라는 경고가 발생합니다\nlasso001 = Lasso(alpha=0.01, max_iter=50000).fit(X_train, y_train)\nprint(\"훈련 세트 점수: {:.2f}\".format(lasso001.score(X_train, y_train)))\nprint(\"테스트 세트 점수: {:.2f}\".format(lasso001.score(X_test, y_test)))\nprint(\"사용한 특성의 개수:\", np.sum(lasso001.coef_ != 0))\n\n훈련 세트 점수: 0.90\n테스트 세트 점수: 0.77\n사용한 특성의 개수: 33\n\n\n\nlasso00001 = Lasso(alpha=0.0001, max_iter=50000).fit(X_train, y_train)\nprint(\"훈련 세트 점수: {:.2f}\".format(lasso00001.score(X_train, y_train)))\nprint(\"테스트 세트 점수: {:.2f}\".format(lasso00001.score(X_test, y_test)))\nprint(\"사용한 특성의 개수:\", np.sum(lasso00001.coef_ != 0))\n\n훈련 세트 점수: 0.95\n테스트 세트 점수: 0.64\n사용한 특성의 개수: 96\n\n\n\nplt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\")\nplt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\nplt.plot(lasso00001.coef_, 'v', label=\"Lasso alpha=0.0001\")\n\nplt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\nplt.legend(ncol=2, loc=(0, 1.05))\nplt.ylim(-25, 25)\nplt.xlabel(\"계수 목록\")\nplt.ylabel(\"계수 크기\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\nQuantileRegressor\n\nfrom sklearn.linear_model import QuantileRegressor\n\nX, y = mglearn.datasets.make_wave(n_samples=60)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\npred_up = QuantileRegressor(quantile=0.9, alpha=0.01).fit(X_train, y_train).predict(X_test)\npred_med = QuantileRegressor(quantile=0.5, alpha=0.01).fit(X_train, y_train).predict(X_test)\npred_low = QuantileRegressor(quantile=0.1, alpha=0.01).fit(X_train, y_train).predict(X_test)\n\nplt.scatter(X_train, y_train, label='훈련 데이터')\nplt.scatter(X_test, y_test, label='테스트 데이터')\nplt.plot(X_test, pred_up, label='백분위:0.9')\nplt.plot(X_test, pred_med, label='백분위:0.5')\nplt.plot(X_test, pred_low, label='백분위:0.1')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n분류용 선형 모델\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\n\nX, y = mglearn.datasets.make_forge()\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))\n\nfor model, ax in zip([LinearSVC(max_iter=5000, dual=\"auto\"), LogisticRegression()], axes):\n    clf = model.fit(X, y)\n    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5, ax=ax, alpha=.7)\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    ax.set_title(clf.__class__.__name__)\n    ax.set_xlabel(\"특성 0\")\n    ax.set_ylabel(\"특성 1\")\naxes[0].legend()\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nmglearn.plots.plot_linear_svc_regularization()\n\n\n\n\n\n\n\n\n\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\nlogreg = LogisticRegression(max_iter=5000).fit(X_train, y_train)\nprint(\"훈련 세트 점수: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"테스트 세트 점수: {:.3f}\".format(logreg.score(X_test, y_test)))\n\n훈련 세트 점수: 0.958\n테스트 세트 점수: 0.958\n\n\n\nlogreg100 = LogisticRegression(C=100, max_iter=50000).fit(X_train, y_train)\nprint(\"훈련 세트 점수: {:.3f}\".format(logreg100.score(X_train, y_train)))\nprint(\"테스트 세트 점수: {:.3f}\".format(logreg100.score(X_test, y_test)))\n\n훈련 세트 점수: 0.981\n테스트 세트 점수: 0.965\n\n\n\nlogreg001 = LogisticRegression(C=0.01, max_iter=50000).fit(X_train, y_train)\nprint(\"훈련 세트 점수: {:.3f}\".format(logreg001.score(X_train, y_train)))\nprint(\"테스트 세트 점수: {:.3f}\".format(logreg001.score(X_test, y_test)))\n\n훈련 세트 점수: 0.953\n테스트 세트 점수: 0.951\n\n\n\nplt.plot(logreg100.coef_.T, '^', label=\"C=100\")\nplt.plot(logreg.coef_.T, 'o', label=\"C=1\")\nplt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\nxlims = plt.xlim()\nplt.hlines(0, xlims[0], xlims[1])\nplt.xlim(xlims)\nplt.ylim(-5, 5)\nplt.xlabel(\"특성\")\nplt.ylabel(\"계수 크기\")\nplt.legend()\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nfor C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n    lr_l1 = LogisticRegression(solver='liblinear', C=C, penalty=\"l1\", max_iter=1000).fit(X_train, y_train)\n    print(\"C={:.3f} 인 l1 로지스틱 회귀의 훈련 정확도: {:.2f}\".format(\n          C, lr_l1.score(X_train, y_train)))\n    print(\"C={:.3f} 인 l1 로지스틱 회귀의 테스트 정확도: {:.2f}\".format(\n          C, lr_l1.score(X_test, y_test)))\n    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\n\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\nxlims = plt.xlim()\nplt.hlines(0, xlims[0], xlims[1])\nplt.xlim(xlims)\nplt.xlabel(\"특성\")\nplt.ylabel(\"계수 크기\")\n\nplt.ylim(-5, 5)\nplt.legend(loc=3)\nplt.show() # 책에는 없음\n\nC=0.001 인 l1 로지스틱 회귀의 훈련 정확도: 0.91\nC=0.001 인 l1 로지스틱 회귀의 테스트 정확도: 0.92\nC=1.000 인 l1 로지스틱 회귀의 훈련 정확도: 0.96\nC=1.000 인 l1 로지스틱 회귀의 테스트 정확도: 0.96\nC=100.000 인 l1 로지스틱 회귀의 훈련 정확도: 0.99\nC=100.000 인 l1 로지스틱 회귀의 테스트 정확도: 0.98\n\n\n\n\n\n\n\n\n\n\n다중 클래스 분류용 선형 모델\n\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(random_state=42)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.legend([\"클래스 0\", \"클래스 1\", \"클래스 2\"])\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nlinear_svm = LinearSVC(dual=\"auto\").fit(X, y)\nprint(\"계수 배열의 크기: \", linear_svm.coef_.shape)\nprint(\"절편 배열의 크기: \", linear_svm.intercept_.shape)\n\n계수 배열의 크기:  (3, 2)\n절편 배열의 크기:  (3,)\n\n\n\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n                                  mglearn.cm3.colors):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nplt.ylim(-10, 15)\nplt.xlim(-10, 8)\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.legend(['클래스 0', '클래스 1', '클래스 2', '클래스 0 경계', '클래스 1 경계',\n            '클래스 2 경계'], loc=(1.01, 0.3))\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nmglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n                                  mglearn.cm3.colors):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nplt.legend(['클래스 0', '클래스 1', '클래스 2', '클래스 0 경계', '클래스 1 경계',\n            '클래스 2 경계'], loc=(1.01, 0.3))\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n# 한 줄에서 모델의 객체를 생성과 학습을 한번에 실행합니다\nlogreg = LogisticRegression(max_iter=5000).fit(X_train, y_train)\n\n\nlogreg = LogisticRegression(max_iter=5000)\ny_pred = logreg.fit(X_train, y_train).predict(X_test)\n\n\ny_pred = LogisticRegression(max_iter=5000).fit(X_train, y_train).predict(X_test)\n\n\n\nSGDClassifier와 SDGRegressor\n\nfrom sklearn.linear_model import SGDClassifier\n\nsgd_c = SGDClassifier(alpha=0.01, learning_rate='adaptive',\n                      eta0=0.1, random_state=42, n_jobs=-1)\nsgd_c.fit(X, y)\n\nmglearn.plots.plot_2d_classification(sgd_c, X, fill=True, alpha=.7)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(sgd_c.coef_, sgd_c.intercept_,\n                                  mglearn.cm3.colors):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nplt.legend(['클래스 0', '클래스 1', '클래스 2', '클래스 0 경계', '클래스 1 경계',\n            '클래스 2 경계'], loc=(1.01, 0.3))\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import SGDRegressor\n\nX, y = mglearn.datasets.load_extended_boston()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nsgd_r = SGDRegressor(learning_rate='adaptive', eta0=0.1, random_state=42)\nsgd_r.fit(X_train, y_train)\n\nprint(\"훈련 세트 점수: {:.2f}\".format(sgd_r.score(X_train, y_train)))\nprint(\"테스트 세트 점수: {:.2f}\".format(sgd_r.score(X_test, y_test)))\n\n훈련 세트 점수: 0.91\n테스트 세트 점수: 0.77\n\n\n\n\n\n\n나이브 베이즈 분류기\n\nX = np.array([[0, 1, 0, 1],\n              [1, 0, 1, 1],\n              [0, 0, 0, 1],\n              [1, 0, 1, 0]])\ny = np.array([0, 1, 0, 1])\n\n\ncounts = {}\nfor label in np.unique(y):\n    # 각 클래스에 대해 반복\n    # 특성마다 1 이 나타난 횟수를 센다.\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"특성 카운트:\\n\", counts)\n\n특성 카운트:\n {0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\n\n\n\n\n결정 트리\n\n결정 트리 만들기\n\nmglearn.plots.plot_tree_progressive()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n결정 트리의 복잡도 제어하기\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\nprint(\"훈련 세트 정확도: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(tree.score(X_test, y_test)))\n\n훈련 세트 정확도: 1.000\n테스트 세트 정확도: 0.937\n\n\n\ntree = DecisionTreeClassifier(max_depth=4, random_state=0)\ntree.fit(X_train, y_train)\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(tree.score(X_test, y_test)))\n\n훈련 세트 정확도: 0.988\n테스트 세트 정확도: 0.951\n\n\n\n\n결정 트리 분석\n\nfrom sklearn.tree import export_graphviz\nexport_graphviz(tree, out_file=\"tree.dot\", class_names=[\"악성\", \"양성\"],\n                feature_names=cancer.feature_names, impurity=False, filled=True)\n\n\nfrom sklearn.tree import plot_tree\nplt.figure(figsize=(12,6))\nplot_tree(tree, class_names=[\"악성\", \"양성\"],\n          feature_names=cancer.feature_names.tolist(),\n          impurity=False, filled=True, rounded=True, fontsize=10)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n트리의 특성 중요도\n\nprint(\"특성 중요도:\\n\", tree.feature_importances_)\n\n특성 중요도:\n [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.048\n 0.    0.    0.002 0.    0.    0.    0.    0.    0.727 0.046 0.    0.\n 0.014 0.    0.018 0.122 0.012 0.   ]\n\n\n\ndef plot_feature_importances_cancer(model):\n    n_features = cancer.data.shape[1]\n    plt.barh(np.arange(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), cancer.feature_names)\n    plt.xlabel(\"특성 중요도\")\n    plt.ylabel(\"특성\")\n    plt.ylim(-1, n_features)\n\nplot_feature_importances_cancer(tree)\n\n\n\n\n\n\n\n\n\nram_prices = pd.read_csv(\"./data/ram_price.csv\")\nplt.yticks(fontname = \"DejaVu Sans\") # 한글 폰트가 지수에 음수를 표시하지 못하므로 ytick의 폰트를 바꾸어 줍니다.\nplt.semilogy(ram_prices.date, ram_prices.price)\nplt.xlabel(\"년\")\nplt.ylabel(\"가격 ($/Mbyte)\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor\n# 2000년 이전을 훈련 데이터로, 2000년 이후를 테스트 데이터로 만듭니다\ndata_train = ram_prices[ram_prices.date &lt; 2000]\ndata_test = ram_prices[ram_prices.date &gt;= 2000]\n\n# 가격 예측을 위해 날짜 특성만을 이용합니다\nX_train = data_train.date.to_numpy()[:, np.newaxis]\n# 데이터와 타깃 사이의 관계를 간단하게 만들기 위해 로그 스케일로 바꿉니다\ny_train = np.log(data_train.price)\n\ntree = DecisionTreeRegressor().fit(X_train, y_train)\nlinear_reg = LinearRegression().fit(X_train, y_train)\n\n# 예측은 전체 기간에 대해서 수행합니다\nX_all = ram_prices.date.to_numpy()[:, np.newaxis]\n\npred_tree = tree.predict(X_all)\npred_lr = linear_reg.predict(X_all)\n\n# 예측한 값의 로그 스케일을 되돌립니다\nprice_tree = np.exp(pred_tree)\nprice_lr = np.exp(pred_lr)\n\n\nplt.yticks(fontname = \"DejaVu Sans\") # 한글 폰트가 지수에 음수를 표시하지 못하므로 ytick의 폰트를 바꾸어 줍니다.\nplt.semilogy(data_train.date, data_train.price, label=\"훈련 데이터\")\nplt.semilogy(data_test.date, data_test.price, label=\"테스트 데이터\")\nplt.semilogy(ram_prices.date, price_tree, label=\"트리 예측\")\nplt.semilogy(ram_prices.date, price_lr, label=\"선형회귀 예측\")\nplt.legend()\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\n판다스 데이터 프레임 연동\n\ntree.fit(data_train[['date']], y_train)\nprint('특성 개수:', tree.n_features_in_)\nprint('특성 이름:', tree.feature_names_in_)\n\n특성 개수: 1\n특성 이름: ['date']\n\n\n\n\n\n랜덤 포레스트(결정 트리의 앙상블)\n\n랜덤 포레스트 분석\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=100, noise=0.25, random_state=3)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\nforest = RandomForestClassifier(n_estimators=5, random_state=2)\nforest.fit(X_train, y_train)\n\nRandomForestClassifier(n_estimators=5, random_state=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(n_estimators=5, random_state=2) \n\n\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 10))\nfor i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n    ax.set_title(\"트리 {}\".format(i))\n    mglearn.plots.plot_tree_partition(X, y, tree, ax=ax)\n\nmglearn.plots.plot_2d_separator(forest, X, fill=True, ax=axes[-1, -1], alpha=.4)\naxes[-1, -1].set_title(\"랜덤 포레스트\")\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\nforest = RandomForestClassifier(n_estimators=100, random_state=0)\nforest.fit(X_train, y_train)\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(forest.score(X_train, y_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(forest.score(X_test, y_test)))\n\n훈련 세트 정확도: 1.000\n테스트 세트 정확도: 0.972\n\n\n\nplot_feature_importances_cancer(forest)\n\n\n\n\n\n\n\n\n\n\n그래디언트 부스팅 회귀 트리\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\n\ngbrt = GradientBoostingClassifier(random_state=0)\ngbrt.fit(X_train, y_train)\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train, y_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(X_test, y_test)))\n\n훈련 세트 정확도: 1.000\n테스트 세트 정확도: 0.965\n\n\n\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\ngbrt.fit(X_train, y_train)\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train, y_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(X_test, y_test)))\n\n훈련 세트 정확도: 0.991\n테스트 세트 정확도: 0.972\n\n\n\ngbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\ngbrt.fit(X_train, y_train)\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(X_train, y_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(X_test, y_test)))\n\n훈련 세트 정확도: 0.988\n테스트 세트 정확도: 0.958\n\n\n\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\ngbrt.fit(X_train, y_train)\n\nplot_feature_importances_cancer(gbrt)\n\n\n\n\n\n\n\n\n\n\n\n그 외 다른 앙상블\n\nXm, ym = make_moons(n_samples=100, noise=0.25, random_state=3)\nXm_train, Xm_test, ym_train, ym_test = train_test_split(\n    Xm, ym, stratify=ym, random_state=42)\n\ncancer = load_breast_cancer()\nXc_train, Xc_test, yc_train, yc_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\n\n\nBagging\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import BaggingClassifier\nbagging = BaggingClassifier(LogisticRegression(solver='liblinear'), n_estimators=100,\n                            oob_score=True, n_jobs=-1, random_state=42)\nbagging.fit(Xc_train, yc_train)\n\nBaggingClassifier(estimator=LogisticRegression(solver='liblinear'),\n                  n_estimators=100, n_jobs=-1, oob_score=True, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  BaggingClassifier?Documentation for BaggingClassifieriFittedBaggingClassifier(estimator=LogisticRegression(solver='liblinear'),\n                  n_estimators=100, n_jobs=-1, oob_score=True, random_state=42) estimator: LogisticRegressionLogisticRegression(solver='liblinear')  LogisticRegression?Documentation for LogisticRegressionLogisticRegression(solver='liblinear') \n\n\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(bagging.score(Xc_train, yc_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(bagging.score(Xc_test, yc_test)))\nprint(\"OOB 샘플의 정확도: {:.3f}\".format(bagging.oob_score_))\n\n훈련 세트 정확도: 0.962\n테스트 세트 정확도: 0.958\nOOB 샘플의 정확도: 0.948\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nbagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=5,\n                            n_jobs=-1, random_state=42)\nbagging.fit(Xm_train, ym_train)\n\nBaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=5, n_jobs=-1,\n                  random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  BaggingClassifier?Documentation for BaggingClassifieriFittedBaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=5, n_jobs=-1,\n                  random_state=42) estimator: DecisionTreeClassifierDecisionTreeClassifier()  DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() \n\n\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 10))\nfor i, (ax, tree) in enumerate(zip(axes.ravel(), bagging.estimators_)):\n    ax.set_title(\"Tree {}\".format(i))\n    mglearn.plots.plot_tree_partition(Xm, ym, tree, ax=ax)\n\nmglearn.plots.plot_2d_separator(bagging, Xm, fill=True, ax=axes[-1, -1], alpha=.4)\naxes[-1, -1].set_title(\"Bagging\")\nmglearn.discrete_scatter(Xm[:, 0], Xm[:, 1], ym)\nplt.show()\n\n\n\n\n\n\n\n\n\nbagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100,\n                            oob_score=True, n_jobs=-1, random_state=42)\nbagging.fit(Xc_train, yc_train)\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(bagging.score(Xc_train, yc_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(bagging.score(Xc_test, yc_test)))\nprint(\"OOB 샘플의 정확도: {:.3f}\".format(bagging.oob_score_))\n\n훈련 세트 정확도: 1.000\n테스트 세트 정확도: 0.965\nOOB 샘플의 정확도: 0.948\n\n\n\n\nExtraTrees\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nxtree = ExtraTreesClassifier(n_estimators=5, n_jobs=-1, random_state=0)\nxtree.fit(Xm_train, ym_train)\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 10))\nfor i, (ax, tree) in enumerate(zip(axes.ravel(), xtree.estimators_)):\n    ax.set_title(\"Tree {}\".format(i))\n    mglearn.plots.plot_tree_partition(Xm, ym, tree, ax=ax)\n\nmglearn.plots.plot_2d_separator(xtree, Xm, fill=True, ax=axes[-1, -1], alpha=.4)\naxes[-1, -1].set_title(\"ExtraTrees\")\nmglearn.discrete_scatter(Xm[:, 0], Xm[:, 1], ym)\nplt.show()\n\n\n\n\n\n\n\n\n\nxtree = ExtraTreesClassifier(n_estimators=100, n_jobs=-1, random_state=0)\nxtree.fit(Xc_train, yc_train)\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(xtree.score(Xc_train, yc_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(xtree.score(Xc_test, yc_test)))\n\n훈련 세트 정확도: 1.000\n테스트 세트 정확도: 0.972\n\n\n\nn_features = cancer.data.shape[1]\nplt.barh(range(n_features), xtree.feature_importances_, align='center')\nplt.yticks(np.arange(n_features), cancer.feature_names)\nplt.xlabel(\"Feature Importance\")\nplt.ylabel(\"Features\")\nplt.ylim(-1, n_features)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAdaBoost\n\nfrom sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(n_estimators=5, random_state=42, algorithm='SAMME')\nada.fit(Xm_train, ym_train)\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 10))\nfor i, (ax, tree) in enumerate(zip(axes.ravel(), ada.estimators_)):\n    ax.set_title(\"Tree {}\".format(i))\n    mglearn.plots.plot_tree_partition(Xm, ym, tree, ax=ax)\n\nmglearn.plots.plot_2d_separator(ada, Xm, fill=True, ax=axes[-1, -1], alpha=.4)\naxes[-1, -1].set_title(\"AdaBoost\")\nmglearn.discrete_scatter(Xm[:, 0], Xm[:, 1], ym)\nplt.show()\n\n\n\n\n\n\n\n\n\nada = AdaBoostClassifier(n_estimators=100, random_state=42,algorithm='SAMME')\nada.fit(Xc_train, yc_train)\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(ada.score(Xc_train, yc_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(ada.score(Xc_test, yc_test)))\n\n훈련 세트 정확도: 1.000\n테스트 세트 정확도: 0.972\n\n\n\nplt.barh(range(n_features), ada.feature_importances_, align='center')\nplt.yticks(np.arange(n_features), cancer.feature_names)\nplt.xlabel(\"Feature Importance\")\nplt.ylabel(\"Features\")\nplt.ylim(-1, n_features)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHistGradientBoostingClassifier\n\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nhgb = HistGradientBoostingClassifier(random_state=42)\nhgb.fit(Xm_train, ym_train)\n\nmglearn.plots.plot_2d_separator(hgb, Xm, fill=True, alpha=.4)\nplt.title(\"HistGradientBoosting\")\nmglearn.discrete_scatter(Xm[:, 0], Xm[:, 1], ym)\nplt.savefig('ch2-fig-histgradientboosting')\nplt.show()\n\n\n\n\n\n\n\n\n\nhgb = HistGradientBoostingClassifier(random_state=42)\nhgb.fit(Xc_train, yc_train)\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(hgb.score(Xc_train, yc_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(hgb.score(Xc_test, yc_test)))\n\n훈련 세트 정확도: 1.000\n테스트 세트 정확도: 0.979\n\n\n\nfrom sklearn.inspection import permutation_importance\n\nresult = permutation_importance(hgb, Xc_train, yc_train,\n                               n_repeats=10, random_state=42, n_jobs=-1)\n\nplt.barh(range(n_features), result.importances_mean, align='center')\nplt.yticks(np.arange(n_features), cancer.feature_names)\nplt.xlabel(\"Feature Importance\")\nplt.ylabel(\"Features\")\nplt.ylim(-1, n_features)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n커널 서포트 벡터 머신\n\n선형 모델과 비선형 특성\n\nX, y = make_blobs(centers=4, random_state=8)\ny = y % 2\n\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nfrom sklearn.svm import LinearSVC\nlinear_svm = LinearSVC(max_iter=5000, tol=1e-3, dual=\"auto\").fit(X, y)\n\nmglearn.plots.plot_2d_separator(linear_svm, X)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n# 두 번째 특성을 제곱하여 추가합니다\nX_new = np.hstack([X, X[:, 1:] ** 2])\nfrom mpl_toolkits.mplot3d import Axes3D, axes3d\nfigure = plt.figure()\n# 3차원 그래프\nax = Axes3D(figure, elev=-152, azim=-26, auto_add_to_figure=False)\nfigure.add_axes(ax)\n# y == 0 인 포인트를 먼저 그리고 그 다음 y == 1 인 포인트를 그립니다\nmask = y == 0\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n           s=60, edgecolor='k')\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n           s=60, edgecolor='k')\nax.set_xlabel(\"특성0\")\nax.set_ylabel(\"특성1\")\nax.set_zlabel(\"특성1 ** 2\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nlinear_svm_3d = LinearSVC(max_iter=5000).fit(X_new, y)\ncoef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n\n# 선형 결정 경계 그리기\nfigure = plt.figure()\nax = Axes3D(figure, elev=-152, azim=-26, auto_add_to_figure=False)\nfigure.add_axes(ax)\nxx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\nyy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\n\nXX, YY = np.meshgrid(xx, yy)\nZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\nax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n           s=60, edgecolor='k')\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n           s=60, edgecolor='k')\n\nax.set_xlabel(\"특성0\")\nax.set_ylabel(\"특성1\")\nax.set_zlabel(\"특성1 ** 2\")\nplt.show() # 책에는 없음\n\nc:\\Users\\sigma\\Practices\\practice-ml\\.venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\nZZ = YY ** 2\ndec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\nplt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\n             cmap=mglearn.cm2, alpha=0.5)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\n커널 기법\n\n\nSVM 이해하기\n\nfrom sklearn.svm import SVC\n\nX, y = mglearn.tools.make_handcrafted_dataset()\nsvm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\nmglearn.plots.plot_2d_separator(svm, X, eps=.5)\n# 데이터 포인트 그리기\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n# 서포트 벡터\nsv = svm.support_vectors_\n# dual_coef_ 의 부호에 의해 서포트 벡터의 클래스 레이블이 결정됩니다\nsv_labels = svm.dual_coef_.ravel() &gt; 0\nmglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\nSVM 매개변수 튜닝\n\nfig, axes = plt.subplots(3, 3, figsize=(15, 10))\n\nfor ax, C in zip(axes, [-1, 0, 3]):\n    for a, gamma in zip(ax, range(-1, 2)):\n        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\n\naxes[0, 0].legend([\"클래스 0\", \"클래스 1\", \"클래스 0 서포트 벡터\", \"클래스 1 서포트 벡터\"],\n                  ncol=4, loc=(.9, 1.2))\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\n\nprint(\"훈련 세트 정확도: {:.2f}\".format(svc.score(X_train, y_train)))\nprint(\"테스트 세트 정확도: {:.2f}\".format(svc.score(X_test, y_test)))\n\n훈련 세트 정확도: 0.90\n테스트 세트 정확도: 0.94\n\n\n\nplt.boxplot(X_train, manage_ticks=False)\nplt.yscale(\"symlog\")\nplt.xlabel(\"특성 목록\")\nplt.ylabel(\"특성 크기\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSVM 을 위한 데이터 전처리\n\n# 훈련 세트에서 특성별 최솟값 계산\nmin_on_training = X_train.min(axis=0)\n# 훈련 세트에서 특성별 (최댓값 - 최솟값) 범위 계산\nrange_on_training = (X_train - min_on_training).max(axis=0)\n\n# 훈련 데이터에 최솟값을 빼고 범위로 나누면\n# 각 특성에 대해 최솟값은 0 최댓값은 1 임\nX_train_scaled = (X_train - min_on_training) / range_on_training\nprint(\"특성별 최솟값\\n\", X_train_scaled.min(axis=0))\nprint(\"특성별 최댓값\\n\", X_train_scaled.max(axis=0))\n\n특성별 최솟값\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n특성별 최댓값\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1.]\n\n\n\n# 테스트 세트에도 같은 작업을 적용하지만\n# 훈련 세트에서 계산한 최솟값과 범위를 사용합니다(자세한 내용은 3장에 있습니다)\nX_test_scaled = (X_test - min_on_training) / range_on_training\n\n\nsvc = SVC()\nsvc.fit(X_train_scaled, y_train)\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(svc.score(X_train_scaled, y_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\n\n훈련 세트 정확도: 0.984\n테스트 세트 정확도: 0.972\n\n\n\nsvc = SVC(C=20)\nsvc.fit(X_train_scaled, y_train)\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(svc.score(X_train_scaled, y_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\n\n훈련 세트 정확도: 0.988\n테스트 세트 정확도: 0.979\n\n\n\n\n\n신경망 (딥러닝)\n\n신경망 모델\n\nline = np.linspace(-3, 3, 100)\nplt.plot(line, np.tanh(line), label=\"tanh\")\nplt.plot(line, np.maximum(line, 0), linestyle='--', label=\"relu\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"x\")\nplt.ylabel(\"relu(x), tanh(x)\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\n신경망 튜닝\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n                                                    random_state=42)\n\nmlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nmlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[10],\n                    max_iter=1000)\nmlp.fit(X_train, y_train)\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n# 10개의 유닛으로 된 두 개의 은닉층\nmlp = MLPClassifier(solver='lbfgs', random_state=0,\n                    hidden_layer_sizes=[10, 10], max_iter=1000)\nmlp.fit(X_train, y_train)\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# tanh 활성화 함수가 적용된 10개의 유닛으로 된 두 개의 은닉층\nmlp = MLPClassifier(solver='lbfgs', activation='tanh',\n                    random_state=0, hidden_layer_sizes=[10, 10], max_iter=1000)\nmlp.fit(X_train, y_train)\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\nplt.xlabel(\"특성 0\")\nplt.ylabel(\"특성 1\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\nfor axx, n_hidden_nodes in zip(axes, [10, 100]):\n    for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 1]):\n        mlp = MLPClassifier(solver='lbfgs', random_state=0,\n                            hidden_layer_sizes=[n_hidden_nodes, n_hidden_nodes],\n                            alpha=alpha, max_iter=1000)\n        mlp.fit(X_train, y_train)\n        mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\n        mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\n        ax.set_title(\"n_hidden=[{}, {}]\\nalpha={:.4f}\".format(\n                      n_hidden_nodes, n_hidden_nodes, alpha))\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\nfor i, ax in enumerate(axes.ravel()):\n    mlp = MLPClassifier(solver='lbfgs', random_state=i,\n                        hidden_layer_sizes=[100, 100])\n    mlp.fit(X_train, y_train)\n    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\n\n\n\n\n\n\n\n\n\nprint(\"유방암 데이터의 특성별 최대값:\\n\", cancer.data.max(axis=0))\n\n유방암 데이터의 특성별 최대값:\n [  28.11    39.28   188.5   2501.       0.163    0.345    0.427    0.201\n    0.304    0.097    2.873    4.885   21.98   542.2      0.031    0.135\n    0.396    0.053    0.079    0.03    36.04    49.54   251.2   4254.\n    0.223    1.058    1.252    0.291    0.664    0.207]\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\n\nmlp = MLPClassifier(random_state=42)\nmlp.fit(X_train, y_train)\n\nprint(\"훈련 세트 정확도: {:.2f}\".format(mlp.score(X_train, y_train)))\nprint(\"테스트 세트 정확도: {:.2f}\".format(mlp.score(X_test, y_test)))\n\n훈련 세트 정확도: 0.94\n테스트 세트 정확도: 0.92\n\n\n\n# 훈련 세트 각 특성의 평균을 계산합니다\nmean_on_train = X_train.mean(axis=0)\n# 훈련 세트 각 특성의 표준 편차를 계산합니다\nstd_on_train = X_train.std(axis=0)\n\n# 데이터에서 평균을 빼고 표준 편차로 나누면\n# 평균 0, 표준 편차 1 인 데이터로 변환됩니다.\nX_train_scaled = (X_train - mean_on_train) / std_on_train\n# (훈련 데이터의 평균과 표준 편차를 이용해) 같은 변환을 테스트 세트에도 합니다\nX_test_scaled = (X_test - mean_on_train) / std_on_train\n\nmlp = MLPClassifier(random_state=0, max_iter=1000)\nmlp.fit(X_train_scaled, y_train)\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(mlp.score(X_train_scaled, y_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n\n훈련 세트 정확도: 1.000\n테스트 세트 정확도: 0.972\n\n\n\nmlp = MLPClassifier(max_iter=1000, random_state=0)\nmlp.fit(X_train_scaled, y_train)\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(mlp.score(X_train_scaled, y_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n\n훈련 세트 정확도: 1.000\n테스트 세트 정확도: 0.972\n\n\n\nmlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)\nmlp.fit(X_train_scaled, y_train)\n\nprint(\"훈련 세트 정확도: {:.3f}\".format(mlp.score(X_train_scaled, y_train)))\nprint(\"테스트 세트 정확도: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n\n훈련 세트 정확도: 0.988\n테스트 세트 정확도: 0.972\n\n\n\nmlp.coefs_[0].std(axis=1), mlp.coefs_[0].var(axis=1)\n\n(array([0.026, 0.041, 0.033, 0.031, 0.017, 0.018, 0.036, 0.041, 0.024,\n        0.017, 0.055, 0.017, 0.04 , 0.037, 0.013, 0.034, 0.012, 0.016,\n        0.014, 0.03 , 0.049, 0.052, 0.042, 0.043, 0.032, 0.015, 0.041,\n        0.049, 0.038, 0.026]),\n array([0.001, 0.002, 0.001, 0.001, 0.   , 0.   , 0.001, 0.002, 0.001,\n        0.   , 0.003, 0.   , 0.002, 0.001, 0.   , 0.001, 0.   , 0.   ,\n        0.   , 0.001, 0.002, 0.003, 0.002, 0.002, 0.001, 0.   , 0.002,\n        0.002, 0.001, 0.001]))\n\n\n\nplt.figure(figsize=(20, 5))\nplt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis')\nplt.yticks(range(30), cancer.feature_names)\nplt.xlabel(\"은닉 유닛\")\nplt.ylabel(\"입력 특성\")\nplt.colorbar()\nplt.show() # 책에는 없음",
    "crumbs": [
      "Introduction",
      "지도 학습"
    ]
  },
  {
    "objectID": "ML02.html#분류-예측의-불확실성-추정",
    "href": "ML02.html#분류-예측의-불확실성-추정",
    "title": "지도 학습",
    "section": "분류 예측의 불확실성 추정",
    "text": "분류 예측의 불확실성 추정\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import make_circles\nX, y = make_circles(noise=0.25, factor=0.5, random_state=1)\n\n# 예제를 위해 클래스의 이름을 \"blue\" 와 \"red\" 로 바꿉니다\ny_named = np.array([\"blue\", \"red\"])[y]\n\n# 여러개의 배열을 한꺼번에 train_test_split 에 넣을 수 있습니다\n# 훈련 세트와 테스트 세트로 나뉘는 방식은 모두 같습니다.\nX_train, X_test, y_train_named, y_test_named, y_train, y_test = \\\n    train_test_split(X, y_named, y, random_state=0)\n\n# 그래디언트 부스팅 모델을 만듭니다\ngbrt = GradientBoostingClassifier(random_state=0)\ngbrt.fit(X_train, y_train_named)\n\nGradientBoostingClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier(random_state=0) \n\n\n\n결정 함수\n\nprint(\"X_test.shape:\", X_test.shape)\nprint(\"결정 함수 결과 형태:\", gbrt.decision_function(X_test).shape)\n\nX_test.shape: (25, 2)\n결정 함수 결과 형태: (25,)\n\n\n\n# 결정 함수 결과 중 앞부분 일부를 확인합니다\nprint(\"결정 함수:\\n\", gbrt.decision_function(X_test)[:6])\n\n결정 함수:\n [ 4.136 -1.702 -3.951 -3.626  4.29   3.662]\n\n\n\nprint(\"임계치와 결정 함수 결과 비교:\\n\",\n      gbrt.decision_function(X_test) &gt; 0)\nprint(\"예측:\\n\", gbrt.predict(X_test))\n\n임계치와 결정 함수 결과 비교:\n [ True False False False  True  True False  True  True  True False  True\n  True False  True False False False  True  True  True  True  True False\n False]\n예측:\n ['red' 'blue' 'blue' 'blue' 'red' 'red' 'blue' 'red' 'red' 'red' 'blue'\n 'red' 'red' 'blue' 'red' 'blue' 'blue' 'blue' 'red' 'red' 'red' 'red'\n 'red' 'blue' 'blue']\n\n\n\n# 불리언 값을 0과 1로 변환합니다\ngreater_zero = (gbrt.decision_function(X_test) &gt; 0).astype(int)\n# classes_에 인덱스로 사용합니다\npred = gbrt.classes_[greater_zero]\n# pred 와 gbrt.predict의 결과를 비교합니다\nprint(\"pred 는 예측 결과와 같다:\",\n      np.all(pred == gbrt.predict(X_test)))\n\npred 는 예측 결과와 같다: True\n\n\n\ndecision_function = gbrt.decision_function(X_test)\nprint(\"결정 함수 최소값: {:.2f} 최대값: {:.2f}\".format(\n      np.min(decision_function), np.max(decision_function)))\n\n결정 함수 최소값: -7.69 최대값: 4.29\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\n\nmglearn.tools.plot_2d_separator(gbrt, X, ax=axes[0], alpha=.4,\n                                fill=True, cm=mglearn.cm2)\nscores_image = mglearn.tools.plot_2d_scores(gbrt, X, ax=axes[1],\n                                            alpha=.4, cm=mglearn.ReBl)\n\nfor ax in axes:\n    # 훈련 포인트와 테스트 포인트를 그리기\n    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\n                             markers='^', ax=ax)\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\n                             markers='o', ax=ax)\n    ax.set_xlabel(\"특성 0\")\n    ax.set_ylabel(\"특성 1\")\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\ncbar.set_alpha(1)\nfig.draw_without_rendering()\naxes[0].legend([\"테스트 클래스 0\", \"테스트 클래스 1\", \"훈련 클래스 0\",\n                \"훈련 클래스 1\"], ncol=4, loc=(.1, 1.1))\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\n예측 확률\n\nprint(\"확률 값의 형태:\", gbrt.predict_proba(X_test).shape)\n\n확률 값의 형태: (25, 2)\n\n\n\n# predict_proba 결과 중 앞부분 일부를 확인합니다\nprint(\"Predicted probabilities:\\n\",\n      gbrt.predict_proba(X_test[:6]))\n\nPredicted probabilities:\n [[0.016 0.984]\n [0.846 0.154]\n [0.981 0.019]\n [0.974 0.026]\n [0.014 0.986]\n [0.025 0.975]]\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\n\nmglearn.tools.plot_2d_separator(\n    gbrt, X, ax=axes[0], alpha=.4, fill=True, cm=mglearn.cm2)\nscores_image = mglearn.tools.plot_2d_scores(\n    gbrt, X, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function='predict_proba')\n\nfor ax in axes:\n    # 훈련 포인트와 테스트 포인트를 그리기\n    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\n                             markers='^', ax=ax)\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\n                             markers='o', ax=ax)\n    ax.set_xlabel(\"특성 0\")\n    ax.set_ylabel(\"특성 1\")\n# colorbar 를 감추지 않습니다.\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\ncbar.set_alpha(1)\nfig.draw_without_rendering()\naxes[0].legend([\"테스트 클래스 0\", \"테스트 클래스 1\", \"훈련 클래스 0\",\n                \"훈련 클래스 1\"], ncol=4, loc=(.1, 1.1))\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\n\n다중 분류에서의 불확실성\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, random_state=42)\n\ngbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)\ngbrt.fit(X_train, y_train)\n\nGradientBoostingClassifier(learning_rate=0.01, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier(learning_rate=0.01, random_state=0) \n\n\n\nprint(\"결정 함수의 결과 형태:\", gbrt.decision_function(X_test).shape)\n# decision function 결과 중 앞부분 일부를 확인합니다.\nprint(\"결정 함수 결과:\\n\", gbrt.decision_function(X_test)[:6, :])\n\n결정 함수의 결과 형태: (38, 3)\n결정 함수 결과:\n [[-0.896  1.147 -0.828]\n [ 1.161 -0.808 -0.828]\n [-0.891 -0.777  1.197]\n [-0.896  1.147 -0.828]\n [-0.898  0.965 -0.104]\n [ 1.161 -0.808 -0.828]]\n\n\n\nprint(\"가장 큰 결정 함수의 인덱스:\\n\",\n      np.argmax(gbrt.decision_function(X_test), axis=1))\nprint(\"예측:\\n\", gbrt.predict(X_test))\n\n가장 큰 결정 함수의 인덱스:\n [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n 0]\n예측:\n [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n 0]\n\n\n\n# predict_proba 결과 중 앞부분 일부를 확인합니다\nprint(\"예측 확률:\\n\", gbrt.predict_proba(X_test)[:6])\n# 행 방향으로 확률을 더하면 1 이 됩니다\nprint(\"합:\", gbrt.predict_proba(X_test)[:6].sum(axis=1))\n\n예측 확률:\n [[0.102 0.788 0.109]\n [0.783 0.109 0.107]\n [0.098 0.11  0.792]\n [0.102 0.788 0.109]\n [0.104 0.667 0.229]\n [0.783 0.109 0.107]]\n합: [1. 1. 1. 1. 1. 1.]\n\n\n\nprint(\"가장 큰 예측 확률의 인덱스:\\n\",\n      np.argmax(gbrt.predict_proba(X_test), axis=1))\nprint(\"예측:\\n\", gbrt.predict(X_test))\n\n가장 큰 예측 확률의 인덱스:\n [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n 0]\n예측:\n [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n 0]\n\n\n\nlogreg = LogisticRegression(max_iter=1000)\n\n# iris 데이터셋의 타깃을 클래스 이름으로 나타내기\nnamed_target = iris.target_names[y_train]\nlogreg.fit(X_train, named_target)\nprint(\"훈련 데이터에 있는 클래스 종류:\", logreg.classes_)\nprint(\"예측:\", logreg.predict(X_test)[:10])\nargmax_dec_func = np.argmax(logreg.decision_function(X_test), axis=1)\nprint(\"가장 큰 결정 함수의 인덱스:\", argmax_dec_func[:10])\nprint(\"인덱스를 classses_에 연결:\",\n      logreg.classes_[argmax_dec_func][:10])\n\n훈련 데이터에 있는 클래스 종류: ['setosa' 'versicolor' 'virginica']\n예측: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor' 'setosa'\n 'versicolor' 'virginica' 'versicolor' 'versicolor']\n가장 큰 결정 함수의 인덱스: [1 0 2 1 1 0 1 2 1 1]\n인덱스를 classses_에 연결: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor' 'setosa'\n 'versicolor' 'virginica' 'versicolor' 'versicolor']",
    "crumbs": [
      "Introduction",
      "지도 학습"
    ]
  },
  {
    "objectID": "ML05.html",
    "href": "ML05.html",
    "title": "모델 평가와 성능 향상",
    "section": "",
    "text": "from preamble import *\nimport koreanize_matplotlib\n%config InlineBackend.figure_format='retina'",
    "crumbs": [
      "Introduction",
      "모델 평가와 성능 향상"
    ]
  },
  {
    "objectID": "ML05.html#교차-검증cross-validation",
    "href": "ML05.html#교차-검증cross-validation",
    "title": "모델 평가와 성능 향상",
    "section": "교차 검증(Cross Validation)",
    "text": "교차 검증(Cross Validation)\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# 인위적인 데이터셋을 만듭니다\nX, y = make_blobs(random_state=0)\n# 데이터와 타깃 레이블을 훈련 세트와 테스트 세트로 나눕니다\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# 모델 객체를 만들고 훈련 세트로 학습시킵니다\nlogreg = LogisticRegression().fit(X_train, y_train)\n# 모델을 테스트 세트로 평가합니다\nprint(\"테스트 세트 점수: {:.2f}\".format(logreg.score(X_test, y_test)))\n\n테스트 세트 점수: 0.88\n\n\n\nmglearn.plots.plot_cross_validation()\n\n\n\n\n\n\n\n\n\nscikit-learn의 교차 검증\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\niris = load_iris()\nlogreg = LogisticRegression(max_iter=1000)\n\nscores = cross_val_score(logreg, iris.data, iris.target)\nprint(\"교차 검증 점수:\", scores)\n\n교차 검증 점수: [0.967 1.    0.933 0.967 1.   ]\n\n\n\nscores = cross_val_score(logreg, iris.data, iris.target, cv=10)\nprint(\"교차 검증 점수:\", scores)\n\n교차 검증 점수: [1.    0.933 1.    1.    0.933 0.933 0.933 1.    1.    1.   ]\n\n\n\nprint(\"교차 검증 평균 점수: {:.2f}\".format(scores.mean()))\n\n교차 검증 평균 점수: 0.97\n\n\n\nfrom sklearn.model_selection import cross_validate\nres = cross_validate(logreg, iris.data, iris.target,\n                     return_train_score=True)\ndisplay(res)\n\n{'fit_time': array([0.005, 0.005, 0.003, 0.004, 0.004]),\n 'score_time': array([0.   , 0.   , 0.002, 0.   , 0.   ]),\n 'test_score': array([0.967, 1.   , 0.933, 0.967, 1.   ]),\n 'train_score': array([0.967, 0.967, 0.983, 0.983, 0.975])}\n\n\n\nres_df = pd.DataFrame(res)\ndisplay(res_df)\nprint(\"평균 시간과 점수:\\n\", res_df.mean())\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_score\ntrain_score\n\n\n\n\n0\n4.50e-03\n0.00e+00\n0.97\n0.97\n\n\n1\n5.01e-03\n0.00e+00\n1.00\n0.97\n\n\n2\n3.00e-03\n1.50e-03\n0.93\n0.98\n\n\n3\n4.00e-03\n0.00e+00\n0.97\n0.98\n\n\n4\n4.00e-03\n0.00e+00\n1.00\n0.97\n\n\n\n\n\n\n\n평균 시간과 점수:\n fit_time       4.10e-03\nscore_time     3.01e-04\ntest_score     9.73e-01\ntrain_score    9.75e-01\ndtype: float64\n\n\n\n\n교차 검증의 장점\n\n계층별(Stratified) k-겹 교차 검증과 그외 전략들\n\nfrom sklearn.datasets import load_iris\niris = load_iris()\nprint(\"Iris 레이블:\\n\", iris.target)\n\nIris 레이블:\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n\n\n\nmglearn.plots.plot_stratified_cross_validation()\n\n\n\n\n\n\n\n\n\n\n\n교차 검증 상세 옵션\n\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits=5)\n\n\nprint(\"교차 검증 점수:\\n\", cross_val_score(logreg, iris.data, iris.target, cv=kfold))\n\n교차 검증 점수:\n [1.    1.    0.867 0.933 0.833]\n\n\n\nkfold = KFold(n_splits=3)\nprint(\"교차 검증 점수:\\n\", cross_val_score(logreg, iris.data, iris.target, cv=kfold))\n\n교차 검증 점수:\n [0. 0. 0.]\n\n\n\nkfold = KFold(n_splits=3, shuffle=True, random_state=0)\nprint(\"교차 검증 점수:\\n\", cross_val_score(logreg, iris.data, iris.target, cv=kfold))\n\n교차 검증 점수:\n [0.98 0.96 0.96]\n\n\n\nLOOCV(Leave-One-Out cross-validation)\n\nfrom sklearn.model_selection import LeaveOneOut\nloo = LeaveOneOut()\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"교차 검증 분할 횟수: \", len(scores))\nprint(\"평균 정확도: {:.2f}\".format(scores.mean()))\n\n교차 검증 분할 횟수:  150\n평균 정확도: 0.97\n\n\n\n\n임의 분할 교차 검증\n\nmglearn.plots.plot_shuffle_split()\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import ShuffleSplit\nshuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\nscores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)\nprint(\"교차 검증 점수:\\n\", scores)\n\n교차 검증 점수:\n [0.96  0.947 0.933 0.96  0.973 0.96  0.96  0.973 0.973 0.973]\n\n\n\n\n그룹별 교차 검증\n\nfrom sklearn.model_selection import GroupKFold\n# 인위적 데이터셋 생성\nX, y = make_blobs(n_samples=12, random_state=0)\n# 처음 세 개의 샘플은 같은 그룹에 속하고\n# 다음은 네 개의 샘플이 같습니다.\ngroups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]\nscores = cross_val_score(logreg, X, y, groups=groups, cv=GroupKFold(n_splits=3))\nprint(\"교차 검증 점수:\\n\", scores)\n\n교차 검증 점수:\n [0.75  0.6   0.667]\n\n\n\nmglearn.plots.plot_group_kfold()",
    "crumbs": [
      "Introduction",
      "모델 평가와 성능 향상"
    ]
  },
  {
    "objectID": "ML05.html#그리드-서치grid-search",
    "href": "ML05.html#그리드-서치grid-search",
    "title": "모델 평가와 성능 향상",
    "section": "그리드 서치(Grid Search)",
    "text": "그리드 서치(Grid Search)\n\n간단한 그리드 서치\n\n# 간단한 그리드 서치 구현\nfrom sklearn.svm import SVC\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n                                                    random_state=0)\nprint(\"훈련 세트의 크기: {}   테스트 세트의 크기: {}\".format(\n      X_train.shape[0], X_test.shape[0]))\n\nbest_score = 0\n\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n        # 매개변수의 각 조합에 대해 SVC를 훈련시킵니다\n        svm = SVC(gamma=gamma, C=C)\n        svm.fit(X_train, y_train)\n        # 테스트 세트로 SVC를 평가합니다\n        score = svm.score(X_test, y_test)\n        # 점수가 더 높으면 매개변수와 함께 기록합니다\n        if score &gt; best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n            \nprint(\"최고 점수: {:.2f}\".format(best_score))\nprint(\"최적 파라미터:\", best_parameters)\n\n훈련 세트의 크기: 112   테스트 세트의 크기: 38\n최고 점수: 0.97\n최적 파라미터: {'C': 100, 'gamma': 0.001}\n\n\n\n\n매개변수 과대적합과 검증 세트\n\nmglearn.plots.plot_threefold_split()\n\n\n\n\n\n\n\n\n\nfrom sklearn.svm import SVC\n# 데이터를 훈련+검증 세트 그리고 테스트 세트로 분할\nX_trainval, X_test, y_trainval, y_test = train_test_split(\n    iris.data, iris.target, random_state=0)\n# 훈련+검증 세트를 훈련 세트와 검증 세트로 분할\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_trainval, y_trainval, random_state=1)\nprint(\"훈련 세트의 크기: {}   검증 세트의 크기: {}   테스트 세트의 크기:\"\n      \" {}\\n\".format(X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\n\nbest_score = 0\n\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n        # 매개변수의 각 조합에 대해 SVC를 훈련시킵니다\n        svm = SVC(gamma=gamma, C=C)\n        svm.fit(X_train, y_train)\n        # 검증 세트로 SVC를 평가합니다\n        score = svm.score(X_valid, y_valid)\n        # 점수가 더 높으면 매개변수와 함께 기록합니다\n        if score &gt; best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n\n# 훈련 세트와 검증 세트를 합쳐 모델을 다시 만든 후\n# 테스트 세트를 사용해 평가합니다\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\ntest_score = svm.score(X_test, y_test)\nprint(\"검증 세트에서 최고 점수: {:.2f}\".format(best_score))\nprint(\"최적 파라미터: \", best_parameters)\nprint(\"최적 파라미터에서 테스트 세트 점수: {:.2f}\".format(test_score))\n\n훈련 세트의 크기: 84   검증 세트의 크기: 28   테스트 세트의 크기: 38\n\n검증 세트에서 최고 점수: 0.96\n최적 파라미터:  {'C': 10, 'gamma': 0.001}\n최적 파라미터에서 테스트 세트 점수: 0.92\n\n\n\n\n교차 검증을 사용한 그리드 서치\n\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n        # 매개변수의 각 조합에 대해 SVC를 훈련시킵니다\n        svm = SVC(gamma=gamma, C=C)\n        # 교차 검증을 적용합니다\n        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)\n        # 교차 검증 정확도의 평균을 계산합니다\n        score = np.mean(scores)\n        # 점수가 더 높으면 매개변수와 함께 기록합니다\n        if score &gt; best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# 훈련 세트와 검증 세트를 합쳐 모델을 다시 만듭니다\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\n\nSVC(C=10, gamma=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC(C=10, gamma=0.1) \n\n\n\nmglearn.plots.plot_cross_val_selection()\n\n\n\n\n\n\n\n\n\nmglearn.plots.plot_grid_search_overview()\n\n\n\n\n\n\n\n\n\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\nprint(\"매개변수 그리드:\\n\", param_grid)\n\n매개변수 그리드:\n {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5, return_train_score=True)\n\n\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n                                                    random_state=0)\n\n\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=5, estimator=SVC(),\n             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n                         'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n             return_train_score=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5, estimator=SVC(),\n             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n                         'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n             return_train_score=True) estimator: SVCSVC()  SVC?Documentation for SVCSVC() \n\n\n\nprint(\"테스트 세트 점수: {:.2f}\".format(grid_search.score(X_test, y_test)))\n\n테스트 세트 점수: 0.97\n\n\n\nprint(\"최적 매개변수:\", grid_search.best_params_)\nprint(\"최고 교차 검증 점수: {:.2f}\".format(grid_search.best_score_))\n\n최적 매개변수: {'C': 10, 'gamma': 0.1}\n최고 교차 검증 점수: 0.97\n\n\n\nprint(\"최고 성능 모델:\\n\", grid_search.best_estimator_)\n\n최고 성능 모델:\n SVC(C=10, gamma=0.1)\n\n\n\n\n교차 검증 결과 분석\n\nimport pandas as pd\npd.set_option('display.max_columns', None)\n# DataFrame으로 변환합니다\nresults = pd.DataFrame(grid_search.cv_results_)\n# 처음 다섯 개 행을 출력합니다\ndisplay(np.transpose(results.head()))\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nmean_fit_time\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nstd_fit_time\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nmean_score_time\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nstd_score_time\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nparam_C\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nparam_gamma\n0.0\n0.01\n0.1\n1\n10\n\n\nparams\n{'C': 0.001, 'gamma': 0.001}\n{'C': 0.001, 'gamma': 0.01}\n{'C': 0.001, 'gamma': 0.1}\n{'C': 0.001, 'gamma': 1}\n{'C': 0.001, 'gamma': 10}\n\n\nsplit0_test_score\n0.35\n0.35\n0.35\n0.35\n0.35\n\n\nsplit1_test_score\n0.35\n0.35\n0.35\n0.35\n0.35\n\n\nsplit2_test_score\n0.36\n0.36\n0.36\n0.36\n0.36\n\n\nsplit3_test_score\n0.36\n0.36\n0.36\n0.36\n0.36\n\n\nsplit4_test_score\n0.41\n0.41\n0.41\n0.41\n0.41\n\n\nmean_test_score\n0.37\n0.37\n0.37\n0.37\n0.37\n\n\nstd_test_score\n0.02\n0.02\n0.02\n0.02\n0.02\n\n\nrank_test_score\n22\n22\n22\n22\n22\n\n\nsplit0_train_score\n0.37\n0.37\n0.37\n0.37\n0.37\n\n\nsplit1_train_score\n0.37\n0.37\n0.37\n0.37\n0.37\n\n\nsplit2_train_score\n0.37\n0.37\n0.37\n0.37\n0.37\n\n\nsplit3_train_score\n0.37\n0.37\n0.37\n0.37\n0.37\n\n\nsplit4_train_score\n0.36\n0.36\n0.36\n0.36\n0.36\n\n\nmean_train_score\n0.37\n0.37\n0.37\n0.37\n0.37\n\n\nstd_train_score\n0.01\n0.01\n0.01\n0.01\n0.01\n\n\n\n\n\n\n\n\nimport mglearn\nscores = np.array(results.mean_test_score).reshape(6, 6)\n# 교차 검증 평균 점수 히트맵 그래프\nmglearn.tools.heatmap(scores, xlabel='gamma', xticklabels=param_grid['gamma'],\n                      ylabel='C', yticklabels=param_grid['C'], cmap=\"viridis\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1, 3, figsize=(13, 5))\n\nparam_grid_linear = {'C': np.linspace(1, 2, 6),\n                     'gamma':  np.linspace(1, 2, 6)}\n\nparam_grid_one_log = {'C': np.linspace(1, 2, 6),\n                     'gamma':  np.logspace(-3, 2, 6)}\n\nparam_grid_range = {'C': np.logspace(-3, 2, 6),\n                     'gamma':  np.logspace(-7, -2, 6)}\n\nfor param_grid, ax in zip([param_grid_linear, param_grid_one_log,\n                           param_grid_range], axes):\n    grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n    grid_search.fit(X_train, y_train)\n    scores = grid_search.cv_results_['mean_test_score'].reshape(6, 6)\n\n    # 교차 검증 평균 점수의 히트맵 그래프\n    scores_image = mglearn.tools.heatmap(\n        scores, xlabel='gamma', ylabel='C', xticklabels=param_grid['gamma'],\n        yticklabels=param_grid['C'], cmap=\"viridis\", ax=ax, vmin=0.3, vmax=0.9)\n    \nplt.colorbar(scores_image, ax=axes.tolist())\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\n\n비대칭 매개변수 그리드 탐색\n\nparam_grid = [{'kernel': ['rbf'],\n               'C': [0.001, 0.01, 0.1, 1, 10, 100],\n               'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n              {'kernel': ['linear'],\n               'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\nprint(\"그리드 목록:\\n\", param_grid)\n\n그리드 목록:\n [{'kernel': ['rbf'], 'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}, {'kernel': ['linear'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\n\n\n\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5, return_train_score=True)\ngrid_search.fit(X_train, y_train)\nprint(\"최적 파라미터:\", grid_search.best_params_)\nprint(\"최고 교차 검증 점수: {:.2f}\".format(grid_search.best_score_))\n\n최적 파라미터: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n최고 교차 검증 점수: 0.97\n\n\n\nresults = pd.DataFrame(grid_search.cv_results_)\n# 좀 더 나은 출력을 위해 결과를 전치시킵니다\ndisplay(results.T)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n\n\nmean_fit_time\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nstd_fit_time\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nmean_score_time\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nstd_score_time\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nparam_C\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.1\n0.1\n0.1\n0.1\n0.1\n0.1\n1\n1\n1\n1\n1\n1\n10\n10\n10\n10\n10\n10\n100\n100\n100\n100\n100\n100\n0.0\n0.01\n0.1\n1\n10\n100\n\n\nparam_gamma\n0.0\n0.01\n0.1\n1\n10\n100\n0.0\n0.01\n0.1\n1\n10\n100\n0.0\n0.01\n0.1\n1\n10\n100\n0.0\n0.01\n0.1\n1\n10\n100\n0.0\n0.01\n0.1\n1\n10\n100\n0.0\n0.01\n0.1\n1\n10\n100\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nparam_kernel\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nrbf\nlinear\nlinear\nlinear\nlinear\nlinear\nlinear\n\n\nparams\n{'C': 0.001, 'gamma': 0.001, 'kernel': 'rbf'}\n{'C': 0.001, 'gamma': 0.01, 'kernel': 'rbf'}\n{'C': 0.001, 'gamma': 0.1, 'kernel': 'rbf'}\n{'C': 0.001, 'gamma': 1, 'kernel': 'rbf'}\n{'C': 0.001, 'gamma': 10, 'kernel': 'rbf'}\n{'C': 0.001, 'gamma': 100, 'kernel': 'rbf'}\n{'C': 0.01, 'gamma': 0.001, 'kernel': 'rbf'}\n{'C': 0.01, 'gamma': 0.01, 'kernel': 'rbf'}\n{'C': 0.01, 'gamma': 0.1, 'kernel': 'rbf'}\n{'C': 0.01, 'gamma': 1, 'kernel': 'rbf'}\n{'C': 0.01, 'gamma': 10, 'kernel': 'rbf'}\n{'C': 0.01, 'gamma': 100, 'kernel': 'rbf'}\n{'C': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}\n{'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n{'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n{'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n{'C': 0.1, 'gamma': 10, 'kernel': 'rbf'}\n{'C': 0.1, 'gamma': 100, 'kernel': 'rbf'}\n{'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n{'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n{'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n{'C': 1, 'gamma': 10, 'kernel': 'rbf'}\n{'C': 1, 'gamma': 100, 'kernel': 'rbf'}\n{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n{'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n{'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n{'C': 10, 'gamma': 10, 'kernel': 'rbf'}\n{'C': 10, 'gamma': 100, 'kernel': 'rbf'}\n{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n{'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n{'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n{'C': 100, 'gamma': 10, 'kernel': 'rbf'}\n{'C': 100, 'gamma': 100, 'kernel': 'rbf'}\n{'C': 0.001, 'kernel': 'linear'}\n{'C': 0.01, 'kernel': 'linear'}\n{'C': 0.1, 'kernel': 'linear'}\n{'C': 1, 'kernel': 'linear'}\n{'C': 10, 'kernel': 'linear'}\n{'C': 100, 'kernel': 'linear'}\n\n\nsplit0_test_score\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.7\n0.91\n1.0\n0.35\n0.35\n0.7\n0.91\n1.0\n0.96\n0.91\n0.39\n0.91\n1.0\n1.0\n0.96\n0.87\n0.52\n1.0\n1.0\n1.0\n0.96\n0.87\n0.52\n0.35\n0.87\n1.0\n1.0\n1.0\n0.96\n\n\nsplit1_test_score\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.35\n0.7\n0.91\n0.91\n0.35\n0.35\n0.7\n0.91\n0.96\n0.91\n0.96\n0.43\n0.91\n0.96\n0.96\n0.96\n0.91\n0.52\n0.96\n0.91\n0.96\n0.96\n0.91\n0.52\n0.35\n0.87\n0.91\n0.96\n1.0\n0.96\n\n\nsplit2_test_score\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.68\n0.91\n1.0\n0.36\n0.36\n0.68\n1.0\n1.0\n1.0\n1.0\n0.55\n1.0\n1.0\n1.0\n1.0\n1.0\n0.59\n1.0\n1.0\n1.0\n1.0\n1.0\n0.59\n0.36\n0.77\n1.0\n1.0\n1.0\n1.0\n\n\nsplit3_test_score\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.68\n0.86\n0.91\n0.36\n0.36\n0.68\n0.91\n0.91\n0.91\n0.82\n0.5\n0.91\n0.91\n0.95\n0.86\n0.82\n0.59\n0.91\n0.95\n0.86\n0.86\n0.82\n0.59\n0.36\n0.77\n0.91\n0.95\n0.91\n0.91\n\n\nsplit4_test_score\n0.41\n0.41\n0.41\n0.41\n0.41\n0.41\n0.41\n0.41\n0.41\n0.41\n0.41\n0.41\n0.41\n0.73\n0.91\n0.95\n0.41\n0.41\n0.73\n0.95\n0.95\n0.95\n0.95\n0.64\n0.95\n0.95\n0.95\n0.95\n0.95\n0.68\n0.95\n0.95\n0.95\n0.95\n0.95\n0.68\n0.41\n0.91\n0.95\n0.95\n0.95\n0.95\n\n\nmean_test_score\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.7\n0.9\n0.96\n0.37\n0.37\n0.7\n0.94\n0.96\n0.95\n0.93\n0.5\n0.94\n0.96\n0.97\n0.95\n0.91\n0.58\n0.96\n0.96\n0.95\n0.95\n0.91\n0.58\n0.37\n0.84\n0.96\n0.97\n0.97\n0.96\n\n\nstd_test_score\n0.02\n0.02\n0.02\n0.02\n0.02\n0.02\n0.02\n0.02\n0.02\n0.02\n0.02\n0.02\n0.02\n0.02\n0.02\n0.04\n0.02\n0.02\n0.02\n0.04\n0.03\n0.03\n0.06\n0.09\n0.04\n0.03\n0.02\n0.04\n0.06\n0.06\n0.03\n0.03\n0.05\n0.04\n0.06\n0.06\n0.02\n0.06\n0.04\n0.02\n0.04\n0.03\n\n\nrank_test_score\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n27\n22\n20\n8\n27\n27\n22\n15\n5\n12\n17\n26\n15\n5\n1\n13\n18\n24\n5\n4\n11\n13\n18\n24\n27\n21\n8\n1\n3\n8\n\n\nsplit0_train_score\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.7\n0.9\n0.94\n0.38\n0.37\n0.7\n0.94\n0.98\n0.98\n1.0\n1.0\n0.94\n0.98\n0.99\n0.99\n1.0\n1.0\n0.98\n0.99\n0.99\n1.0\n1.0\n1.0\n0.37\n0.85\n0.97\n0.99\n0.99\n0.98\n\n\nsplit1_train_score\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.7\n0.93\n0.98\n0.39\n0.37\n0.7\n0.93\n0.97\n1.0\n1.0\n1.0\n0.93\n0.98\n0.99\n0.99\n1.0\n1.0\n0.98\n0.99\n0.99\n1.0\n1.0\n1.0\n0.37\n0.91\n0.97\n0.98\n0.99\n0.99\n\n\nsplit2_train_score\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.7\n0.88\n0.94\n0.37\n0.37\n0.7\n0.92\n0.97\n0.98\n1.0\n1.0\n0.92\n0.97\n0.98\n0.98\n1.0\n1.0\n0.97\n0.98\n0.98\n1.0\n1.0\n1.0\n0.37\n0.84\n0.94\n0.98\n0.98\n0.99\n\n\nsplit3_train_score\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.7\n0.89\n0.97\n0.4\n0.37\n0.7\n0.94\n0.98\n1.0\n1.0\n1.0\n0.93\n0.99\n0.99\n1.0\n1.0\n1.0\n0.99\n0.99\n1.0\n1.0\n1.0\n1.0\n0.37\n0.77\n0.97\n0.99\n0.99\n1.0\n\n\nsplit4_train_score\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.36\n0.69\n0.93\n0.96\n0.36\n0.36\n0.69\n0.93\n0.98\n0.98\n1.0\n1.0\n0.93\n0.98\n1.0\n0.99\n1.0\n1.0\n0.98\n0.99\n1.0\n1.0\n1.0\n1.0\n0.36\n0.88\n0.97\n0.99\n1.0\n1.0\n\n\nmean_train_score\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.7\n0.91\n0.96\n0.38\n0.37\n0.7\n0.94\n0.97\n0.99\n1.0\n1.0\n0.93\n0.98\n0.99\n0.99\n1.0\n1.0\n0.98\n0.99\n0.99\n1.0\n1.0\n1.0\n0.37\n0.85\n0.96\n0.98\n0.99\n0.99\n\n\nstd_train_score\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.0\n0.02\n0.01\n0.02\n0.01\n0.0\n0.01\n0.01\n0.01\n0.0\n0.0\n0.01\n0.01\n0.01\n0.01\n0.0\n0.0\n0.01\n0.0\n0.01\n0.0\n0.0\n0.0\n0.01\n0.05\n0.01\n0.01\n0.01\n0.01\n\n\n\n\n\n\n\n\n\n그리드 서치에 다양한 교차 검증 적용\n\n중첩 교차 검증\n\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\nscores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),\n                         iris.data, iris.target, cv=5)\nprint(\"교차 검증 점수: \", scores)\nprint(\"교차 검증 평균 점수: \", scores.mean())\n\n교차 검증 점수:  [0.967 1.    0.967 0.967 1.   ]\n교차 검증 평균 점수:  0.9800000000000001\n\n\n\ndef nested_cv(X, y, inner_cv, outer_cv, Classifier, parameter_grid):\n    outer_scores = []\n    # outer_cv의 분할을 순회하는 for 루프\n    # (split 메소드는 훈련과 테스트 세트에 해당하는 인덱스를 리턴합니다)\n    for training_samples, test_samples in outer_cv.split(X, y):\n        # 최적의 매개변수를 찾습니다\n        best_parms = {}\n        best_score = -np.inf\n        # 매개변수 그리드를 순회합니다\n        for parameters in parameter_grid:\n            # 안쪽 교차 검증의 점수를 기록합니다\n            cv_scores = []\n            # inner_cv의 분할을 순회하는 for 루프\n            for inner_train, inner_test in inner_cv.split(\n                    X[training_samples], y[training_samples]):\n                # 훈련 데이터와 주어진 매개변수로 분류기를 만듭니다\n                clf = Classifier(**parameters)\n                clf.fit(X[inner_train], y[inner_train])\n                # 검증 세트로 평가합니다\n                score = clf.score(X[inner_test], y[inner_test])\n                cv_scores.append(score)\n            # 안쪽 교차 검증의 평균 점수를 계산합니다\n            mean_score = np.mean(cv_scores)\n            if mean_score &gt; best_score:\n                # 점수가 더 높은면 매개변수와 함께 기록합니다\n                best_score = mean_score\n                best_params = parameters\n        # 바깥쪽 훈련 데이터 전체를 사용해 분류기를 만듭니다\n        clf = Classifier(**best_params)\n        clf.fit(X[training_samples], y[training_samples])\n        # 테스트 세트를 사용해 평가합니다\n        outer_scores.append(clf.score(X[test_samples], y[test_samples]))\n    return np.array(outer_scores)\n\n\nfrom sklearn.model_selection import ParameterGrid, StratifiedKFold\nscores = nested_cv(iris.data, iris.target, StratifiedKFold(5),\n                   StratifiedKFold(5), SVC, ParameterGrid(param_grid))\nprint(\"교차 검증 점수:\", scores)\n\n교차 검증 점수: [0.967 1.    0.967 0.967 1.   ]",
    "crumbs": [
      "Introduction",
      "모델 평가와 성능 향상"
    ]
  },
  {
    "objectID": "ML05.html#평가-지표-및-측정",
    "href": "ML05.html#평가-지표-및-측정",
    "title": "모델 평가와 성능 향상",
    "section": "평가 지표 및 측정",
    "text": "평가 지표 및 측정\n\nfrom sklearn.datasets import load_digits\n\ndigits = load_digits()\ny = digits.target == 9\n\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, y, random_state=0)\n\n\nfrom sklearn.dummy import DummyClassifier\ndummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\npred_most_frequent = dummy_majority.predict(X_test)\nprint(\"예측된 레이블의 고유값:\", np.unique(pred_most_frequent))\nprint(\"테스트 점수: {:.2f}\".format(dummy_majority.score(X_test, y_test)))\n\n예측된 레이블의 고유값: [False]\n테스트 점수: 0.90\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\npred_tree = tree.predict(X_test)\nprint(\"테스트 점수: {:.2f}\".format(tree.score(X_test, y_test)))\n\n테스트 점수: 0.92\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\ndummy = DummyClassifier(strategy='stratified').fit(X_train, y_train)\npred_dummy = dummy.predict(X_test)\nprint(\"dummy 점수: {:.2f}\".format(dummy.score(X_test, y_test)))\n\nlogreg = LogisticRegression(C=0.1, max_iter=1000).fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)\nprint(\"logreg 점수: {:.2f}\".format(logreg.score(X_test, y_test)))\n\ndummy 점수: 0.86\nlogreg 점수: 0.98\n\n\n\n오차 행렬(Confusion matrices)\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion = confusion_matrix(y_test, pred_logreg)\nprint(\"오차 행렬:\\n\", confusion)\n\n오차 행렬:\n [[402   1]\n [  6  41]]\n\n\n\nmglearn.plots.plot_confusion_matrix_illustration()\n\n\n\n\n\n\n\n\n\nmglearn.plots.plot_binary_confusion_matrix()\n\n\n\n\n\n\n\n\n\nprint(\"빈도 기반 더미 모델:\")\nprint(confusion_matrix(y_test, pred_most_frequent))\nprint(\"\\n무작위 더미 모델:\")\nprint(confusion_matrix(y_test, pred_dummy))\nprint(\"\\n결정 트리:\")\nprint(confusion_matrix(y_test, pred_tree))\nprint(\"\\n로지스틱 회귀\")\nprint(confusion_matrix(y_test, pred_logreg))\n\n빈도 기반 더미 모델:\n[[403   0]\n [ 47   0]]\n\n무작위 더미 모델:\n[[369  34]\n [ 46   1]]\n\n결정 트리:\n[[390  13]\n [ 24  23]]\n\n로지스틱 회귀\n[[402   1]\n [  6  41]]\n\n\n\n정확도와의 관계\n\\[\\begin{equation}\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n\\end{equation}\\]\n\n정밀도, 재현율, f-점수\n\\[\\begin{equation}\n\\text{정밀도} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\text{재현율} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\text{F} = 2 \\cdot \\frac{\\text{정밀도} \\cdot \\text{재현율}}{\\text{정밀도} + \\text{재현율}}\n\\end{equation}\\]\n\nfrom sklearn.metrics import f1_score\nprint(\"빈도 기반 더미 모델의 f1 score: {:.2f}\".format(\n    f1_score(y_test, pred_most_frequent)))\nprint(\"무작위 더미 모델의 f1 score: {:.2f}\".format(f1_score(y_test, pred_dummy)))\nprint(\"트리 모델의 f1 score: {:.2f}\".format(f1_score(y_test, pred_tree)))\nprint(\"로지스틱 회귀 모델의 f1 score: {:.2f}\".format(\n    f1_score(y_test, pred_logreg)))\n\n빈도 기반 더미 모델의 f1 score: 0.00\n무작위 더미 모델의 f1 score: 0.02\n트리 모델의 f1 score: 0.55\n로지스틱 회귀 모델의 f1 score: 0.92\n\n\n\nfrom sklearn.metrics import classification_report\n# 0 나눗셈 경고를 피하기 위해 zero_division 매개변수를 0으로 지정합니다.\nprint(classification_report(y_test, pred_most_frequent,\n                            target_names=[\"9 아님\", \"9\"], zero_division=0))\n\n              precision    recall  f1-score   support\n\n        9 아님       0.90      1.00      0.94       403\n           9       0.00      0.00      0.00        47\n\n    accuracy                           0.90       450\n   macro avg       0.45      0.50      0.47       450\nweighted avg       0.80      0.90      0.85       450\n\n\n\n\nprint(classification_report(y_test, pred_dummy,\n                            target_names=[\"9 아님\", \"9\"]))\n\n              precision    recall  f1-score   support\n\n        9 아님       0.89      0.92      0.90       403\n           9       0.03      0.02      0.02        47\n\n    accuracy                           0.82       450\n   macro avg       0.46      0.47      0.46       450\nweighted avg       0.80      0.82      0.81       450\n\n\n\n\nprint(classification_report(y_test, pred_logreg,\n                            target_names=[\"9 아님\", \"9\"]))\n\n              precision    recall  f1-score   support\n\n        9 아님       0.99      1.00      0.99       403\n           9       0.98      0.87      0.92        47\n\n    accuracy                           0.98       450\n   macro avg       0.98      0.93      0.96       450\nweighted avg       0.98      0.98      0.98       450\n\n\n\n\n\n\n불확실성 고려\n\nX, y = make_blobs(n_samples=(400, 50), cluster_std=[7.0, 2], random_state=22)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nsvc = SVC(gamma=.05).fit(X_train, y_train)                                     \n\n\nmglearn.plots.plot_decision_threshold()\n\n\n\n\n\n\n\n\n\nprint(classification_report(y_test, svc.predict(X_test)))\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.89      0.93       104\n           1       0.35      0.67      0.46         9\n\n    accuracy                           0.88       113\n   macro avg       0.66      0.78      0.70       113\nweighted avg       0.92      0.88      0.89       113\n\n\n\n\ny_pred_lower_threshold = svc.decision_function(X_test) &gt; -.8\n\n\nprint(classification_report(y_test, y_pred_lower_threshold))\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.82      0.90       104\n           1       0.32      1.00      0.49         9\n\n    accuracy                           0.83       113\n   macro avg       0.66      0.91      0.69       113\nweighted avg       0.95      0.83      0.87       113\n\n\n\n\n\n정밀도-재현율 곡선과 ROC 곡선\n\nfrom sklearn.metrics import precision_recall_curve\nprecision, recall, thresholds = precision_recall_curve(\n    y_test, svc.decision_function(X_test))\n\n\n# 부드러운 곡선을 위해 데이터 포인트 수를 늘립니다\nX, y = make_blobs(n_samples=(4000, 500), cluster_std=[7.0, 2], random_state=22)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nsvc = SVC(gamma=.05).fit(X_train, y_train)\n\nprecision, recall, thresholds = precision_recall_curve(\n    y_test, svc.decision_function(X_test))\n# 0에 가까운 임계값을 찾습니다\nclose_zero = np.argmin(np.abs(thresholds))\nplt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n         label=\"임계값 0\", fillstyle=\"none\", c='k', mew=2)\n\nplt.plot(precision, recall, label=\"정밀도-재현율 곡선\")\nplt.xlabel(\"정밀도\")\nplt.ylabel(\"재현율\")\nplt.legend(loc=\"best\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100, random_state=0, max_features=2)\nrf.fit(X_train, y_train)\n\n# RandomForestClassifier는 decision_function 대신 predict_proba를 제공합니다.\nprecision_rf, recall_rf, thresholds_rf = precision_recall_curve(\n    y_test, rf.predict_proba(X_test)[:, 1])\n\nplt.plot(precision, recall, label=\"svc\")\n\nplt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n         label=\"svc: 임계값 0\", fillstyle=\"none\", c='k', mew=2)\n\nplt.plot(precision_rf, recall_rf, label=\"rf\")\n\nclose_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\nplt.plot(precision_rf[close_default_rf], recall_rf[close_default_rf], '^', c='k',\n         markersize=10, label=\"rf: 임계값 0.5\", fillstyle=\"none\", mew=2)\nplt.xlabel(\"정밀도\")\nplt.ylabel(\"재현율\")\nplt.legend(loc=\"best\")\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nprint(\"랜덤 포레스트의 f1_score: {:.3f}\".format(\n    f1_score(y_test, rf.predict(X_test))))\nprint(\"svc의 f1_score: {:.3f}\".format(f1_score(y_test, svc.predict(X_test))))\n\n랜덤 포레스트의 f1_score: 0.610\nsvc의 f1_score: 0.656\n\n\n\nfrom sklearn.metrics import average_precision_score\nap_rf = average_precision_score(y_test, rf.predict_proba(X_test)[:, 1])\nap_svc = average_precision_score(y_test, svc.decision_function(X_test))\nprint(\"랜덤 포레스트의 평균 정밀도: {:.3f}\".format(ap_rf))\nprint(\"svc의 평균 정밀도: {:.3f}\".format(ap_svc))\n\n랜덤 포레스트의 평균 정밀도: 0.660\nsvc의 평균 정밀도: 0.666\n\n\n\n\nROC 와 AUC\n\\[\\begin{equation}\n\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n\\end{equation}\\]\n\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, svc.decision_function(X_test))\n\nplt.plot(fpr, tpr, label=\"ROC 곡선\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR (재현율)\")\n# 0 근처의 임계값을 찾습니다\nclose_zero = np.argmin(np.abs(thresholds))\nplt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10,\n         label=\"임계값 0\", fillstyle=\"none\", c='k', mew=2)\nplt.legend(loc=4)\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import roc_curve\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf.predict_proba(X_test)[:, 1])\n\nplt.plot(fpr, tpr, label=\"SVC의 ROC 곡선\")\nplt.plot(fpr_rf, tpr_rf, label=\"RF의 ROC 곡선\")\n\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR (재현율)\")\nplt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10,\n         label=\"SVC 임계값 0\", fillstyle=\"none\", c='k', mew=2)\nclose_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\nplt.plot(fpr_rf[close_default_rf], tpr_rf[close_default_rf], '^', markersize=10,\n         label=\"RF 임계값 0.5\", fillstyle=\"none\", c='k', mew=2)\n\nplt.legend(loc=4)\nplt.show() # 책에는 없음\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import roc_auc_score\nrf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\nsvc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\nprint(\"랜덤 포레스트의 AUC: {:.3f}\".format(rf_auc))\nprint(\"SVC의 AUC: {:.3f}\".format(svc_auc))\n\n랜덤 포레스트의 AUC: 0.937\nSVC의 AUC: 0.916\n\n\n\ny = digits.target == 9\n\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, y, random_state=0)\n\nplt.figure()\n\nfor gamma in [1, 0.1, 0.01]:\n    svc = SVC(gamma=gamma).fit(X_train, y_train)\n    accuracy = svc.score(X_test, y_test)\n    auc = roc_auc_score(y_test, svc.decision_function(X_test))\n    fpr, tpr, _ = roc_curve(y_test , svc.decision_function(X_test))\n    print(\"gamma = {:.2f}  정확도 = {:.2f}  AUC = {:.2f}\".format(\n        gamma, accuracy, auc))\n    plt.plot(fpr, tpr, label=\"gamma={:.2f}\".format(gamma))\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.xlim(-0.01, 1)\nplt.ylim(0, 1.02)\nplt.legend(loc=\"best\")\nplt.show() # 책에는 없음\n\ngamma = 1.00  정확도 = 0.90  AUC = 0.50\ngamma = 0.10  정확도 = 0.90  AUC = 0.96\ngamma = 0.01  정확도 = 0.90  AUC = 1.00\n\n\n\n\n\n\n\n\n\n\n\n\n다중 분류의 평가 지표\n\nfrom sklearn.metrics import accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, digits.target, random_state=0)\nlr = LogisticRegression(max_iter=5000).fit(X_train, y_train)\npred = lr.predict(X_test)\nprint(\"정확도: {:.3f}\".format(accuracy_score(y_test, pred)))\nprint(\"오차 행렬:\\n\", confusion_matrix(y_test, pred))\n\n정확도: 0.953\n오차 행렬:\n [[37  0  0  0  0  0  0  0  0  0]\n [ 0 40  0  0  0  0  0  0  2  1]\n [ 0  0 41  3  0  0  0  0  0  0]\n [ 0  0  0 43  0  0  0  0  1  1]\n [ 0  0  0  0 37  0  0  1  0  0]\n [ 0  0  0  0  0 46  0  0  0  2]\n [ 0  1  0  0  0  0 51  0  0  0]\n [ 0  0  0  1  1  0  0 46  0  0]\n [ 0  3  1  0  0  0  0  0 43  1]\n [ 0  0  0  0  0  1  0  0  1 45]]\n\n\n\nscores_image = mglearn.tools.heatmap(\n    confusion_matrix(y_test, pred), xlabel='예측 레이블',\n    ylabel='진짜 레이블', xticklabels=digits.target_names,\n    yticklabels=digits.target_names, cmap=plt.cm.gray_r, fmt=\"%d\")    \nplt.title(\"오차 행렬\")\nplt.gca().invert_yaxis()\n\n\n\n\n\n\n\n\n\nprint(classification_report(y_test, pred))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        37\n           1       0.91      0.93      0.92        43\n           2       0.98      0.93      0.95        44\n           3       0.91      0.96      0.93        45\n           4       0.97      0.97      0.97        38\n           5       0.98      0.96      0.97        48\n           6       1.00      0.98      0.99        52\n           7       0.98      0.96      0.97        48\n           8       0.91      0.90      0.91        48\n           9       0.90      0.96      0.93        47\n\n    accuracy                           0.95       450\n   macro avg       0.95      0.95      0.95       450\nweighted avg       0.95      0.95      0.95       450\n\n\n\n\nprint(\"micro 평균 f1 점수: {:.3f}\".format(f1_score(y_test, pred, average=\"micro\")))\nprint(\"macro 평균 f1 점수: {:.3f}\".format(f1_score(y_test, pred, average=\"macro\")))\n\nmicro 평균 f1 점수: 0.953\nmacro 평균 f1 점수: 0.954\n\n\n\n\n회귀 평가 지표\n\n\n모델 선택에서 평가 지표 사용하기\n\n# 분류의 기본 평가 지표는 정확도 입니다\nprint(\"기본 평가 지표:\",\n      cross_val_score(SVC(), digits.data, digits.target == 9, cv=5))\n# scoring=\"accuracy\"의 결과와 같습니다.\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                                     scoring=\"accuracy\", cv=5)\nprint(\"정확도 지표:\", explicit_accuracy)\nap =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                           scoring=\"average_precision\", cv=5)\nprint(\"평균 정밀도 지표:\", ap)\n\n기본 평가 지표: [0.975 0.992 1.    0.994 0.981]\n정확도 지표: [0.975 0.992 1.    0.994 0.981]\n평균 정밀도 지표: [0.976 0.989 1.    0.999 0.95 ]\n\n\n\nres = cross_validate(SVC(), digits.data, digits.target == 9,\n                     scoring=[\"accuracy\", \"average_precision\", \"recall_macro\"],\n                     return_train_score=True, cv=5)\ndisplay(pd.DataFrame(res))\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_accuracy\ntrain_accuracy\ntest_average_precision\ntrain_average_precision\ntest_recall_macro\ntrain_recall_macro\n\n\n\n\n0\n6.12e-03\n8.02e-03\n0.97\n0.99\n0.98\n0.99\n0.89\n0.97\n\n\n1\n7.51e-03\n7.51e-03\n0.99\n1.00\n0.99\n1.00\n0.96\n0.98\n\n\n2\n6.50e-03\n7.00e-03\n1.00\n1.00\n1.00\n1.00\n1.00\n0.98\n\n\n3\n6.00e-03\n6.51e-03\n0.99\n1.00\n1.00\n1.00\n0.97\n0.98\n\n\n4\n6.00e-03\n6.51e-03\n0.98\n1.00\n0.95\n1.00\n0.90\n0.99\n\n\n\n\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, digits.target == 9, random_state=0)\n\n# 일부러 적절하지 않은 그리드를 만듭니다\nparam_grid = {'gamma': [0.0001, 0.01, 0.1, 1, 10]}\n# 기본 정확도 측정 지표를 사용합니다\ngrid = GridSearchCV(SVC(), param_grid=param_grid)\ngrid.fit(X_train, y_train)\nprint(\"정확도 지표를 사용한 그리드 서치\")\nprint(\"최적의 파라미터:\", grid.best_params_)\nprint(\"최상의 교차 검증 점수 (정확도)): {:.3f}\".format(grid.best_score_))\nprint(\"테스트 세트 평균 정밀도: {:.3f}\".format(\n      average_precision_score(y_test, grid.decision_function(X_test))))\nprint(\"테스트 세트 정확도: {:.3f}\".format(\n      # grid.score 점수와 동일합니다\n      accuracy_score(y_test, grid.predict(X_test))))\n\n정확도 지표를 사용한 그리드 서치\n최적의 파라미터: {'gamma': 0.0001}\n최상의 교차 검증 점수 (정확도)): 0.976\n테스트 세트 평균 정밀도: 0.966\n테스트 세트 정확도: 0.973\n\n\n\n# 평균 정밀도 지표 사용\ngrid = GridSearchCV(SVC(), param_grid=param_grid, scoring=\"average_precision\")\ngrid.fit(X_train, y_train)\nprint(\"평균 정밀도 지표를 사용한 그리드 서치\")\nprint(\"최적의 파라미터:\", grid.best_params_)\nprint(\"최상의 교차 검증 점수 (평균 정밀도): {:.3f}\".format(grid.best_score_))\nprint(\"테스트 세트 평균 정밀도: {:.3f}\".format(\n      # grid.score 점수와 동일합니다\n      average_precision_score(y_test, grid.decision_function(X_test))))\nprint(\"테스트 세트 정확도: {:.3f}\".format(\n      accuracy_score(y_test, grid.predict(X_test))))\n\n평균 정밀도 지표를 사용한 그리드 서치\n최적의 파라미터: {'gamma': 0.01}\n최상의 교차 검증 점수 (평균 정밀도): 0.988\n테스트 세트 평균 정밀도: 0.996\n테스트 세트 정확도: 0.896",
    "crumbs": [
      "Introduction",
      "모델 평가와 성능 향상"
    ]
  }
]